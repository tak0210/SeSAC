{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”\n",
    "- **ëª©í‘œ**: ê¸°ìƒë³€ìˆ˜ë¥¼ í™œìš©í•œ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ\n",
    "- **ë°ì´í„°**: 2021-2022ë…„ 1ì‹œê°„ ë‹¨ìœ„ ê¸°ìƒ + ì—´ìˆ˜ìš” ë°ì´í„°\n",
    "- **ì „ì²˜ë¦¬**: ê²°ì¸¡ì¹˜ ì²˜ë¦¬, íŒŒìƒë³€ìˆ˜ ìƒì„±, ìŠ¤ì¼€ì¼ë§, ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "âœ… í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: (52557, 11)\n",
      "   ì»¬ëŸ¼: ['tm', 'branch_id', 'ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand']\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: (26280, 11)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train_path, test_path=None):\n",
    "    \"\"\"\n",
    "    í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "    train_path (str): í›ˆë ¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "    test_path (str): í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (train_df, test_df) ë˜ëŠ” train_dfë§Œ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„° ë¡œë“œ\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    print(f\"âœ… í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {train_df.shape}\")\n",
    "    print(f\"   ì»¬ëŸ¼: {list(train_df.columns)}\")\n",
    "    \n",
    "    # Unnamed ì»¬ëŸ¼ ì œê±°\n",
    "    if 'Unnamed: 0' in train_df.columns:\n",
    "        train_df = train_df.drop(columns=['Unnamed: 0'])\n",
    "        print(\"   Unnamed: 0 ì»¬ëŸ¼ ì œê±°\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ìˆëŠ” ê²½ìš°)\n",
    "    if test_path:\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {test_df.shape}\")\n",
    "        \n",
    "        if 'Unnamed: 0' in test_df.columns:\n",
    "            test_df = test_df.drop(columns=['Unnamed: 0'])\n",
    "            print(\"   Unnamed: 0 ì»¬ëŸ¼ ì œê±°\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        return train_df\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ (íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”)\n",
    "# train_df = load_data('train_heat.csv')\n",
    "# test_dfê°€ ë³„ë„ë¡œ ìˆë‹¤ë©´: train_df, test_df = load_data('train_heat.csv', 'test_heat.csv')\n",
    "\n",
    "\n",
    "train_df, test_df = load_data('train_heat_ABD.csv', 'test_heat_ABD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\n",
      "   ë°ì´í„° í˜•íƒœ: (52557, 11)\n",
      "   ì§€ì‚¬ë³„ ë°ì´í„° ìˆ˜:\n",
      "branch_id\n",
      "A    17519\n",
      "B    17519\n",
      "D    17519\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“Š ê²°ì¸¡ì¹˜ í˜„í™©:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "print(f\"\\nğŸ“ˆ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
    "print(f\"   ë°ì´í„° í˜•íƒœ: {train_df.shape}\")\n",
    "print(f\"   ì§€ì‚¬ë³„ ë°ì´í„° ìˆ˜:\")\n",
    "if 'train_heat.branch_id' in train_df.columns:\n",
    "    print(train_df['train_heat.branch_id'].value_counts().head())\n",
    "elif 'branch_id' in train_df.columns:\n",
    "    print(train_df['branch_id'].value_counts().head())\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ í˜„í™© í™•ì¸\n",
    "print(f\"\\nğŸ“Š ê²°ì¸¡ì¹˜ í˜„í™©:\")\n",
    "missing_info = train_df.isnull().sum()\n",
    "print(missing_info[missing_info > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì»¬ëŸ¼ëª…ì—ì„œ 'train_heat.' ì ‘ë‘ì‚¬ ì œê±°\n",
    "# print(\"ğŸ”§ ì»¬ëŸ¼ëª… ì •ë¦¬ ì¤‘...\")\n",
    "# print(f\"   ê¸°ì¡´ ì»¬ëŸ¼ëª…: {list(train_df.columns)}\")\n",
    "\n",
    "# # 'train_heat.' ì ‘ë‘ì‚¬ ì œê±°\n",
    "# train_df.columns = train_df.columns.str.replace('train_heat.', '', regex=False)\n",
    "\n",
    "# print(f\"   ë³€ê²½ëœ ì»¬ëŸ¼ëª…: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ê²°ì¸¡ì¹˜ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘...\n",
      "   ì»¬ëŸ¼ëª… ì •ë¦¬ ì™„ë£Œ: ['tm', 'branch_id', 'ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand']\n",
      "   ğŸ”„ -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ ì¤‘...\n",
      "   âœ… ì´ 54449ê°œì˜ -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
      "   â˜€ï¸ ì¼ì‚¬ëŸ‰(si) íŠ¹ë³„ ì²˜ë¦¬ ì¤‘...\n",
      "   âœ… ì•¼ê°„ì‹œê°„ëŒ€ ì¼ì‚¬ëŸ‰ 23796ê°œë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬\n",
      "   ğŸ“ˆ ì§€ì‚¬ë³„ ì„ í˜•ë³´ê°„ ì²˜ë¦¬ ì¤‘...\n",
      "   âœ… ì„ í˜•ë³´ê°„ ì™„ë£Œ\n",
      "   ğŸ‰ ëª¨ë“  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    1. -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "    2. ì¼ì‚¬ëŸ‰(si) íŠ¹ë³„ ì²˜ë¦¬ (ì•¼ê°„ì‹œê°„ 0 ì²˜ë¦¬)\n",
    "    3. ì§€ì‚¬ë³„ ì„ í˜•ë³´ê°„\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ì»¬ëŸ¼ëª… ì •ë¦¬ (train_heat. ì ‘ë‘ì‚¬ ì œê±°)\n",
    "    df.columns = [col.replace('train_heat.', '') for col in df.columns]\n",
    "    print(f\"   ì»¬ëŸ¼ëª… ì •ë¦¬ ì™„ë£Œ: {list(df.columns)}\")\n",
    "    \n",
    "    # 2-1. -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "    print(\"   ğŸ”„ -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ ì¤‘...\")\n",
    "    missing_cols = ['ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand']\n",
    "    before_count = 0\n",
    "    for col in missing_cols:\n",
    "        if col in df.columns:\n",
    "            count = (df[col] == -99).sum()\n",
    "            before_count += count\n",
    "            df[col] = df[col].replace(-99, np.nan)\n",
    "    \n",
    "    print(f\"   âœ… ì´ {before_count}ê°œì˜ -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\")\n",
    "    \n",
    "    # 2-2. ì¼ì‚¬ëŸ‰(si) íŠ¹ë³„ ì²˜ë¦¬\n",
    "    if 'si' in df.columns and 'tm' in df.columns:\n",
    "        print(\"   â˜€ï¸ ì¼ì‚¬ëŸ‰(si) íŠ¹ë³„ ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # tmì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "        df['datetime'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        \n",
    "        # ì•¼ê°„ì‹œê°„ëŒ€ (18ì‹œ-08ì‹œ) NaNì„ 0ìœ¼ë¡œ ì²˜ë¦¬\n",
    "        night_mask = (df['hour'] < 8) | (df['hour'] > 18)\n",
    "        night_nan_count = df.loc[night_mask, 'si'].isna().sum()\n",
    "        df.loc[night_mask & df['si'].isna(), 'si'] = 0\n",
    "        \n",
    "        print(f\"   âœ… ì•¼ê°„ì‹œê°„ëŒ€ ì¼ì‚¬ëŸ‰ {night_nan_count}ê°œë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬\")\n",
    "    \n",
    "    # 2-3. ì§€ì‚¬ë³„ ì„ í˜•ë³´ê°„\n",
    "    print(\"   ğŸ“ˆ ì§€ì‚¬ë³„ ì„ í˜•ë³´ê°„ ì²˜ë¦¬ ì¤‘...\")\n",
    "    \n",
    "    if 'branch_id' in df.columns:\n",
    "        # datetime ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "        df = df.sort_values(['branch_id', 'datetime'])\n",
    "        \n",
    "        # ì§€ì‚¬ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì„ í˜•ë³´ê°„\n",
    "        for branch in df['branch_id'].unique():\n",
    "            branch_mask = df['branch_id'] == branch\n",
    "            branch_data = df[branch_mask].copy()\n",
    "            \n",
    "            # ê° ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ì„ í˜•ë³´ê°„\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                if col in branch_data.columns:\n",
    "                    # ì‹œê°„ ìˆœì„œë¡œ ì„ í˜•ë³´ê°„\n",
    "                    branch_data[col] = branch_data[col].interpolate(method='linear')\n",
    "                    # ì•ë’¤ ê²°ì¸¡ì¹˜ëŠ” forward/backward fill\n",
    "                    branch_data[col] = branch_data[col].fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            # ì›ë³¸ ë°ì´í„°ì— ë°˜ì˜\n",
    "            df.loc[branch_mask, numeric_cols] = branch_data[numeric_cols]\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n",
    "    missing_after = df.isnull().sum()\n",
    "    missing_cols_after = missing_after[missing_after > 0]\n",
    "    \n",
    "    print(f\"   âœ… ì„ í˜•ë³´ê°„ ì™„ë£Œ\")\n",
    "    if len(missing_cols_after) > 0:\n",
    "        print(f\"   âš ï¸ ë‚¨ì€ ê²°ì¸¡ì¹˜: {missing_cols_after.to_dict()}\")\n",
    "    else:\n",
    "        print(f\"   ğŸ‰ ëª¨ë“  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹¤í–‰\n",
    "train_df = handle_missing_values(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¡ï¸ ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„± ì¤‘...\n",
      "   ğŸ“… ì‹œê°„ ê¸°ë³¸ ë³€ìˆ˜ ìƒì„±...\n",
      "   ğŸ”„ ê³„ì ˆì„± ìˆœí™˜ ë³€ìˆ˜ ìƒì„±...\n",
      "âœ… ì‹œê°„ ê¸°ë³¸ ë³€ìˆ˜ ë° ìˆœí™˜ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def create_weather_time_features(df):\n",
    "    \"\"\"\n",
    "    ê¸°ìƒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸŒ¡ï¸ ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„± ì¤‘...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 3-1. ì‹œê°„ ê¸°ë³¸ ë³€ìˆ˜ ìƒì„±\n",
    "    print(\"   ğŸ“… ì‹œê°„ ê¸°ë³¸ ë³€ìˆ˜ ìƒì„±...\")\n",
    "    if 'datetime' not in df.columns and 'tm' in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
    "    \n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek  # 0:ì›”ìš”ì¼, 6:ì¼ìš”ì¼\n",
    "    df['dayofyear'] = df['datetime'].dt.dayofyear\n",
    "    df['week'] = df['datetime'].dt.isocalendar().week\n",
    "    \n",
    "    # 3-2. ê³„ì ˆì„± ë°˜ì˜ ë³€ìˆ˜ (ìˆœí™˜í˜• ì¸ì½”ë”©)\n",
    "    print(\"   ğŸ”„ ê³„ì ˆì„± ìˆœí™˜ ë³€ìˆ˜ ìƒì„±...\")\n",
    "    \n",
    "    # ì‹œê°„ì˜ ìˆœí™˜ì„±ì„ sin/cosë¡œ í‘œí˜„\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "    \n",
    "    # ê³„ì ˆ êµ¬ë¶„\n",
    "    df['season'] = df['month'].map({12: 0, 1: 0, 2: 0,  # ê²¨ìš¸\n",
    "                                   3: 1, 4: 1, 5: 1,    # ë´„\n",
    "                                   6: 2, 7: 2, 8: 2,    # ì—¬ë¦„\n",
    "                                   9: 3, 10: 3, 11: 3}) # ê°€ì„\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„± ì‹¤í–‰\n",
    "train_df = create_weather_time_features(train_df)\n",
    "print(\"âœ… ì‹œê°„ ê¸°ë³¸ ë³€ìˆ˜ ë° ìˆœí™˜ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ë‚œë°© ê´€ë ¨ ì‹œê°„ ë³€ìˆ˜ ìƒì„±...\n",
      "   âœ… ë‚œë°© ê´€ë ¨ ì‹œê°„ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def create_heating_related_features(df):\n",
    "    \"\"\"\n",
    "    ë‚œë°© ê´€ë ¨ ì‹œê°„ ë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”¥ ë‚œë°© ê´€ë ¨ ì‹œê°„ ë³€ìˆ˜ ìƒì„±...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ë‚œë°©ì‹œì¦Œ (10ì›”-4ì›”)\n",
    "    df['heating_season'] = df['month'].isin([10, 11, 12, 1, 2, 3, 4]).astype(int)\n",
    "    # í”¼í¬ ë‚œë°©ì‹œì¦Œ (12ì›”-2ì›”)\n",
    "    df['peak_heating'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "    # ì¤‘ê°„ê³„ì ˆ (3-4ì›”, 10-11ì›”)\n",
    "    df['shoulder_season'] = df['month'].isin([3, 4, 10, 11]).astype(int)\n",
    "    \n",
    "    # ì‹œê°„ëŒ€ë³„ êµ¬ë¶„\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_work_hour'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)\n",
    "    df['is_peak_morning'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
    "    df['is_peak_evening'] = ((df['hour'] >= 18) & (df['hour'] <= 22)).astype(int)\n",
    "    df['is_night'] = ((df['hour'] >= 23) | (df['hour'] <= 6)).astype(int)\n",
    "    \n",
    "    print(\"   âœ… ë‚œë°© ê´€ë ¨ ì‹œê°„ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ\")\n",
    "    return df\n",
    "\n",
    "train_df = create_heating_related_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¤ï¸ ê¸°ìƒ íŒŒìƒë³€ìˆ˜ ìƒì„±...\n",
      "   âœ… ê¸°ìƒ íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def create_weather_derived_features(df):\n",
    "    \"\"\"\n",
    "    ê¸°ìƒ íŒŒìƒë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸŒ¤ï¸ ê¸°ìƒ íŒŒìƒë³€ìˆ˜ ìƒì„±...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ë‚œë°©ë„ì¼ (HDD) & ëƒ‰ë°©ë„ì¼ (CDD)\n",
    "    base_temp_heating = 18.0  # ë‚œë°© ê¸°ì¤€ì˜¨ë„\n",
    "    base_temp_cooling = 26.0  # ëƒ‰ë°© ê¸°ì¤€ì˜¨ë„\n",
    "    \n",
    "    if 'ta' in df.columns:\n",
    "        df['HDD_18'] = np.maximum(base_temp_heating - df['ta'], 0)\n",
    "        # df['HDD_20'] = np.maximum(20 - df['ta'], 0)  # ì¶”ê°€ ê¸°ì¤€ì˜¨ë„\n",
    "        # df['CDD_26'] = np.maximum(df['ta'] - base_temp_cooling, 0)\n",
    "    \n",
    "    # # ì²´ê°ì˜¨ë„ ê³„ì‚° (í’ì† ê³ ë ¤)\n",
    "    # if 'ta' in df.columns and 'ws' in df.columns:\n",
    "    #     df['wind_chill'] = 13.12 + 0.6215 * df['ta'] - 11.37 * (df['ws'] ** 0.16) + 0.3965 * df['ta'] * (df['ws'] ** 0.16)\n",
    "    \n",
    "    # ë¶ˆì¾Œì§€ìˆ˜ (ì˜¨ë„ + ìŠµë„)\n",
    "    if 'ta' in df.columns and 'hm' in df.columns:\n",
    "        df['discomfort_index'] = 0.81 * df['ta'] + 0.01 * df['hm'] * (0.99 * df['ta'] - 14.3) + 46.3\n",
    "    \n",
    "    # ê¸°ì˜¨ ë²”ì£¼í™”\n",
    "    if 'ta' in df.columns:\n",
    "        df['temp_category'] = pd.cut(df['ta'], \n",
    "                                   bins=[-np.inf, 0, 10, 20, 30, np.inf], \n",
    "                                   labels=[0, 1, 2, 3, 4])  # ë§¤ìš°ì¶”ì›€~ë”ì›€\n",
    "        df['temp_category'] = df['temp_category'].astype(int)\n",
    "    \n",
    "    # ê°•ìˆ˜ ê´€ë ¨\n",
    "    if 'rn_day' in df.columns:\n",
    "        df['is_rainy'] = (df['rn_day'] > 0).astype(int)\n",
    "        df['is_heavy_rain'] = (df['rn_day'] > 10).astype(int)\n",
    "        df['rain_intensity'] = pd.cut(df['rn_day'], \n",
    "                                    bins=[-1, 0, 1, 5, 10, np.inf], \n",
    "                                    labels=[0, 1, 2, 3, 4])\n",
    "        df['rain_intensity'] = df['rain_intensity'].astype(int)\n",
    "    \n",
    "    print(\"   âœ… ê¸°ìƒ íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ\")\n",
    "    return df\n",
    "\n",
    "train_df = create_weather_derived_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„±...\n",
      "   âœ… Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„± ì™„ë£Œ! (ì´ 32ê°œ)\n"
     ]
    }
   ],
   "source": [
    "def create_rolling_and_lag_features(df):\n",
    "    \"\"\"\n",
    "    Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„±...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ì§€ì‚¬ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬\n",
    "    if 'branch_id' in df.columns:\n",
    "        df = df.sort_values(['branch_id', 'datetime'])\n",
    "        \n",
    "        for branch in df['branch_id'].unique():\n",
    "            branch_mask = df['branch_id'] == branch\n",
    "            branch_data = df[branch_mask].copy()\n",
    "            \n",
    "            # ê¸°ì˜¨ ê´€ë ¨ rolling í†µê³„\n",
    "            if 'ta' in branch_data.columns:\n",
    "                for window in [6, 12, 24, 48]:\n",
    "                    rolling_ta = branch_data['ta'].rolling(window=window, min_periods=1)\n",
    "                    branch_data[f'ta_mean_{window}h'] = rolling_ta.mean()\n",
    "                    branch_data[f'ta_std_{window}h'] = rolling_ta.std().fillna(0)  # stdê°€ NaNì¼ ìˆ˜ ìˆìŒ\n",
    "                    branch_data[f'ta_max_{window}h'] = rolling_ta.max()\n",
    "                    branch_data[f'ta_min_{window}h'] = rolling_ta.min()\n",
    "                \n",
    "                # ê¸°ì˜¨ ì°¨ë¶„ ë° ë³€í™”ìœ¨\n",
    "                branch_data['ta_diff_1h'] = branch_data['ta'].diff()\n",
    "                branch_data['ta_diff_24h'] = branch_data['ta'].diff(periods=24)\n",
    "                \n",
    "                # Lag ë³€ìˆ˜\n",
    "                for lag in [1, 2, 3, 6, 12, 24]:\n",
    "                    branch_data[f'ta_lag_{lag}'] = branch_data['ta'].shift(lag)\n",
    "            \n",
    "            # HDD rolling í•©ê³„\n",
    "            if 'HDD_18' in branch_data.columns:\n",
    "                for window in [6, 12, 24, 48]:\n",
    "                    branch_data[f'HDD_sum_{window}h'] = branch_data['HDD_18'].rolling(window=window, min_periods=1).sum()\n",
    "            \n",
    "            # ê°•ìˆ˜ëŸ‰ ëˆ„ì \n",
    "            if 'rn_hr1' in branch_data.columns:\n",
    "                for window in [3, 6, 12, 24]:\n",
    "                    branch_data[f'rain_sum_{window}h'] = branch_data['rn_hr1'].rolling(window=window, min_periods=1).sum()\n",
    "            \n",
    "            # ë¬´í•œê°’ ë° NaN ì²´í¬ ë° ì²˜ë¦¬\n",
    "            for col in branch_data.columns:\n",
    "                if branch_data[col].dtype in ['float64', 'float32']:\n",
    "                    # ë¬´í•œê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "                    branch_data[col] = branch_data[col].replace([np.inf, -np.inf], np.nan)\n",
    "                    # NaNì„ ì ì ˆí•œ ê°’ìœ¼ë¡œ ë³€í™˜ (rolling í†µê³„ëŠ” 0, lagëŠ” forward fill)\n",
    "                    if any(keyword in col for keyword in ['_mean_', '_std_', '_max_', '_min_', '_sum_', '_diff_']):\n",
    "                        branch_data[col] = branch_data[col].fillna(0)\n",
    "                    elif '_lag_' in col:\n",
    "                        branch_data[col] = branch_data[col].fillna(method='ffill').fillna(0)\n",
    "            \n",
    "            # ì›ë³¸ ë°ì´í„°ì— ë°˜ì˜\n",
    "            new_cols = [col for col in branch_data.columns if col not in df.columns]\n",
    "            if new_cols:\n",
    "                df.loc[branch_mask, new_cols] = branch_data[new_cols]\n",
    "    \n",
    "    rolling_cols = len([col for col in df.columns if any(keyword in col for keyword in ['_mean_', '_std_', '_max_', '_min_', '_sum_', '_lag_', '_diff_'])])\n",
    "    print(f\"   âœ… Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„± ì™„ë£Œ! (ì´ {rolling_cols}ê°œ)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = create_rolling_and_lag_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± ì¤‘...\n",
      "   âœ… ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ! (ì´ 29ê°œ)\n"
     ]
    }
   ],
   "source": [
    "def create_heat_demand_features(df):\n",
    "    \"\"\"\n",
    "    ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”¥ ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± ì¤‘...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'heat_demand' not in df.columns:\n",
    "        print(\"   âš ï¸ heat_demand ì»¬ëŸ¼ì´ ì—†ì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return df\n",
    "    \n",
    "    # ì§€ì‚¬ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬\n",
    "    if 'branch_id' in df.columns:\n",
    "        df = df.sort_values(['branch_id', 'datetime'])\n",
    "        \n",
    "        for branch in df['branch_id'].unique():\n",
    "            branch_mask = df['branch_id'] == branch\n",
    "            branch_data = df[branch_mask].copy()\n",
    "            \n",
    "            # 4-1. ì—´ìˆ˜ìš” Lag ë³€ìˆ˜\n",
    "            for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
    "                branch_data[f'demand_lag_{lag}'] = branch_data['heat_demand'].shift(lag)\n",
    "            \n",
    "            # 4-2. ì—´ìˆ˜ìš” Rolling í†µê³„\n",
    "            for window in [6, 12, 24, 48]:\n",
    "                rolling_demand = branch_data['heat_demand'].rolling(window=window, min_periods=1)\n",
    "                branch_data[f'demand_mean_{window}h'] = rolling_demand.mean()\n",
    "                branch_data[f'demand_std_{window}h'] = rolling_demand.std()\n",
    "                branch_data[f'demand_max_{window}h'] = rolling_demand.max()\n",
    "                branch_data[f'demand_min_{window}h'] = rolling_demand.min()\n",
    "            \n",
    "            # 4-3. ì—´ìˆ˜ìš” ë³€í™”ìœ¨ ë° ì°¨ë¶„\n",
    "            branch_data['demand_diff_1h'] = branch_data['heat_demand'].diff()\n",
    "            branch_data['demand_diff_24h'] = branch_data['heat_demand'].diff(periods=24)\n",
    "            branch_data['demand_pct_change_1h'] = branch_data['heat_demand'].pct_change()\n",
    "            \n",
    "            # 4-4. ê³„ì ˆì„± ë° ì£¼ê¸°ì„± íŠ¹ì„±\n",
    "            # ê°™ì€ ì‹œê°„ëŒ€ í‰ê·  ëŒ€ë¹„ ë¹„ìœ¨\n",
    "            hourly_avg = branch_data.groupby('hour')['heat_demand'].transform('mean')\n",
    "            branch_data['demand_vs_hourly_avg'] = branch_data['heat_demand'] / (hourly_avg + 1e-8)\n",
    "            \n",
    "            # 4-5. íš¨ìœ¨ì„± ì§€í‘œ\n",
    "            if 'HDD_18' in branch_data.columns:\n",
    "                branch_data['heating_efficiency'] = branch_data['heat_demand'] / (branch_data['HDD_18'] + 1e-8)\n",
    "            \n",
    "            if 'ta' in branch_data.columns:\n",
    "                branch_data['temp_sensitivity'] = branch_data['demand_diff_1h'] / (branch_data['ta_diff_1h'] + 1e-8)\n",
    "            \n",
    "            # ì›ë³¸ ë°ì´í„°ì— ë°˜ì˜\n",
    "            new_cols = [col for col in branch_data.columns if col.startswith('demand_') or col in ['heating_efficiency', 'temp_sensitivity']]\n",
    "            df.loc[branch_mask, new_cols] = branch_data[new_cols]\n",
    "    \n",
    "    demand_cols = len([col for col in df.columns if col.startswith('demand_') or col in ['heating_efficiency', 'temp_sensitivity']])\n",
    "    print(f\"   âœ… ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ! (ì´ {demand_cols}ê°œ)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± ì‹¤í–‰\n",
    "train_df = create_heat_demand_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì¤‘...\n",
      "   âœ… ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì™„ë£Œ! (ì´ 16ê°œ)\n"
     ]
    }
   ],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    ìƒí˜¸ì‘ìš© íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”— ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ê¸°ì˜¨ê³¼ ì‹œê°„ì˜ ìƒí˜¸ì‘ìš©\n",
    "    if 'ta' in df.columns:\n",
    "        df['ta_hour_interaction'] = df['ta'] * df['hour']\n",
    "        df['ta_month_interaction'] = df['ta'] * df['month']\n",
    "        df['ta_weekend_interaction'] = df['ta'] * df['is_weekend']\n",
    "    \n",
    "    # HDDì™€ ì‹œê°„ì˜ ìƒí˜¸ì‘ìš©\n",
    "    if 'HDD_18' in df.columns:\n",
    "        df['hdd_hour_interaction'] = df['HDD_18'] * df['hour']\n",
    "        df['hdd_weekend_interaction'] = df['HDD_18'] * df['is_weekend']\n",
    "    \n",
    "    # ê¸°ì˜¨ê³¼ ìŠµë„ì˜ ìƒí˜¸ì‘ìš©\n",
    "    if 'ta' in df.columns and 'hm' in df.columns:\n",
    "        df['ta_humidity_interaction'] = df['ta'] * df['hm']\n",
    "    \n",
    "    # í’ì†ê³¼ ê¸°ì˜¨ì˜ ìƒí˜¸ì‘ìš©\n",
    "    if 'ws' in df.columns and 'ta' in df.columns:\n",
    "        df['wind_temp_interaction'] = df['ws'] * df['ta']\n",
    "    \n",
    "    # ê°•ìˆ˜ì™€ ê³„ì ˆì˜ ìƒí˜¸ì‘ìš©\n",
    "    if 'rn_day' in df.columns:\n",
    "        df['rain_season_interaction'] = df['rn_day'] * df['season']\n",
    "        df['rain_weekend_interaction'] = df['rn_day'] * df['is_weekend']\n",
    "    \n",
    "    # ì§€ì‚¬ë³„ ìƒí˜¸ì‘ìš© (ìƒìœ„ 3ê°œ ì§€ì‚¬ë§Œ)\n",
    "    if 'branch_id' in df.columns and 'ta' in df.columns:\n",
    "        top_branches = df['branch_id'].value_counts().head(3).index.tolist()\n",
    "        for branch in top_branches:\n",
    "            branch_dummy = (df['branch_id'] == branch).astype(int)\n",
    "            df[f'branch_{branch}_temp'] = branch_dummy * df['ta']\n",
    "            if 'HDD_18' in df.columns:\n",
    "                df[f'branch_{branch}_hdd'] = branch_dummy * df['HDD_18']\n",
    "    \n",
    "    interaction_cols = [col for col in df.columns if 'interaction' in col or col.startswith('branch_')]\n",
    "    print(f\"   âœ… ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì™„ë£Œ! (ì´ {len(interaction_cols)}ê°œ)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì‹¤í–‰\n",
    "train_df = create_interaction_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ ìµœì¢… ì „ì²˜ë¦¬ ë° ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ìµœì¢… íŠ¹ì„± ì¤€ë¹„ ì¤‘...\n",
      "   ğŸ”§ ê²°ì¸¡ì¹˜ ìµœì¢… ì²˜ë¦¬...\n",
      "   ğŸ“ íŠ¹ì„± ì„ íƒ...\n",
      "   âœ… ì´ 114ê°œ íŠ¹ì„± ì„ íƒ\n",
      "   ğŸ“‹ ë²”ì£¼í˜• ë³€ìˆ˜: ['branch_id']\n",
      "   ğŸ”„ ì›í•« ì¸ì½”ë”© ìˆ˜í–‰...\n",
      "   âœ… ì›í•« ì¸ì½”ë”© ì™„ë£Œ: 3ê°œ ë”ë¯¸ ë³€ìˆ˜ ìƒì„±\n",
      "   ğŸ“ ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰...\n",
      "   ğŸ”§ ë¬´í•œê°’ ë° ê·¹ê°’ ì²˜ë¦¬...\n",
      "   âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ! ìµœì¢… ë°ì´í„° í˜•íƒœ: (52557, 117)\n"
     ]
    }
   ],
   "source": [
    "def prepare_final_features(df, target_col='heat_demand'):\n",
    "    \"\"\"\n",
    "    ìµœì¢… íŠ¹ì„± ì¤€ë¹„, ì¸ì½”ë”©, ìŠ¤ì¼€ì¼ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ ìµœì¢… íŠ¹ì„± ì¤€ë¹„ ì¤‘...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 6-1. ê²°ì¸¡ì¹˜ ìµœì¢… ì²˜ë¦¬\n",
    "    print(\"   ğŸ”§ ê²°ì¸¡ì¹˜ ìµœì¢… ì²˜ë¦¬...\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # 6-2. íŠ¹ì„± ì„ íƒ\n",
    "    print(\"   ğŸ“ íŠ¹ì„± ì„ íƒ...\")\n",
    "    \n",
    "    # ì œì™¸í•  ì»¬ëŸ¼ë“¤\n",
    "    exclude_cols = ['tm', 'datetime', 'year']  # ê¸°ë³¸ ì œì™¸\n",
    "    if target_col in df.columns:\n",
    "        exclude_cols.append(target_col)  # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸\n",
    "    \n",
    "    # ì‚¬ìš©í•  íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # ë²”ì£¼í˜• ì»¬ëŸ¼ ì‹ë³„\n",
    "    categorical_cols = []\n",
    "    if 'branch_id' in feature_cols:\n",
    "        categorical_cols.append('branch_id')\n",
    "    \n",
    "    print(f\"   âœ… ì´ {len(feature_cols)}ê°œ íŠ¹ì„± ì„ íƒ\")\n",
    "    print(f\"   ğŸ“‹ ë²”ì£¼í˜• ë³€ìˆ˜: {categorical_cols}\")\n",
    "    \n",
    "    # 6-3. ì›í•« ì¸ì½”ë”©\n",
    "    print(\"   ğŸ”„ ì›í•« ì¸ì½”ë”© ìˆ˜í–‰...\")\n",
    "    if categorical_cols:\n",
    "        df_encoded = pd.get_dummies(df[feature_cols + [target_col] if target_col in df.columns else feature_cols], \n",
    "                                  columns=categorical_cols, \n",
    "                                  prefix=categorical_cols)\n",
    "        print(f\"   âœ… ì›í•« ì¸ì½”ë”© ì™„ë£Œ: {df_encoded.shape[1] - len(df[feature_cols].columns)}ê°œ ë”ë¯¸ ë³€ìˆ˜ ìƒì„±\")\n",
    "    else:\n",
    "        df_encoded = df[feature_cols + [target_col] if target_col in df.columns else feature_cols].copy()\n",
    "    \n",
    "    # 6-4. ìŠ¤ì¼€ì¼ë§\n",
    "    print(\"   ğŸ“ ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰...\")\n",
    "    \n",
    "    if target_col in df_encoded.columns:\n",
    "        feature_cols_encoded = [col for col in df_encoded.columns if col != target_col]\n",
    "        X = df_encoded[feature_cols_encoded]\n",
    "        y = df_encoded[target_col]\n",
    "    else:\n",
    "        feature_cols_encoded = df_encoded.columns.tolist()\n",
    "        X = df_encoded\n",
    "        y = None\n",
    "    \n",
    "    # ë¬´í•œê°’ ë° ê·¹ê°’ ì²˜ë¦¬\n",
    "    print(\"   ğŸ”§ ë¬´í•œê°’ ë° ê·¹ê°’ ì²˜ë¦¬...\")\n",
    "    \n",
    "    # ë¬´í•œê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # ê·¹ê°’ ì²˜ë¦¬ (99.9% ë¶„ìœ„ìˆ˜ë¡œ í´ë¦¬í•‘)\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "            # ë¶„ìœ„ìˆ˜ ê³„ì‚° (NaN ì œì™¸)\n",
    "            q99 = X[col].quantile(0.999)\n",
    "            q01 = X[col].quantile(0.001)\n",
    "            \n",
    "            # ê·¹ê°’ í´ë¦¬í•‘\n",
    "            X[col] = X[col].clip(lower=q01, upper=q99)\n",
    "    \n",
    "    # ë‚¨ì€ NaN ê°’ ì²˜ë¦¬\n",
    "    X = X.fillna(X.median()).fillna(0)\n",
    "    \n",
    "    # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜\n",
    "    X = X.astype(np.float64)\n",
    "    \n",
    "    # ìµœì¢… ë¬´í•œê°’ ì²´í¬\n",
    "    if np.any(np.isinf(X.values)) or np.any(np.isnan(X.values)):\n",
    "        print(\"   âš ï¸ ì—¬ì „íˆ ë¬´í•œê°’/NaNì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì¶”ê°€ ì²˜ë¦¬ ì¤‘...\")\n",
    "        # ë‚¨ì€ ë¬¸ì œ ì»¬ëŸ¼ í™•ì¸\n",
    "        remaining_inf_cols = X.columns[np.any(np.isinf(X.values), axis=0)]\n",
    "        remaining_nan_cols = X.columns[np.any(np.isnan(X.values), axis=0)]\n",
    "        if len(remaining_inf_cols) > 0:\n",
    "            print(f\"      - ë‚¨ì€ ë¬´í•œê°’ ì»¬ëŸ¼: {list(remaining_inf_cols)}\")\n",
    "        if len(remaining_nan_cols) > 0:\n",
    "            print(f\"      - ë‚¨ì€ NaN ì»¬ëŸ¼: {list(remaining_nan_cols)}\")\n",
    "        X = X.replace([np.inf, -np.inf, np.nan], 0)\n",
    "    # MinMax ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols_encoded, index=X.index)\n",
    "    \n",
    "    if y is not None:\n",
    "        final_df = X_scaled_df.copy()\n",
    "        final_df[target_col] = y.values\n",
    "    else:\n",
    "        final_df = X_scaled_df.copy()\n",
    "    \n",
    "    print(f\"   âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ! ìµœì¢… ë°ì´í„° í˜•íƒœ: {final_df.shape}\")\n",
    "    \n",
    "    return final_df, scaler, feature_cols_encoded\n",
    "\n",
    "# ìµœì¢… ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "final_train_df, scaler, feature_columns = prepare_final_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ ë°ì´í„° ë¶„í•  (í•„ìš” ì‹œ ì§„í–‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì—°ë„ë³„ ë°ì´í„° ë¶„í•  ì¤‘...\n",
      "   âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:\n",
      "      í›ˆë ¨ ë°ì´í„°: (21022, 117)\n",
      "      ê²€ì¦ ë°ì´í„°: (5255, 117)\n",
      "      í…ŒìŠ¤íŠ¸ ë°ì´í„°: (26280, 117)\n"
     ]
    }
   ],
   "source": [
    "def split_data_by_year(df, train_years=[2021], test_years=[2022], val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    ì—°ë„ë³„ë¡œ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š ì—°ë„ë³„ ë°ì´í„° ë¶„í•  ì¤‘...\")\n",
    "    \n",
    "    # ì›ë³¸ ë°ì´í„°ì—ì„œ ì—°ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    df_with_year = df.copy()\n",
    "    if 'year' not in df_with_year.columns and 'datetime' in train_df.columns:\n",
    "        df_with_year['year'] = train_df['datetime'].dt.year\n",
    "    elif 'year' not in df_with_year.columns and 'tm' in train_df.columns:\n",
    "        df_with_year['year'] = train_df['tm'].astype(str).str[:4].astype(int)\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„°\n",
    "    train_mask = df_with_year['year'].isin(train_years)\n",
    "    train_data = df[train_mask].copy()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "    test_mask = df_with_year['year'].isin(test_years)\n",
    "    test_data = df[test_mask].copy()\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„° (í›ˆë ¨ ë°ì´í„°ì—ì„œ ì‹œê°„ìˆœìœ¼ë¡œ ë¶„í• )\n",
    "    if val_ratio > 0:\n",
    "        val_size = int(len(train_data) * val_ratio)\n",
    "        val_data = train_data.iloc[-val_size:].copy()\n",
    "        train_data = train_data.iloc[:-val_size].copy()\n",
    "    else:\n",
    "        val_data = None\n",
    "    \n",
    "    print(f\"   âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "    print(f\"      í›ˆë ¨ ë°ì´í„°: {train_data.shape}\")\n",
    "    if val_data is not None:\n",
    "        print(f\"      ê²€ì¦ ë°ì´í„°: {val_data.shape}\")\n",
    "    print(f\"      í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_data.shape}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  ì‹¤í–‰\n",
    "train_data, val_data, test_data = split_data_by_year(final_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š ìµœì¢… ë°ì´í„° ì •ë³´:\n",
      "   í›ˆë ¨ ë°ì´í„°: (21022, 117)\n",
      "   í…ŒìŠ¤íŠ¸ ë°ì´í„°: (26280, 117)\n",
      "\n",
      "ğŸ“ˆ ìƒì„±ëœ íŠ¹ì„±:\n",
      "   ì‹œê°„ íŠ¹ì„±: 28ê°œ\n",
      "   ê¸°ìƒ ê¸°ë³¸: 8ê°œ\n",
      "   ê¸°ìƒ íŒŒìƒ: 6ê°œ\n",
      "   Rolling í†µê³„: 40ê°œ\n",
      "   Lag ë³€ìˆ˜: 13ê°œ\n",
      "   ìƒí˜¸ì‘ìš©: 18ê°œ\n",
      "   ì—´ìˆ˜ìš” ê´€ë ¨: 27ê°œ\n",
      "\n",
      "ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤€ë¹„ ì™„ë£Œ\n",
      "   - train_data: ëª¨ë¸ í›ˆë ¨ìš©\n",
      "   - test_data: ìµœì¢… í‰ê°€ìš©\n",
      "   - scaler: ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ (ì˜ˆì¸¡ì‹œ ì—­ë³€í™˜ í•„ìš”)\n",
      "   - feature_columns: íŠ¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
      "\n",
      "ğŸš€ ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ëª¨ë¸ë§ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ë°ì´í„° ì •ë³´:\")\n",
    "print(f\"   í›ˆë ¨ ë°ì´í„°: {train_data.shape}\")\n",
    "# if val_data is not None:\n",
    "#     print(f\"   ê²€ì¦ ë°ì´í„°: {val_data.shape}\")\n",
    "if test_data is not None:\n",
    "    print(f\"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_data.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ìƒì„±ëœ íŠ¹ì„±:\")\n",
    "feature_types = {\n",
    "    'ì‹œê°„ íŠ¹ì„±': [col for col in feature_columns if any(keyword in col for keyword in ['hour', 'month', 'day', 'week', 'season'])],\n",
    "    'ê¸°ìƒ ê¸°ë³¸': [col for col in feature_columns if col in ['ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi']],\n",
    "    'ê¸°ìƒ íŒŒìƒ': [col for col in feature_columns if any(keyword in col for keyword in ['HDD', 'CDD', 'wind_chill', 'discomfort'])],\n",
    "    'Rolling í†µê³„': [col for col in feature_columns if any(keyword in col for keyword in ['mean_', 'std_', 'max_', 'min_', 'sum_'])],\n",
    "    'Lag ë³€ìˆ˜': [col for col in feature_columns if 'lag_' in col],\n",
    "    'ìƒí˜¸ì‘ìš©': [col for col in feature_columns if 'interaction' in col or col.startswith('branch_')],\n",
    "    'ì—´ìˆ˜ìš” ê´€ë ¨': [col for col in feature_columns if col.startswith('demand_')]\n",
    "}\n",
    "\n",
    "for feature_type, cols in feature_types.items():\n",
    "    if cols:\n",
    "        print(f\"   {feature_type}: {len(cols)}ê°œ\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"   - train_data: ëª¨ë¸ í›ˆë ¨ìš©\")\n",
    "# print(\"   - val_data: ëª¨ë¸ ê²€ì¦ìš©\") \n",
    "print(\"   - test_data: ìµœì¢… í‰ê°€ìš©\")\n",
    "print(\"   - scaler: ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ (ì˜ˆì¸¡ì‹œ ì—­ë³€í™˜ í•„ìš”)\")\n",
    "print(\"   - feature_columns: íŠ¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\")\n",
    "\n",
    "print(\"\\nğŸš€ ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ëª¨ë¸ë§ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ! ğŸ¯\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”ì‹œ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "train_data.to_csv('processed_train_data.csv', index=False)\n",
    "if val_data is not None:\n",
    "    val_data.to_csv('processed_val_data.csv', index=False)\n",
    "if test_data is not None:\n",
    "    test_data.to_csv('processed_test_data.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ! ğŸ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ: ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# X_train = train_data.drop(columns=['heat_demand'])\n",
    "# y_train = train_data['heat_demand']\n",
    "# X_val = val_data.drop(columns=['heat_demand']) if val_data is not None else None\n",
    "# y_val = val_data['heat_demand'] if val_data is not None else None\n",
    "\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# if X_val is not None:\n",
    "#     y_pred = model.predict(X_val)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "#     print(f\"Validation RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê°œì„ ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ ë°ì´í„° ë¡œë“œ...\n",
      "ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "âœ… í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: (52557, 11)\n",
      "   ì»¬ëŸ¼: ['tm', 'branch_id', 'ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand']\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: (26280, 11)\n",
      "\n",
      "2ï¸âƒ£ ê²°ì¸¡ì¹˜ ì²˜ë¦¬...\n",
      "ğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘...\n",
      "   ì»¬ëŸ¼ëª… ì •ë¦¬ ì™„ë£Œ: ['tm', 'branch_id', 'ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand']\n",
      "   ğŸ”„ -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ ì¤‘...\n",
      "   âœ… ì´ 54449ê°œì˜ -99 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
      "   â˜€ï¸ ì¼ì‚¬ëŸ‰(si) íŠ¹ë³„ ì²˜ë¦¬ ì¤‘...\n",
      "   âœ… ì•¼ê°„ì‹œê°„ëŒ€ ì¼ì‚¬ëŸ‰ 23796ê°œë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬\n",
      "   ğŸ“ˆ ì§€ì‚¬ë³„ ì„ í˜•ë³´ê°„ ì²˜ë¦¬ ì¤‘...\n",
      "   âœ… ì„ í˜•ë³´ê°„ ì™„ë£Œ\n",
      "   ğŸ‰ ëª¨ë“  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\n",
      "\n",
      "3ï¸âƒ£ ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„±...\n",
      "ğŸŒ¡ï¸ ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„± ì¤‘...\n",
      "   ğŸ“… ì‹œê°„ ê¸°ë³¸ ë³€ìˆ˜ ìƒì„±...\n",
      "   ğŸ”„ ê³„ì ˆì„± ìˆœí™˜ ë³€ìˆ˜ ìƒì„±...\n",
      "ğŸ”¥ ë‚œë°© ê´€ë ¨ ì‹œê°„ ë³€ìˆ˜ ìƒì„±...\n",
      "   âœ… ë‚œë°© ê´€ë ¨ ì‹œê°„ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ\n",
      "ğŸŒ¤ï¸ ê¸°ìƒ íŒŒìƒë³€ìˆ˜ ìƒì„±...\n",
      "   âœ… ê¸°ìƒ íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ\n",
      "\n",
      "4ï¸âƒ£ Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„±...\n",
      "ğŸ“Š Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„±...\n",
      "   âœ… Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„± ì™„ë£Œ! (ì´ 32ê°œ)\n",
      "\n",
      "5ï¸âƒ£ ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„±...\n",
      "ğŸ”¥ ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± ì¤‘...\n",
      "   âœ… ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ! (ì´ 29ê°œ)\n",
      "\n",
      "6ï¸âƒ£ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±...\n",
      "ğŸ”— ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì¤‘...\n",
      "   âœ… ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì™„ë£Œ! (ì´ 16ê°œ)\n",
      "\n",
      "7ï¸âƒ£ ìµœì¢… ì „ì²˜ë¦¬ ë° ì¸ì½”ë”©...\n",
      "ğŸ¯ ìµœì¢… íŠ¹ì„± ì¤€ë¹„ ì¤‘...\n",
      "   ğŸ”§ ê²°ì¸¡ì¹˜ ìµœì¢… ì²˜ë¦¬...\n",
      "   ğŸ“ íŠ¹ì„± ì„ íƒ...\n",
      "   âœ… ì´ 114ê°œ íŠ¹ì„± ì„ íƒ\n",
      "   ğŸ“‹ ë²”ì£¼í˜• ë³€ìˆ˜: ['branch_id']\n",
      "   ğŸ”„ ì›í•« ì¸ì½”ë”© ìˆ˜í–‰...\n",
      "   âœ… ì›í•« ì¸ì½”ë”© ì™„ë£Œ: 3ê°œ ë”ë¯¸ ë³€ìˆ˜ ìƒì„±\n",
      "   ğŸ“ ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰...\n",
      "   ğŸ”§ ë¬´í•œê°’ ë° ê·¹ê°’ ì²˜ë¦¬...\n",
      "   âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ! ìµœì¢… ë°ì´í„° í˜•íƒœ: (52557, 117)\n",
      "\n",
      "ğŸ‰ ê°œì„ ëœ ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
      "ìµœì¢… ë°ì´í„° í˜•íƒœ: (52557, 117)\n",
      "íŠ¹ì„± ê°œìˆ˜: 116\n",
      "ë¬´í•œê°’: 0ê°œ, NaN: 0ê°œ\n",
      "âœ… ë¬´í•œê°’/NaN ë¬¸ì œ í•´ê²° ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ ê°œì„ ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "print(\"ğŸš€ ê°œì„ ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "print(\"\\n1ï¸âƒ£ ë°ì´í„° ë¡œë“œ...\")\n",
    "train_df_new, test_df_new = load_data('train_heat_ABD.csv', 'test_heat_ABD.csv')\n",
    "\n",
    "# 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "print(\"\\n2ï¸âƒ£ ê²°ì¸¡ì¹˜ ì²˜ë¦¬...\")\n",
    "train_df_new = handle_missing_values(train_df_new)\n",
    "\n",
    "# 3. ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "print(\"\\n3ï¸âƒ£ ê¸°ìƒ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„±...\")\n",
    "train_df_new = create_weather_time_features(train_df_new)\n",
    "train_df_new = create_heating_related_features(train_df_new)\n",
    "train_df_new = create_weather_derived_features(train_df_new)\n",
    "\n",
    "# 4. Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„± (ê°œì„ ëœ ë²„ì „)\n",
    "print(\"\\n4ï¸âƒ£ Rolling í†µê³„ ë° ì§€ì—° ë³€ìˆ˜ ìƒì„±...\")\n",
    "train_df_new = create_rolling_and_lag_features(train_df_new)\n",
    "\n",
    "# 5. ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± (ê°œì„ ëœ ë²„ì „)\n",
    "print(\"\\n5ï¸âƒ£ ì—´ìˆ˜ìš” ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„±...\")\n",
    "train_df_new = create_heat_demand_features(train_df_new)\n",
    "\n",
    "# 6. ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±\n",
    "print(\"\\n6ï¸âƒ£ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±...\")\n",
    "train_df_new = create_interaction_features(train_df_new)\n",
    "\n",
    "# 7. ìµœì¢… ì „ì²˜ë¦¬ ë° ì¸ì½”ë”©\n",
    "print(\"\\n7ï¸âƒ£ ìµœì¢… ì „ì²˜ë¦¬ ë° ì¸ì½”ë”©...\")\n",
    "final_train_df_new, scaler_new, feature_columns_new = prepare_final_features(train_df_new)\n",
    "\n",
    "print(\"\\nğŸ‰ ê°œì„ ëœ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì¢… ë°ì´í„° í˜•íƒœ: {final_train_df_new.shape}\")\n",
    "print(f\"íŠ¹ì„± ê°œìˆ˜: {len(feature_columns_new)}\")\n",
    "\n",
    "# ë¬´í•œê°’ ìµœì¢… ì²´í¬\n",
    "inf_check = np.isinf(final_train_df_new.select_dtypes(include=[np.number]).values).sum()\n",
    "nan_check = np.isnan(final_train_df_new.select_dtypes(include=[np.number]).values).sum()\n",
    "print(f\"ë¬´í•œê°’: {inf_check}ê°œ, NaN: {nan_check}ê°œ\")\n",
    "\n",
    "if inf_check == 0 and nan_check == 0:\n",
    "    print(\"âœ… ë¬´í•œê°’/NaN ë¬¸ì œ í•´ê²° ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì—¬ì „íˆ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### (ì¶”ê°€) ğŸš¨ ë¬´í•œê°’ ë°œìƒ ì›ì¸ ë° í•´ê²°ì±… ì •ë¦¬\n",
    "\n",
    "### ğŸ“‹ **ì£¼ìš” ì›ì¸ë“¤**\n",
    "\n",
    "#### 1ï¸âƒ£ **0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì—°ì‚°**\n",
    "```python\n",
    "# ğŸ”¥ ë¬¸ì œê°€ ë˜ì—ˆë˜ ì½”ë“œë“¤:\n",
    "branch_data['demand_pct_change_1h'] = branch_data['heat_demand'].pct_change()\n",
    "branch_data['demand_vs_hourly_avg'] = branch_data['heat_demand'] / (hourly_avg + 1e-8)\n",
    "branch_data['heating_efficiency'] = branch_data['heat_demand'] / (branch_data['HDD_18'] + 1e-8)\n",
    "branch_data['temp_sensitivity'] = branch_data['demand_diff_1h'] / (branch_data['ta_diff_1h'] + 1e-8)\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™©ë“¤**\n",
    "- **`pct_change()` í•¨ìˆ˜**: ì´ì „ ê°’ì´ 0ì¼ ë•Œ ë¬´í•œê°’ ìƒì„±\n",
    "- **HDD_18ì´ 0ì¸ ê²½ìš°**: ì—¬ë¦„ì² ì— ë‚œë°©ë„ì¼ì´ 0ì´ ë˜ì–´ ë‚˜ëˆ—ì…ˆì—ì„œ ë¬¸ì œ\n",
    "- **ê¸°ì˜¨ ì°¨ë¶„ì´ 0ì¸ ê²½ìš°**: ì—°ì†ëœ ì‹œê°„ì— ê¸°ì˜¨ì´ ë™ì¼í•  ë•Œ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n",
    "- **ì‹œê°„ëŒ€ë³„ í‰ê· ì´ 0ì¸ ê²½ìš°**: íŠ¹ì • ì‹œê°„ëŒ€ì— ì—´ìˆ˜ìš”ê°€ 0ì¼ ë•Œ ë¬¸ì œ\n",
    "- **Rolling í‘œì¤€í¸ì°¨**: ë‹¨ì¼ ê°’ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ìœˆë„ìš°ì—ì„œ std() = 0\n",
    "\n",
    "### âœ… **ì ìš©ëœ í•´ê²°ì±…ë“¤**\n",
    "\n",
    "#### 1ï¸âƒ£ **ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆ ì²˜ë¦¬**\n",
    "```python\n",
    "# ê¸°ì¡´: ìœ„í—˜í•œ ë°©ì‹\n",
    "value / (denominator + 1e-8)\n",
    "\n",
    "# ê°œì„ : ì•ˆì „í•œ ë°©ì‹\n",
    "safe_denominator = np.where(np.abs(denominator) < 1e-6, 1e-6, denominator)\n",
    "result = value / safe_denominator\n",
    "result = np.clip(result, min_value, max_value)  # ê·¹ê°’ í´ë¦¬í•‘\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **í¼ì„¼íŠ¸ ë³€í™”ìœ¨ ì•ˆì „ ê³„ì‚°**\n",
    "```python\n",
    "# ê¸°ì¡´: pct_change() ì§ì ‘ ì‚¬ìš©\n",
    "branch_data['demand_pct_change_1h'] = branch_data['heat_demand'].pct_change()\n",
    "\n",
    "# ê°œì„ : ìˆ˜ë™ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ê³„ì‚°\n",
    "prev_demand = branch_data['heat_demand'].shift(1)\n",
    "demand_change = branch_data['heat_demand'] - prev_demand\n",
    "safe_prev_demand = np.where(np.abs(prev_demand) < 1e-6, 1e-6, prev_demand)\n",
    "branch_data['demand_pct_change_1h'] = np.clip(demand_change / safe_prev_demand, -10, 10)\n",
    "```\n",
    "\n",
    "#### 3ï¸âƒ£ **Rolling í†µê³„ NaN ì²˜ë¦¬**\n",
    "```python\n",
    "# í‘œì¤€í¸ì°¨ê°€ NaNì¼ ìˆ˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "branch_data[f'ta_std_{window}h'] = rolling_ta.std().fillna(0)\n",
    "```\n",
    "\n",
    "#### 4ï¸âƒ£ **ë‹¨ê³„ë³„ ë¬´í•œê°’ ì²´í¬ ë° ì²˜ë¦¬**\n",
    "```python\n",
    "# ê° ë‹¨ê³„ë§ˆë‹¤ ë¬´í•œê°’ ì²´í¬\n",
    "for col in branch_data.columns:\n",
    "    if branch_data[col].dtype in ['float64', 'float32']:\n",
    "        branch_data[col] = branch_data[col].replace([np.inf, -np.inf], np.nan)\n",
    "        branch_data[col] = branch_data[col].fillna(0)  # ë˜ëŠ” ì ì ˆí•œ ëŒ€ì²´ê°’\n",
    "```\n",
    "\n",
    "### ğŸ¯ **í•µì‹¬ êµí›ˆ**\n",
    "\n",
    "1. **1e-8 ê°™ì€ ì‘ì€ ê°’ ë”í•˜ê¸°ëŠ” ì™„ì „í•œ í•´ê²°ì±…ì´ ì•„ë‹˜**\n",
    "2. **`np.where()`ë¥¼ ì‚¬ìš©í•œ ì¡°ê±´ë¶€ ì²˜ë¦¬ê°€ ë” ì•ˆì „**\n",
    "3. **ê·¹ê°’ í´ë¦¬í•‘ìœ¼ë¡œ í˜„ì‹¤ì ì¸ ë²”ìœ„ ìœ ì§€**\n",
    "4. **ê° ë‹¨ê³„ë§ˆë‹¤ ë¬´í•œê°’/NaN ì²´í¬ í•„ìš”**\n",
    "5. **pandasì˜ `pct_change()` ê°™ì€ í•¨ìˆ˜ë„ ì£¼ì˜ í•„ìš”**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
