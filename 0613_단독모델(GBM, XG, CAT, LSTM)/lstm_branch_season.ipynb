{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9N8kUTzi6IA"
      },
      "source": [
        "# ğŸ”¥ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡: ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ LightGBM ëª¨ë¸\n",
        "\n",
        "## ğŸ“‹ ëª¨ë¸ë§ ì „ëµ\n",
        "- **ì‹œì¦Œ ë¶„í• **: Heating Season vs Non-Heating Season\n",
        "- **ë¸Œëœì¹˜ë³„ ê°œë³„ ëª¨ë¸**: ê° branch_idë§ˆë‹¤ ì „ìš© ëª¨ë¸\n",
        "- **LightGBM**: ëª¨ë“  ëª¨ë¸ì— LightGBM ì‚¬ìš©\n",
        "- **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: Optuna TPE ì‚¬ìš©\n",
        "- **ì´ ëª¨ë¸ ìˆ˜**: 38ê°œ (2ì‹œì¦Œ Ã— 19ë¸Œëœì¹˜)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6nmbT6Edi6ID"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\n"
          ]
        }
      ],
      "source": [
        "# Google Colab í™˜ê²½ í™•ì¸ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"ğŸ”¥ Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\")\n",
        "    !pip install xgboost optuna\n",
        "    from google.colab import files, drive\n",
        "    print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cc2w0lOti6IH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ TensorFlow ë²„ì „: 2.19.0\n",
            "ğŸ’» CPU ì‚¬ìš©\n"
          ]
        }
      ],
      "source": [
        "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import ìˆ˜ì •\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# TensorFlow/Keras (LSTMìš©)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# GPU ì„¤ì •\n",
        "print(f\"ğŸš€ TensorFlow ë²„ì „: {tf.__version__}\")\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {tf.config.list_physical_devices('GPU')}\")\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "else:\n",
        "    print(\"ğŸ’» CPU ì‚¬ìš©\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HsWjgGvyi6II"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
        "if IN_COLAB:\n",
        "    print(\"ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë°©ë²• ì„ íƒ:\")\n",
        "    print(\"1. ì§ì ‘ ì—…ë¡œë“œ\")\n",
        "    print(\"2. Google Drive\")\n",
        "\n",
        "    method = input(\"ì„ íƒ (1 ë˜ëŠ” 2): \")\n",
        "\n",
        "    if method == \"1\":\n",
        "        uploaded = files.upload()\n",
        "        files_list = list(uploaded.keys())\n",
        "        train_path = [f for f in files_list if 'train' in f.lower()][0]\n",
        "        test_path = [f for f in files_list if 'test' in f.lower()][0]\n",
        "    else:\n",
        "        drive.mount('/content/drive')\n",
        "        train_path = \"/content/drive/MyDrive/train_heat.csv\"\n",
        "        test_path = \"/content/drive/MyDrive/test_heat.csv\"\n",
        "else:\n",
        "    train_path = 'dataset/train_data_2122.csv'\n",
        "    test_path = 'dataset/test_data_23.csv'\n",
        "\n",
        "print(f\"âœ… íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BnYi_t0i6IJ"
      },
      "source": [
        "## 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iy4VEB5gi6IK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\n",
            "   í›ˆë ¨: (332861, 15), í…ŒìŠ¤íŠ¸: (166440, 15)\n",
            "   ê¸°ê°„: 2021-01-01 01:00:00 ~ 2023-12-31 23:00:00\n",
            "   ë¸Œëœì¹˜: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S']\n"
          ]
        }
      ],
      "source": [
        "def load_and_preprocess(train_path, test_path):\n",
        "    print(\"ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\")\n",
        "\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    def process_df(df):\n",
        "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "            df = df.drop(columns=['Unnamed: 0'])\n",
        "        df.columns = [col.replace('train_heat.', '') for col in df.columns]\n",
        "\n",
        "        # ì‹œê°„ ë³€ìˆ˜\n",
        "        df['datetime'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
        "        # df['year'] = df['datetime'].dt.year\n",
        "        df['month'] = df['datetime'].dt.month\n",
        "        # df['day'] = df['datetime'].dt.day\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "        df['dayofweek'] = df['datetime'].dt.dayofweek\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        missing_cols = ['ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi']\n",
        "        if 'heat_demand' in df.columns:\n",
        "            missing_cols.append('heat_demand')\n",
        "\n",
        "        for col in missing_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].replace(-99, np.nan)\n",
        "\n",
        "        # wd -9.9 ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        df.loc[df['wd'] == -9.9, 'wd'] = np.nan\n",
        "\n",
        "        # ì¼ì‚¬ëŸ‰ ì•¼ê°„ ì²˜ë¦¬\n",
        "        if 'si' in df.columns:\n",
        "            night_mask = (df['hour'] < 8) | (df['hour'] > 18)\n",
        "            df.loc[night_mask & df['si'].isna(), 'si'] = 0\n",
        "\n",
        "        # ì§€ì‚¬ë³„ ë³´ê°„\n",
        "        df = df.sort_values(['branch_id', 'datetime'])\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        for branch in df['branch_id'].unique():\n",
        "            mask = df['branch_id'] == branch\n",
        "            df.loc[mask, numeric_cols] = df.loc[mask, numeric_cols].interpolate().fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        return df\n",
        "\n",
        "    train_df = process_df(train_df)\n",
        "    test_df = process_df(test_df)\n",
        "\n",
        "    print(f\"   í›ˆë ¨: {train_df.shape}, í…ŒìŠ¤íŠ¸: {test_df.shape}\")\n",
        "    print(f\"   ê¸°ê°„: {train_df['datetime'].min()} ~ {test_df['datetime'].max()}\")\n",
        "    print(f\"   ë¸Œëœì¹˜: {sorted(train_df['branch_id'].unique())}\")\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_and_preprocess(train_path, test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnbWsoYUi6IK"
      },
      "source": [
        "## 2ï¸âƒ£ íŒŒìƒë³€ìˆ˜ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qmwGrilMi6IL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ: 37ê°œ ì»¬ëŸ¼\n"
          ]
        }
      ],
      "source": [
        "def create_features(df):\n",
        "    \"\"\"HDD, wind_chill, ìˆœí™˜í˜• ì¸ì½”ë”©, ë²”ì£¼í˜• ë³€ìˆ˜ ìƒì„±\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # â­ HDD (ìˆ˜ì¹˜í˜•)\n",
        "    if 'ta' in df.columns:\n",
        "        df['HDD_18'] = np.maximum(18 - df['ta'], 0)\n",
        "        df['HDD_20'] = np.maximum(20 - df['ta'], 0)\n",
        "\n",
        "    # â­ wind_chill (ìˆ˜ì¹˜í˜•)\n",
        "    if 'ta' in df.columns and 'ws' in df.columns:\n",
        "        df['wind_chill'] = np.where(\n",
        "            (df['ta'] <= 10) & (df['ws'] > 0),\n",
        "            13.12 + 0.6215 * df['ta'] - 11.37 * (df['ws'] ** 0.16) + 0.3965 * df['ta'] * (df['ws'] ** 0.16),\n",
        "            df['ta']\n",
        "        )\n",
        "\n",
        "    # â­ heating_season (ë²”ì£¼í˜•)\n",
        "    df['heating_season'] = df['month'].isin([10, 11, 12, 1, 2, 3, 4]).astype(int)\n",
        "\n",
        "    # ì‹œê°„ëŒ€ ë²”ì£¼í˜•\n",
        "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
        "    df['is_peak_morning'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
        "    df['is_peak_evening'] = ((df['hour'] >= 18) & (df['hour'] <= 22)).astype(int)\n",
        "    df['is_night'] = ((df['hour'] >= 23) | (df['hour'] <= 5)).astype(int)\n",
        "\n",
        "    # í”¼í¬ì‹œê°„ í†µí•©\n",
        "    df['peak_time_category'] = 0\n",
        "    df.loc[df['is_peak_morning'] == 1, 'peak_time_category'] = 1\n",
        "    df.loc[df['is_peak_evening'] == 1, 'peak_time_category'] = 2\n",
        "    df.loc[df['is_night'] == 1, 'peak_time_category'] = 3\n",
        "\n",
        "    # â­ ê¸°ì˜¨ ë²”ì£¼ (ë²”ì£¼í˜•)\n",
        "    if 'ta' in df.columns:\n",
        "        df['temp_category'] = pd.cut(df['ta'],\n",
        "                                   bins=[-np.inf, 0, 10, 20, 30, np.inf],\n",
        "                                   labels=[0, 1, 2, 3, 4]).astype(int)\n",
        "\n",
        "    # â­ ê°•ìˆ˜ ê°•ë„ (ë²”ì£¼í˜•)\n",
        "    if 'rn_day' in df.columns:\n",
        "        df['rain_intensity'] = pd.cut(df['rn_day'],\n",
        "                                   bins=[-1, 0, 1, 5, 10, np.inf],\n",
        "                                   labels=[0, 1, 2, 3, 4]).astype(int)\n",
        "\n",
        "    # â­ ìˆœí™˜í˜• ì¸ì½”ë”© (ì‹œê°„ cos, sin)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
        "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
        "\n",
        "    # ì‹œê°„ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜\n",
        "    df['hour_squared'] = df['hour'] ** 2\n",
        "    df['month_day_interaction'] = df['month'] * df['datetime'].dt.day\n",
        "    \n",
        "    # ê¸°ì˜¨ ê´€ë ¨ íŒŒìƒë³€ìˆ˜\n",
        "    if 'ta' in df.columns:\n",
        "        df['ta_squared'] = df['ta'] ** 2\n",
        "        df['ta_cubed'] = df['ta'] ** 3\n",
        "    \n",
        "    # ìŠµë„ì™€ ê¸°ì˜¨ ìƒí˜¸ì‘ìš©\n",
        "    if 'hm' in df.columns and 'ta' in df.columns:\n",
        "        df['hm_ta_interaction'] = df['hm'] * df['ta']\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
        "train_df = create_features(train_df)\n",
        "test_df = create_features(test_df)\n",
        "\n",
        "print(f\"âœ… íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ: {train_df.shape[1]}ê°œ ì»¬ëŸ¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BnrDxhRi6IM"
      },
      "source": [
        "## 3ï¸âƒ£ ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ ë°ì´í„° ë¶„í• "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jmdSkH3Ci6IN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š ë°ì´í„° ë¶„í•  ì •ë³´:\n",
            "   ë¸Œëœì¹˜: 19ê°œ - ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S']\n",
            "   ì‹œì¦Œ: 2ê°œ - ['ë¹„ë‚œë°©', 'ë‚œë°©']\n",
            "   ì´ ì¡°í•©: 38ê°œ\n",
            "   ë¹„ë‚œë°©_A: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_B: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_C: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_D: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_E: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_F: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_G: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_H: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_I: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_J: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_K: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_L: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_M: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_N: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_O: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_P: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_Q: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_R: 7,344ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_S: 7,344ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_A: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_B: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_C: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_D: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_E: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_F: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_G: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_H: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_I: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_J: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_K: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_L: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_M: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_N: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_O: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_P: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_Q: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_R: 10,175ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_S: 10,175ê°œ ë°ì´í„°\n",
            "ğŸ“Š ë°ì´í„° ë¶„í•  ì •ë³´:\n",
            "   ë¸Œëœì¹˜: 19ê°œ - ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S']\n",
            "   ì‹œì¦Œ: 2ê°œ - ['ë¹„ë‚œë°©', 'ë‚œë°©']\n",
            "   ì´ ì¡°í•©: 38ê°œ\n",
            "   ë¹„ë‚œë°©_A: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_B: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_C: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_D: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_E: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_F: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_G: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_H: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_I: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_J: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_K: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_L: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_M: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_N: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_O: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_P: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_Q: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_R: 3,672ê°œ ë°ì´í„°\n",
            "   ë¹„ë‚œë°©_S: 3,672ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_A: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_B: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_C: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_D: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_E: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_F: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_G: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_H: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_I: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_J: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_K: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_L: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_M: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_N: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_O: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_P: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_Q: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_R: 5,088ê°œ ë°ì´í„°\n",
            "   ë‚œë°©_S: 5,088ê°œ ë°ì´í„°\n",
            "\n",
            "âœ… í›ˆë ¨ ë°ì´í„°: 38ê°œ ë¶„í• \n",
            "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: 38ê°œ ë¶„í• \n"
          ]
        }
      ],
      "source": [
        "# ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ ë°ì´í„° ë¶„í• \n",
        "def split_by_season_and_branch(df):\n",
        "    data_splits = {}\n",
        "    \n",
        "    branches = sorted(df['branch_id'].unique())\n",
        "    seasons = [0, 1]  # 0: ë¹„ë‚œë°©ì‹œì¦Œ, 1: ë‚œë°©ì‹œì¦Œ\n",
        "    season_names = {0: 'ë¹„ë‚œë°©', 1: 'ë‚œë°©'}\n",
        "    \n",
        "    print(f\"ğŸ“Š ë°ì´í„° ë¶„í•  ì •ë³´:\")\n",
        "    print(f\"   ë¸Œëœì¹˜: {len(branches)}ê°œ - {branches}\")\n",
        "    print(f\"   ì‹œì¦Œ: {len(seasons)}ê°œ - {[season_names[s] for s in seasons]}\")\n",
        "    print(f\"   ì´ ì¡°í•©: {len(branches) * len(seasons)}ê°œ\")\n",
        "    \n",
        "    for season in seasons:\n",
        "        for branch in branches:\n",
        "            key = f\"{season_names[season]}_{branch}\"\n",
        "            \n",
        "            # ì‹œì¦Œê³¼ ë¸Œëœì¹˜ë¡œ í•„í„°ë§\n",
        "            subset = df[(df['heating_season'] == season) & (df['branch_id'] == branch)].copy()\n",
        "            \n",
        "            if len(subset) > 0:\n",
        "                data_splits[key] = subset\n",
        "                print(f\"   {key}: {len(subset):,}ê°œ ë°ì´í„°\")\n",
        "            else:\n",
        "                print(f\"   {key}: ë°ì´í„° ì—†ìŒ âš ï¸\")\n",
        "    \n",
        "    return data_splits\n",
        "\n",
        "# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \n",
        "train_splits = split_by_season_and_branch(train_df)\n",
        "test_splits = split_by_season_and_branch(test_df)\n",
        "\n",
        "print(f\"\\nâœ… í›ˆë ¨ ë°ì´í„°: {len(train_splits)}ê°œ ë¶„í• \")\n",
        "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_splits)}ê°œ ë¶„í• \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Œ Validationì‹œ ì›”ë³„ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_temporal_split_indices(df, test_size=0.2, min_val_samples=10):\n",
        "    \"\"\"ì‹œê°„ ê¸°ë°˜ ë¶„í•  ì¸ë±ìŠ¤ ë°˜í™˜\"\"\"\n",
        "    \n",
        "    month_counts = df['month'].value_counts()\n",
        "    valid_months = month_counts[month_counts >= min_val_samples * 2].index\n",
        "    \n",
        "    if len(valid_months) < 3:\n",
        "        # í´ë°±: ê¸°ì¡´ ë°©ì‹\n",
        "        split_idx = int(len(df) * (1 - test_size))\n",
        "        return df.index[:split_idx], df.index[split_idx:]\n",
        "    \n",
        "    train_indices = []\n",
        "    val_indices = []\n",
        "    \n",
        "    for month in valid_months:\n",
        "        month_data = df[df['month'] == month].sort_values('datetime')\n",
        "        val_size = max(min_val_samples, int(len(month_data) * test_size))\n",
        "        \n",
        "        train_indices.extend(month_data.iloc[:-val_size].index)\n",
        "        val_indices.extend(month_data.iloc[-val_size:].index)\n",
        "    \n",
        "    print(f\"      ğŸ“… ì›”ë³„ ë¶„í• : {len(valid_months)}ê°œì›”, ê²€ì¦ {len(val_indices)}ê°œ\")\n",
        "    \n",
        "    return train_indices, val_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# *. ğŸ”¥ ë¸Œëœì¹˜ ì„ ë³„ í•„í„°ë§ í•¨ìˆ˜ (ì¤‘ê°„ ì¶”ê°€)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. íƒ€ê²Ÿ ë¸Œëœì¹˜ ì •ì˜ ë° ë°ì´í„° í•„í„°ë§ í•¨ìˆ˜\n",
        "TARGET_BRANCHES = ['A', 'B', 'C', 'D', 'G', 'H']\n",
        "TARGET_SEASON = 'heating'  # ë‚œë°©ì‹œì¦Œë§Œ\n",
        "\n",
        "def filter_target_splits(splits_dict, target_branches=TARGET_BRANCHES, target_season='ë‚œë°©'):\n",
        "    \"\"\"ê¸°ì¡´ train_splits/test_splitsì—ì„œ íƒ€ê²Ÿ ë¸Œëœì¹˜ë§Œ í•„í„°ë§\"\"\"\n",
        "    \n",
        "    print(f\"ğŸ¯ íƒ€ê²Ÿ ë¸Œëœì¹˜ í•„í„°ë§: {target_season}ì‹œì¦Œ {target_branches}\")\n",
        "    \n",
        "    filtered_splits = {}\n",
        "    \n",
        "    for key, data in splits_dict.items():\n",
        "        # key í˜•íƒœ: \"ë‚œë°©_A\", \"ë¹„ë‚œë°©_B\" ë“±\n",
        "        season, branch = key.split('_')\n",
        "        \n",
        "        if season == target_season and branch in target_branches:\n",
        "            filtered_splits[key] = data\n",
        "            print(f\"   âœ… {key}: {len(data):,}ê°œ ë°ì´í„°\")\n",
        "        else:\n",
        "            print(f\"   â­ï¸ {key}: ê±´ë„ˆë›°ê¸°\")\n",
        "    \n",
        "    print(f\"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(filtered_splits)}ê°œ ëª¨ë¸ (ê¸°ì¡´ {len(splits_dict)}ê°œ)\")\n",
        "    return filtered_splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# *. LSTM ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_lstm_sequences(X, y, sequence_length=24):\n",
        "    \"\"\"LSTMìš© ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    \n",
        "    for i in range(sequence_length, len(X)):\n",
        "        X_seq.append(X.iloc[i-sequence_length:i].values)\n",
        "        y_seq.append(y.iloc[i])\n",
        "    \n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "def create_branch_lstm_data(df, feature_cols, target_col, sequence_length=24):\n",
        "    \"\"\"ë¸Œëœì¹˜ë³„ LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\"\"\"\n",
        "    # ì‹œê°„ìˆœ ì •ë ¬\n",
        "    df_sorted = df.sort_values('datetime').copy()\n",
        "    \n",
        "    X = df_sorted[feature_cols]\n",
        "    y = df_sorted[target_col]\n",
        "    \n",
        "    if len(X) > sequence_length:\n",
        "        X_seq, y_seq = create_lstm_sequences(X, y, sequence_length)\n",
        "        print(f\"      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: {len(X_seq)}ê°œ (ì›ë³¸ {len(X)}ê°œ)\")\n",
        "        return X_seq, y_seq\n",
        "    else:\n",
        "        print(f\"      âš ï¸ ë°ì´í„° ë¶€ì¡±: {len(X)}ê°œ (ìµœì†Œ {sequence_length+1}ê°œ í•„ìš”)\")\n",
        "        return np.array([]), np.array([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmL7oLW1i6IN"
      },
      "source": [
        "## 4ï¸âƒ£ LSTM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LSTM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "class OptimizedLSTMModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "        self.feature_cols = None\n",
        "        self.study = None\n",
        "        self.best_score = None\n",
        "        self.sequence_length = 24\n",
        "        self.scaler_X = MinMaxScaler()\n",
        "        self.scaler_y = MinMaxScaler()\n",
        "        \n",
        "    def define_feature_columns(self, df):\n",
        "        \"\"\"íŠ¹ì„± ì»¬ëŸ¼ ì •ì˜\"\"\"\n",
        "        exclude_cols = [\n",
        "            'tm', 'datetime', 'year', 'heat_demand', 'branch_id'\n",
        "        ]\n",
        "        \n",
        "        self.feature_cols = [col for col in df.columns \n",
        "                           if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]\n",
        "        \n",
        "        print(f\"   ğŸ“‹ {self.model_name}: {len(self.feature_cols)}ê°œ íŠ¹ì„± ì‚¬ìš©\")\n",
        "        return self.feature_cols\n",
        "    \n",
        "    def get_temporal_split_indices_lstm(self, df, test_size=0.2):\n",
        "        \"\"\"LSTMìš© ì‹œê°„ ê¸°ë°˜ ë¶„í•  (ìˆœì°¨ì„± ìœ ì§€)\"\"\"\n",
        "        df_sorted = df.sort_values('datetime')\n",
        "        split_idx = int(len(df_sorted) * (1 - test_size))\n",
        "        \n",
        "        train_indices = df_sorted.index[:split_idx]\n",
        "        val_indices = df_sorted.index[split_idx:]\n",
        "        \n",
        "        print(f\"      ğŸ“… ì‹œê³„ì—´ ë¶„í• : í›ˆë ¨ {len(train_indices)}ê°œ, ê²€ì¦ {len(val_indices)}ê°œ\")\n",
        "        \n",
        "        return train_indices, val_indices\n",
        "    \n",
        "    def build_lstm_model(self, input_shape, trial=None):\n",
        "        \"\"\"LSTM ëª¨ë¸ êµ¬ì¡° ì •ì˜\"\"\"\n",
        "        if trial is not None:\n",
        "            lstm_units_1 = trial.suggest_int('lstm_units_1', 32, 128)\n",
        "            lstm_units_2 = trial.suggest_int('lstm_units_2', 16, 64)\n",
        "            dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "            learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "            l2_reg = trial.suggest_float('l2_reg', 1e-6, 1e-3, log=True)\n",
        "        else:\n",
        "            lstm_units_1 = 64\n",
        "            lstm_units_2 = 32\n",
        "            dropout_rate = 0.3\n",
        "            learning_rate = 0.001\n",
        "            l2_reg = 1e-4\n",
        "        \n",
        "        model = Sequential([\n",
        "            LSTM(lstm_units_1, \n",
        "                 return_sequences=True,\n",
        "                 input_shape=input_shape,\n",
        "                 kernel_regularizer=l2(l2_reg)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(dropout_rate),\n",
        "            \n",
        "            LSTM(lstm_units_2,\n",
        "                 return_sequences=False,\n",
        "                 kernel_regularizer=l2(l2_reg)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(dropout_rate),\n",
        "            \n",
        "            Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
        "            Dropout(dropout_rate * 0.5),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "        \n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "        \n",
        "        return model, {\n",
        "            'lstm_units_1': lstm_units_1,\n",
        "            'lstm_units_2': lstm_units_2,\n",
        "            'dropout_rate': dropout_rate,\n",
        "            'learning_rate': learning_rate,\n",
        "            'l2_reg': l2_reg\n",
        "        }\n",
        "    \n",
        "    def objective(self, trial, X_train_seq, y_train_seq, X_val_seq, y_val_seq):\n",
        "        \"\"\"Optuna ëª©ì  í•¨ìˆ˜\"\"\"\n",
        "        \n",
        "        input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "        model, params = self.build_lstm_model(input_shape, trial)\n",
        "        \n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        model.fit(\n",
        "            X_train_seq, y_train_seq,\n",
        "            validation_data=(X_val_seq, y_val_seq),\n",
        "            epochs=50,  # ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ì¶•ì†Œ\n",
        "            batch_size=32,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        y_pred_scaled = model.predict(X_val_seq, verbose=0)\n",
        "        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "        y_val_orig = self.scaler_y.inverse_transform(y_val_seq.reshape(-1, 1)).flatten()\n",
        "        \n",
        "        rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred))\n",
        "        \n",
        "        del model\n",
        "        tf.keras.backend.clear_session()\n",
        "        \n",
        "        return rmse\n",
        "    \n",
        "    def fit(self, df, target_col='heat_demand', n_trials=5):\n",
        "        \"\"\"ëª¨ë¸ í›ˆë ¨ (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)\"\"\"\n",
        "        print(f\"\\nğŸ” {self.model_name} LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "        \n",
        "        if len(df) < self.sequence_length * 3:\n",
        "            print(f\"   âš ï¸ ë°ì´í„° ë¶€ì¡± ({len(df)}ê°œ) - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
        "            self._fit_basic_model(df, target_col)\n",
        "            return\n",
        "        \n",
        "        # 1. ë¶„í•  ì¸ë±ìŠ¤ êµ¬í•˜ê¸°\n",
        "        train_indices, val_indices = self.get_temporal_split_indices_lstm(df, test_size=0.2)\n",
        "        \n",
        "        # 2. íŠ¹ì„± ì»¬ëŸ¼ ì •ì˜\n",
        "        self.define_feature_columns(df)\n",
        "        \n",
        "        # 3. ë°ì´í„° ì¤€ë¹„\n",
        "        X = df[self.feature_cols].copy()\n",
        "        y = df[target_col].copy()\n",
        "        \n",
        "        # ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ…\n",
        "        self.scaler_X.fit(X)\n",
        "        self.scaler_y.fit(y.values.reshape(-1, 1))\n",
        "        \n",
        "        # 4. í›ˆë ¨/ê²€ì¦ ë¶„í• \n",
        "        train_df = df.loc[train_indices]\n",
        "        val_df = df.loc[val_indices]\n",
        "        \n",
        "        # 5. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\n",
        "        X_train_seq, y_train_seq = create_branch_lstm_data(\n",
        "            train_df, self.feature_cols, target_col, self.sequence_length\n",
        "        )\n",
        "        X_val_seq, y_val_seq = create_branch_lstm_data(\n",
        "            val_df, self.feature_cols, target_col, self.sequence_length\n",
        "        )\n",
        "        \n",
        "        if len(X_train_seq) == 0 or len(X_val_seq) == 0:\n",
        "            print(f\"   âš ï¸ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨ - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
        "            self._fit_basic_model(df, target_col)\n",
        "            return\n",
        "        \n",
        "        # 6. ì‹œí€€ìŠ¤ ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
        "        X_train_seq_scaled = np.zeros_like(X_train_seq)\n",
        "        X_val_seq_scaled = np.zeros_like(X_val_seq)\n",
        "        \n",
        "        for i in range(X_train_seq.shape[0]):\n",
        "            X_train_seq_scaled[i] = self.scaler_X.transform(X_train_seq[i])\n",
        "        for i in range(X_val_seq.shape[0]):\n",
        "            X_val_seq_scaled[i] = self.scaler_X.transform(X_val_seq[i])\n",
        "            \n",
        "        y_train_seq_scaled = self.scaler_y.transform(y_train_seq.reshape(-1, 1)).flatten()\n",
        "        y_val_seq_scaled = self.scaler_y.transform(y_val_seq.reshape(-1, 1)).flatten()\n",
        "        \n",
        "        print(f\"      ğŸ“Š ì‹œí€€ìŠ¤ í›ˆë ¨: {len(X_train_seq_scaled):,}ê°œ, ê²€ì¦: {len(X_val_seq_scaled):,}ê°œ\")\n",
        "        \n",
        "        # Optuna ìµœì í™”\n",
        "        print(f\"   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: \", end=\"\", flush=True)\n",
        "        \n",
        "        try:\n",
        "            optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "            \n",
        "            self.study = optuna.create_study(\n",
        "                direction='minimize',\n",
        "                sampler=TPESampler(seed=42),\n",
        "                study_name=f\"lstm_{self.model_name}\"\n",
        "            )\n",
        "            \n",
        "            def progress_callback(study, trial):\n",
        "                print(f\"\\r   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: {len(study.trials)}/{n_trials} (Best RMSE: {study.best_value:.4f})\", end=\"\", flush=True)\n",
        "            \n",
        "            self.study.optimize(\n",
        "                lambda trial: self.objective(trial, X_train_seq_scaled, y_train_seq_scaled, \n",
        "                                           X_val_seq_scaled, y_val_seq_scaled),\n",
        "                n_trials=n_trials,\n",
        "                callbacks=[progress_callback]\n",
        "            )\n",
        "            \n",
        "            print()\n",
        "            \n",
        "            if len(self.study.trials) > 0 and self.study.best_trial is not None:\n",
        "                self.best_score = self.study.best_value\n",
        "                self.best_params = self.study.best_params.copy()\n",
        "                \n",
        "                # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
        "                input_shape = (X_train_seq_scaled.shape[1], X_train_seq_scaled.shape[2])\n",
        "                self.model, _ = self.build_lstm_model(input_shape)\n",
        "                \n",
        "                # ì „ì²´ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ í›ˆë ¨\n",
        "                X_full_seq, y_full_seq = create_branch_lstm_data(\n",
        "                    df, self.feature_cols, target_col, self.sequence_length\n",
        "                )\n",
        "                \n",
        "                X_full_seq_scaled = np.zeros_like(X_full_seq)\n",
        "                for i in range(X_full_seq.shape[0]):\n",
        "                    X_full_seq_scaled[i] = self.scaler_X.transform(X_full_seq[i])\n",
        "                y_full_seq_scaled = self.scaler_y.transform(y_full_seq.reshape(-1, 1)).flatten()\n",
        "                \n",
        "                early_stopping = EarlyStopping(\n",
        "                    monitor='val_loss', patience=15, restore_best_weights=True, verbose=0\n",
        "                )\n",
        "                \n",
        "                self.model.fit(\n",
        "                    X_full_seq_scaled, y_full_seq_scaled,\n",
        "                    validation_data=(X_val_seq_scaled, y_val_seq_scaled),\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=0\n",
        "                )\n",
        "                \n",
        "                # ì„±ëŠ¥ ì •ë³´\n",
        "                val_pred_scaled = self.model.predict(X_val_seq_scaled, verbose=0)\n",
        "                val_pred = self.scaler_y.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()\n",
        "                y_val_orig = self.scaler_y.inverse_transform(y_val_seq_scaled.reshape(-1, 1)).flatten()\n",
        "                val_rmse = np.sqrt(mean_squared_error(y_val_orig, val_pred))\n",
        "                \n",
        "                print(f\"   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = {self.best_score:.4f}\")\n",
        "                print(f\"   ğŸ“Š ê²€ì¦ RMSE = {val_rmse:.4f}\")\n",
        "                print(f\"   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr={self.best_params.get('learning_rate', 0.001):.5f}, units={self.best_params.get('lstm_units_1', 64)}\")\n",
        "                \n",
        "            else:\n",
        "                print(f\"   âš ï¸ ìµœì í™” ì‹¤íŒ¨: ìœ íš¨í•œ trial ì—†ìŒ - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
        "                self._fit_basic_model(df, target_col)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"\\n   âš ï¸ ìµœì í™” ì‹¤íŒ¨: {str(e)[:30]}... - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
        "            self._fit_basic_model(df, target_col)\n",
        "    \n",
        "    def _fit_basic_model(self, df, target_col):\n",
        "        \"\"\"ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
        "        try:\n",
        "            X_full_seq, y_full_seq = create_branch_lstm_data(\n",
        "                df, self.feature_cols, target_col, self.sequence_length\n",
        "            )\n",
        "            \n",
        "            if len(X_full_seq) == 0:\n",
        "                print(f\"   âŒ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì‹¤íŒ¨\")\n",
        "                self.model = None\n",
        "                return\n",
        "            \n",
        "            X_full_seq_scaled = np.zeros_like(X_full_seq)\n",
        "            for i in range(X_full_seq.shape[0]):\n",
        "                X_full_seq_scaled[i] = self.scaler_X.transform(X_full_seq[i])\n",
        "            y_full_seq_scaled = self.scaler_y.transform(y_full_seq.reshape(-1, 1)).flatten()\n",
        "            \n",
        "            input_shape = (X_full_seq_scaled.shape[1], X_full_seq_scaled.shape[2])\n",
        "            self.model, _ = self.build_lstm_model(input_shape)\n",
        "            \n",
        "            self.model.fit(X_full_seq_scaled, y_full_seq_scaled, epochs=30, batch_size=32, verbose=0)\n",
        "            \n",
        "            self.best_score = None\n",
        "            self.best_params = None\n",
        "            print(f\"   ğŸ”§ ê¸°ë³¸ LSTM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(e)[:30]}...\")\n",
        "            self.model = None\n",
        "    \n",
        "    def predict(self, df):\n",
        "        \"\"\"ì˜ˆì¸¡\"\"\"\n",
        "        if self.model is None:\n",
        "            return np.full(len(df), 0)\n",
        "        \n",
        "        try:\n",
        "            X_seq, _ = create_branch_lstm_data(\n",
        "                df, self.feature_cols, 'heat_demand', self.sequence_length\n",
        "            )\n",
        "            \n",
        "            if len(X_seq) == 0:\n",
        "                return np.full(len(df), 0)\n",
        "            \n",
        "            X_seq_scaled = np.zeros_like(X_seq)\n",
        "            for i in range(X_seq.shape[0]):\n",
        "                X_seq_scaled[i] = self.scaler_X.transform(X_seq[i])\n",
        "            \n",
        "            pred_scaled = self.model.predict(X_seq_scaled, verbose=0)\n",
        "            predictions = self.scaler_y.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
        "            \n",
        "            # ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì•ì˜ ë°ì´í„°ëŠ” 0ìœ¼ë¡œ íŒ¨ë”©\n",
        "            full_predictions = np.zeros(len(df))\n",
        "            full_predictions[self.sequence_length:self.sequence_length+len(predictions)] = predictions\n",
        "            \n",
        "            return np.maximum(full_predictions, 0)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)[:30]}...\")\n",
        "            return np.full(len(df), 0)\n",
        "\n",
        "print(\"âœ… LSTM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SewcIVN_i6IR"
      },
      "source": [
        "## 5ï¸âƒ£ ì¼ë‹¨ ì¼ë¶€ë§Œ ëª¨ë¸ í›ˆë ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ 6ê°œ ë¸Œëœì¹˜ LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\n",
            "============================================================\n",
            "ğŸ¯ íƒ€ê²Ÿ ë¸Œëœì¹˜ í•„í„°ë§: ë‚œë°©ì‹œì¦Œ ['A', 'B', 'C', 'D', 'G', 'H']\n",
            "   â­ï¸ ë¹„ë‚œë°©_A: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_B: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_C: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_D: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_E: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_F: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_G: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_H: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_I: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_J: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_K: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_L: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_M: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_N: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_O: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_P: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_Q: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_R: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_S: ê±´ë„ˆë›°ê¸°\n",
            "   âœ… ë‚œë°©_A: 10,175ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_B: 10,175ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_C: 10,175ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_D: 10,175ê°œ ë°ì´í„°\n",
            "   â­ï¸ ë‚œë°©_E: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_F: ê±´ë„ˆë›°ê¸°\n",
            "   âœ… ë‚œë°©_G: 10,175ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_H: 10,175ê°œ ë°ì´í„°\n",
            "   â­ï¸ ë‚œë°©_I: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_J: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_K: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_L: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_M: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_N: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_O: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_P: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_Q: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_R: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_S: ê±´ë„ˆë›°ê¸°\n",
            "ğŸ“Š í•„í„°ë§ ê²°ê³¼: 6ê°œ ëª¨ë¸ (ê¸°ì¡´ 38ê°œ)\n",
            "ğŸ¯ íƒ€ê²Ÿ ë¸Œëœì¹˜ í•„í„°ë§: ë‚œë°©ì‹œì¦Œ ['A', 'B', 'C', 'D', 'G', 'H']\n",
            "   â­ï¸ ë¹„ë‚œë°©_A: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_B: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_C: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_D: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_E: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_F: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_G: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_H: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_I: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_J: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_K: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_L: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_M: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_N: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_O: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_P: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_Q: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_R: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë¹„ë‚œë°©_S: ê±´ë„ˆë›°ê¸°\n",
            "   âœ… ë‚œë°©_A: 5,088ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_B: 5,088ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_C: 5,088ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_D: 5,088ê°œ ë°ì´í„°\n",
            "   â­ï¸ ë‚œë°©_E: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_F: ê±´ë„ˆë›°ê¸°\n",
            "   âœ… ë‚œë°©_G: 5,088ê°œ ë°ì´í„°\n",
            "   âœ… ë‚œë°©_H: 5,088ê°œ ë°ì´í„°\n",
            "   â­ï¸ ë‚œë°©_I: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_J: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_K: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_L: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_M: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_N: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_O: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_P: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_Q: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_R: ê±´ë„ˆë›°ê¸°\n",
            "   â­ï¸ ë‚œë°©_S: ê±´ë„ˆë›°ê¸°\n",
            "ğŸ“Š í•„í„°ë§ ê²°ê³¼: 6ê°œ ëª¨ë¸ (ê¸°ì¡´ 38ê°œ)\n",
            "\n",
            "[ 1/6] ğŸ”¥ ë‚œë°©_A\n",
            "         ğŸ“Š ë°ì´í„°: 10,175ê°œ\n",
            "\n",
            "ğŸ” ë‚œë°©_A LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "      ğŸ“… ì‹œê³„ì—´ ë¶„í• : í›ˆë ¨ 8140ê°œ, ê²€ì¦ 2035ê°œ\n",
            "   ğŸ“‹ ë‚œë°©_A: 21ê°œ íŠ¹ì„± ì‚¬ìš©\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 8116ê°œ (ì›ë³¸ 8140ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 2011ê°œ (ì›ë³¸ 2035ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ í›ˆë ¨: 8,116ê°œ, ê²€ì¦: 2,011ê°œ\n",
            "   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: WARNING:tensorflow:From c:\\Users\\dlsxk\\Python_Projects\\heat_demand\\MyProject\\.venv\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: 5/5 (Best RMSE: 20.9227)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 10151ê°œ (ì›ë³¸ 10175ê°œ)\n",
            "   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = 20.9227\n",
            "   ğŸ“Š ê²€ì¦ RMSE = 17.1153\n",
            "   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr=0.00158, units=68\n",
            "         âœ… ìµœì í™” | RMSE: 20.923 | â±ï¸ 759.2ì´ˆ\n",
            "\n",
            "[ 2/6] ğŸ”¥ ë‚œë°©_B\n",
            "         ğŸ“Š ë°ì´í„°: 10,175ê°œ\n",
            "\n",
            "ğŸ” ë‚œë°©_B LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "      ğŸ“… ì‹œê³„ì—´ ë¶„í• : í›ˆë ¨ 8140ê°œ, ê²€ì¦ 2035ê°œ\n",
            "   ğŸ“‹ ë‚œë°©_B: 21ê°œ íŠ¹ì„± ì‚¬ìš©\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 8116ê°œ (ì›ë³¸ 8140ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 2011ê°œ (ì›ë³¸ 2035ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ í›ˆë ¨: 8,116ê°œ, ê²€ì¦: 2,011ê°œ\n",
            "   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: 5/5 (Best RMSE: 46.5207)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 10151ê°œ (ì›ë³¸ 10175ê°œ)\n",
            "   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = 46.5207\n",
            "   ğŸ“Š ê²€ì¦ RMSE = 36.0539\n",
            "   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr=0.00073, units=49\n",
            "         âœ… ìµœì í™” | RMSE: 46.521 | â±ï¸ 881.8ì´ˆ\n",
            "\n",
            "[ 3/6] ğŸ”¥ ë‚œë°©_C\n",
            "         ğŸ“Š ë°ì´í„°: 10,175ê°œ\n",
            "\n",
            "ğŸ” ë‚œë°©_C LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "      ğŸ“… ì‹œê³„ì—´ ë¶„í• : í›ˆë ¨ 8140ê°œ, ê²€ì¦ 2035ê°œ\n",
            "   ğŸ“‹ ë‚œë°©_C: 21ê°œ íŠ¹ì„± ì‚¬ìš©\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 8116ê°œ (ì›ë³¸ 8140ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 2011ê°œ (ì›ë³¸ 2035ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ í›ˆë ¨: 8,116ê°œ, ê²€ì¦: 2,011ê°œ\n",
            "   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: 5/5 (Best RMSE: 33.7062)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 10151ê°œ (ì›ë³¸ 10175ê°œ)\n",
            "   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = 33.7062\n",
            "   ğŸ“Š ê²€ì¦ RMSE = 27.5349\n",
            "   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr=0.00158, units=68\n",
            "         âœ… ìµœì í™” | RMSE: 33.706 | â±ï¸ 983.4ì´ˆ\n",
            "\n",
            "[ 4/6] ğŸ”¥ ë‚œë°©_D\n",
            "         ğŸ“Š ë°ì´í„°: 10,175ê°œ\n",
            "\n",
            "ğŸ” ë‚œë°©_D LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "      ğŸ“… ì‹œê³„ì—´ ë¶„í• : í›ˆë ¨ 8140ê°œ, ê²€ì¦ 2035ê°œ\n",
            "   ğŸ“‹ ë‚œë°©_D: 21ê°œ íŠ¹ì„± ì‚¬ìš©\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 8116ê°œ (ì›ë³¸ 8140ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 2011ê°œ (ì›ë³¸ 2035ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ í›ˆë ¨: 8,116ê°œ, ê²€ì¦: 2,011ê°œ\n",
            "   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: 5/5 (Best RMSE: 32.4766)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 10151ê°œ (ì›ë³¸ 10175ê°œ)\n",
            "   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = 32.4766\n",
            "   ğŸ“Š ê²€ì¦ RMSE = 25.3558\n",
            "   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr=0.00054, units=91\n",
            "         âœ… ìµœì í™” | RMSE: 32.477 | â±ï¸ 1335.0ì´ˆ\n",
            "\n",
            "[ 5/6] ğŸ”¥ ë‚œë°©_G\n",
            "         ğŸ“Š ë°ì´í„°: 10,175ê°œ\n",
            "\n",
            "ğŸ” ë‚œë°©_G LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "      ğŸ“… ì‹œê³„ì—´ ë¶„í• : í›ˆë ¨ 8140ê°œ, ê²€ì¦ 2035ê°œ\n",
            "   ğŸ“‹ ë‚œë°©_G: 21ê°œ íŠ¹ì„± ì‚¬ìš©\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 8116ê°œ (ì›ë³¸ 8140ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 2011ê°œ (ì›ë³¸ 2035ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ í›ˆë ¨: 8,116ê°œ, ê²€ì¦: 2,011ê°œ\n",
            "   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: 5/5 (Best RMSE: 29.1212)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 10151ê°œ (ì›ë³¸ 10175ê°œ)\n",
            "   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = 29.1212\n",
            "   ğŸ“Š ê²€ì¦ RMSE = 23.6967\n",
            "   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr=0.00073, units=49\n",
            "         âœ… ìµœì í™” | RMSE: 29.121 | â±ï¸ 850.5ì´ˆ\n",
            "\n",
            "[ 6/6] ğŸ”¥ ë‚œë°©_H\n",
            "         ğŸ“Š ë°ì´í„°: 10,175ê°œ\n",
            "\n",
            "ğŸ” ë‚œë°©_H LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "      ğŸ“… ì‹œê³„ì—´ ë¶„í• : í›ˆë ¨ 8140ê°œ, ê²€ì¦ 2035ê°œ\n",
            "   ğŸ“‹ ë‚œë°©_H: 21ê°œ íŠ¹ì„± ì‚¬ìš©\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 8116ê°œ (ì›ë³¸ 8140ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 2011ê°œ (ì›ë³¸ 2035ê°œ)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ í›ˆë ¨: 8,116ê°œ, ê²€ì¦: 2,011ê°œ\n",
            "   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: 5/5 (Best RMSE: 37.3062)\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 10151ê°œ (ì›ë³¸ 10175ê°œ)\n",
            "   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = 37.3062\n",
            "   ğŸ“Š ê²€ì¦ RMSE = 24.5256\n",
            "   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr=0.00054, units=91\n",
            "         âœ… ìµœì í™” | RMSE: 37.306 | â±ï¸ 711.4ì´ˆ\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ 6ê°œ LSTM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\n",
            "â±ï¸  ì´ ì†Œìš” ì‹œê°„: 92.0ë¶„\n",
            "ğŸ“Š í›ˆë ¨ ê²°ê³¼:\n",
            "   âœ… ì„±ê³µ: 6ê°œ\n",
            "   âŒ ì‹¤íŒ¨: 0ê°œ\n",
            "   ğŸ¯ ëŒ€ìƒ: ë‚œë°©ì‹œì¦Œ ['A', 'B', 'C', 'D', 'G', 'H'] ë¸Œëœì¹˜\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”¥ 6ê°œ ë¸Œëœì¹˜ LSTM ëª¨ë¸ í›ˆë ¨\n",
        "print(\"ğŸš€ 6ê°œ ë¸Œëœì¹˜ LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. íƒ€ê²Ÿ ë¸Œëœì¹˜ í•„í„°ë§\n",
        "TARGET_BRANCHES = ['A', 'B', 'C', 'D', 'G', 'H']\n",
        "filtered_train_splits = filter_target_splits(train_splits, TARGET_BRANCHES, 'ë‚œë°©')\n",
        "filtered_test_splits = filter_target_splits(test_splits, TARGET_BRANCHES, 'ë‚œë°©')\n",
        "\n",
        "# 2. 6ê°œ ëª¨ë¸ í›ˆë ¨ (ê¸°ì¡´ê³¼ ë™ì¼í•œ êµ¬ì¡°)\n",
        "models = {}\n",
        "training_results = {}\n",
        "n_trials_per_model = 5  # LSTMì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì¶•ì†Œ\n",
        "\n",
        "start_time = datetime.now()\n",
        "success_count = 0\n",
        "failed_count = 0\n",
        "\n",
        "for i, (model_key, train_data) in enumerate(filtered_train_splits.items(), 1):\n",
        "    print(f\"\\n[{i:2d}/{len(filtered_train_splits)}] ğŸ”¥ {model_key}\")\n",
        "    print(f\"         ğŸ“Š ë°ì´í„°: {len(train_data):,}ê°œ\")\n",
        "    \n",
        "    # LSTM ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\n",
        "    model = OptimizedLSTMModel(model_key)\n",
        "    \n",
        "    try:\n",
        "        model_start = datetime.now()\n",
        "        model.fit(train_data, n_trials=n_trials_per_model)\n",
        "        model_time = (datetime.now() - model_start).total_seconds()\n",
        "        \n",
        "        models[model_key] = model\n",
        "        \n",
        "        if model.best_score is not None:\n",
        "            success_count += 1\n",
        "            status = \"âœ… ìµœì í™”\"\n",
        "            score_text = f\"RMSE: {model.best_score:.3f}\"\n",
        "        else:\n",
        "            status = \"ğŸ”§ ê¸°ë³¸ëª¨ë¸\"\n",
        "            score_text = \"ê¸°ë³¸íŒŒë¼ë¯¸í„°\"\n",
        "        \n",
        "        training_results[model_key] = {\n",
        "            'data_size': len(train_data),\n",
        "            'training_time': model_time,\n",
        "            'best_score': model.best_score,\n",
        "            'best_params': model.best_params,\n",
        "            'optimization_success': model.best_score is not None\n",
        "        }\n",
        "        \n",
        "        print(f\"         {status} | {score_text} | â±ï¸ {model_time:.1f}ì´ˆ\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"         âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)[:30]}...\")\n",
        "        failed_count += 1\n",
        "        \n",
        "        # ë”ë¯¸ ëª¨ë¸ ì €ì¥\n",
        "        dummy_model = OptimizedLSTMModel(model_key)\n",
        "        dummy_model.model = None\n",
        "        models[model_key] = dummy_model\n",
        "        \n",
        "        training_results[model_key] = {\n",
        "            'data_size': len(train_data),\n",
        "            'training_time': 0,\n",
        "            'best_score': None,\n",
        "            'best_params': None,\n",
        "            'optimization_success': False,\n",
        "            'error': str(e)[:50]\n",
        "        }\n",
        "\n",
        "total_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"ğŸ‰ 6ê°œ LSTM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "print(f\"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
        "print(f\"ğŸ“Š í›ˆë ¨ ê²°ê³¼:\")\n",
        "print(f\"   âœ… ì„±ê³µ: {success_count}ê°œ\")\n",
        "print(f\"   âŒ ì‹¤íŒ¨: {failed_count}ê°œ\")\n",
        "print(f\"   ğŸ¯ ëŒ€ìƒ: ë‚œë°©ì‹œì¦Œ {TARGET_BRANCHES} ë¸Œëœì¹˜\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-cKYeui6IS"
      },
      "source": [
        "## 6ï¸âƒ£ í›ˆë ¨ ê²°ê³¼ ë¶„ì„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LSTM í›ˆë ¨ ê²°ê³¼ ë¶„ì„ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n",
            "\n",
            "ğŸ“Š LSTM í›ˆë ¨ ê²°ê³¼ ë¶„ì„\n",
            "============================================================\n",
            "âœ… ìµœì í™” ì„±ê³µ: 6ê°œ ëª¨ë¸\n",
            "ğŸ”§ ê¸°ë³¸ ëª¨ë¸: 0ê°œ ëª¨ë¸\n",
            "âŒ ì™„ì „ ì‹¤íŒ¨: 0ê°œ ëª¨ë¸\n",
            "\n",
            "ğŸ† LSTM ìµœì í™” ì„±ëŠ¥ í†µê³„:\n",
            "   í‰ê·  RMSE: 33.3423\n",
            "   ìµœì†Œ RMSE: 20.9227\n",
            "   ìµœëŒ€ RMSE: 46.5207\n",
            "   í‘œì¤€í¸ì°¨: 8.5127\n",
            "\n",
            "ğŸ“ˆ ë¸Œëœì¹˜ë³„ RMSE:\n",
            "   ë¸Œëœì¹˜ A: RMSE = 20.9227 | ë°ì´í„° = 10,175ê°œ\n",
            "   ë¸Œëœì¹˜ B: RMSE = 46.5207 | ë°ì´í„° = 10,175ê°œ\n",
            "   ë¸Œëœì¹˜ C: RMSE = 33.7062 | ë°ì´í„° = 10,175ê°œ\n",
            "   ë¸Œëœì¹˜ D: RMSE = 32.4766 | ë°ì´í„° = 10,175ê°œ\n",
            "   ë¸Œëœì¹˜ G: RMSE = 29.1212 | ë°ì´í„° = 10,175ê°œ\n",
            "   ë¸Œëœì¹˜ H: RMSE = 37.3062 | ë°ì´í„° = 10,175ê°œ\n",
            "\n",
            "ğŸ¥‡ ì„±ëŠ¥ ìˆœìœ„ (RMSE ë‚®ì€ ìˆœ):\n",
            "   1. ë¸Œëœì¹˜ A: RMSE = 20.9227\n",
            "   2. ë¸Œëœì¹˜ G: RMSE = 29.1212\n",
            "   3. ë¸Œëœì¹˜ D: RMSE = 32.4766\n",
            "   4. ë¸Œëœì¹˜ C: RMSE = 33.7062\n",
            "   5. ë¸Œëœì¹˜ H: RMSE = 37.3062\n",
            "   6. ë¸Œëœì¹˜ B: RMSE = 46.5207\n",
            "\n",
            "â±ï¸ í›ˆë ¨ ì‹œê°„ í†µê³„:\n",
            "   í‰ê·  ëª¨ë¸ë‹¹: 920.2ì´ˆ\n",
            "   ì´ í›ˆë ¨ ì‹œê°„: 92.0ë¶„\n",
            "\n",
            "ğŸ“Š ë°ì´í„° í¬ê¸° vs ì„±ëŠ¥ ë¶„ì„:\n",
            "   ë¸Œëœì¹˜ A: 10.2kê°œ â†’ RMSE 20.9227\n",
            "   ë¸Œëœì¹˜ B: 10.2kê°œ â†’ RMSE 46.5207\n",
            "   ë¸Œëœì¹˜ C: 10.2kê°œ â†’ RMSE 33.7062\n",
            "   ë¸Œëœì¹˜ D: 10.2kê°œ â†’ RMSE 32.4766\n",
            "   ë¸Œëœì¹˜ G: 10.2kê°œ â†’ RMSE 29.1212\n",
            "   ë¸Œëœì¹˜ H: 10.2kê°œ â†’ RMSE 37.3062\n",
            "\n",
            "ğŸ”¬ LSTM ëª¨ë¸ íŠ¹ì„±:\n",
            "   ì‹œí€€ìŠ¤ ê¸¸ì´: 24ì‹œê°„\n",
            "   í•™ìŠµ ëŒ€ìƒ: ë‚œë°©ì‹œì¦Œë§Œ\n",
            "   íƒ€ê²Ÿ ë¸Œëœì¹˜: A, B, C, D, G, H\n",
            "\n",
            "ğŸ¯ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„:\n",
            "   ë¸Œëœì¹˜ A: lr=0.00158, units=(68,62), dropout=0.393\n",
            "   ë¸Œëœì¹˜ B: lr=0.00073, units=(49,30), dropout=0.310\n",
            "   ë¸Œëœì¹˜ C: lr=0.00158, units=(68,62), dropout=0.393\n",
            "   ë¸Œëœì¹˜ D: lr=0.00054, units=(91,22), dropout=0.217\n",
            "   ë¸Œëœì¹˜ G: lr=0.00073, units=(49,30), dropout=0.310\n",
            "   ë¸Œëœì¹˜ H: lr=0.00054, units=(91,22), dropout=0.217\n"
          ]
        }
      ],
      "source": [
        "def analyze_lstm_training_results(training_results, models):\n",
        "    \"\"\"6ê°œ LSTM ëª¨ë¸ í›ˆë ¨ ê²°ê³¼ ë¶„ì„\"\"\"\n",
        "    print(\"\\nğŸ“Š LSTM í›ˆë ¨ ê²°ê³¼ ë¶„ì„\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # ê²°ê³¼ ì •ë¦¬\n",
        "    results_list = []\n",
        "    for model_name, result in training_results.items():\n",
        "        # best_scoreë¥¼ ì•ˆì „í•˜ê²Œ ë³€í™˜\n",
        "        best_score = result.get('best_score')\n",
        "        if best_score is not None:\n",
        "            try:\n",
        "                best_score = float(best_score)\n",
        "            except (ValueError, TypeError):\n",
        "                best_score = None\n",
        "        \n",
        "        # ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "        safe_result = {\n",
        "            'model_name': model_name,\n",
        "            'branch': model_name.split('_')[1] if '_' in model_name else 'unknown',\n",
        "            'data_size': result.get('data_size', 0),\n",
        "            'training_time': result.get('training_time', 0),\n",
        "            'best_score': best_score,\n",
        "            'optimization_success': result.get('optimization_success', False),\n",
        "            'has_model': models.get(model_name) is not None and models.get(model_name).model is not None\n",
        "        }\n",
        "        results_list.append(safe_result)\n",
        "    \n",
        "    # DataFrame ìƒì„±\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "    # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„\n",
        "    successful_models = results_df[results_df['optimization_success'] == True]\n",
        "    basic_models = results_df[(results_df['optimization_success'] == False) & (results_df['has_model'] == True)]\n",
        "    failed_models = results_df[results_df['has_model'] == False]\n",
        "    \n",
        "    print(f\"âœ… ìµœì í™” ì„±ê³µ: {len(successful_models)}ê°œ ëª¨ë¸\")\n",
        "    print(f\"ğŸ”§ ê¸°ë³¸ ëª¨ë¸: {len(basic_models)}ê°œ ëª¨ë¸\")\n",
        "    print(f\"âŒ ì™„ì „ ì‹¤íŒ¨: {len(failed_models)}ê°œ ëª¨ë¸\")\n",
        "    \n",
        "    # ì„±ê³µí•œ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ í†µê³„\n",
        "    if len(successful_models) > 0:\n",
        "        valid_scores = successful_models[successful_models['best_score'].notna()]\n",
        "        \n",
        "        if len(valid_scores) > 0:\n",
        "            print(f\"\\nğŸ† LSTM ìµœì í™” ì„±ëŠ¥ í†µê³„:\")\n",
        "            print(f\"   í‰ê·  RMSE: {valid_scores['best_score'].mean():.4f}\")\n",
        "            print(f\"   ìµœì†Œ RMSE: {valid_scores['best_score'].min():.4f}\")\n",
        "            print(f\"   ìµœëŒ€ RMSE: {valid_scores['best_score'].max():.4f}\")\n",
        "            print(f\"   í‘œì¤€í¸ì°¨: {valid_scores['best_score'].std():.4f}\")\n",
        "            \n",
        "            # ë¸Œëœì¹˜ë³„ ì„±ëŠ¥\n",
        "            print(f\"\\nğŸ“ˆ ë¸Œëœì¹˜ë³„ RMSE:\")\n",
        "            for _, row in valid_scores.iterrows():\n",
        "                print(f\"   ë¸Œëœì¹˜ {row['branch']}: RMSE = {row['best_score']:.4f} | ë°ì´í„° = {row['data_size']:,}ê°œ\")\n",
        "            \n",
        "            # ì„±ëŠ¥ ìˆœìœ„\n",
        "            print(f\"\\nğŸ¥‡ ì„±ëŠ¥ ìˆœìœ„ (RMSE ë‚®ì€ ìˆœ):\")\n",
        "            try:\n",
        "                # best_scoreë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ í™•ì‹¤íˆ ë³€í™˜\n",
        "                valid_scores_copy = valid_scores.copy()\n",
        "                valid_scores_copy['best_score'] = pd.to_numeric(valid_scores_copy['best_score'], errors='coerce')\n",
        "                \n",
        "                # NaN ì œê±° í›„ ì •ë ¬\n",
        "                top_models = valid_scores_copy.dropna(subset=['best_score']).nsmallest(6, 'best_score')\n",
        "                \n",
        "                for idx, (_, row) in enumerate(top_models.iterrows(), 1):\n",
        "                    print(f\"   {idx}. ë¸Œëœì¹˜ {row['branch']}: RMSE = {row['best_score']:.4f}\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸ ìˆœìœ„ ì •ë ¬ ì‹¤íŒ¨: {str(e)}\")\n",
        "                # ëŒ€ì•ˆ: ìˆ˜ë™ ì •ë ¬\n",
        "                try:\n",
        "                    scores_list = [(row['branch'], row['best_score']) \n",
        "                                 for _, row in valid_scores.iterrows() \n",
        "                                 if pd.notna(row['best_score'])]\n",
        "                    scores_list.sort(key=lambda x: float(x[1]))\n",
        "                    \n",
        "                    for idx, (branch, score) in enumerate(scores_list, 1):\n",
        "                        print(f\"   {idx}. ë¸Œëœì¹˜ {branch}: RMSE = {score:.4f}\")\n",
        "                except Exception as e2:\n",
        "                    print(f\"   âŒ ìˆ˜ë™ ì •ë ¬ë„ ì‹¤íŒ¨: {str(e2)}\")\n",
        "    \n",
        "    # ì‹¤íŒ¨í•œ ëª¨ë¸ ì •ë³´\n",
        "    if len(failed_models) > 0:\n",
        "        print(f\"\\nâš ï¸ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:\")\n",
        "        for _, row in failed_models.iterrows():\n",
        "            error_msg = training_results[row['model_name']].get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')\n",
        "            print(f\"   ë¸Œëœì¹˜ {row['branch']}: {error_msg}\")\n",
        "    \n",
        "    # í›ˆë ¨ ì‹œê°„ í†µê³„\n",
        "    avg_time = results_df['training_time'].mean()\n",
        "    total_time_min = results_df['training_time'].sum() / 60\n",
        "    print(f\"\\nâ±ï¸ í›ˆë ¨ ì‹œê°„ í†µê³„:\")\n",
        "    print(f\"   í‰ê·  ëª¨ë¸ë‹¹: {avg_time:.1f}ì´ˆ\")\n",
        "    print(f\"   ì´ í›ˆë ¨ ì‹œê°„: {total_time_min:.1f}ë¶„\")\n",
        "    \n",
        "    # ë°ì´í„° í¬ê¸°ë³„ ì„±ëŠ¥ ë¶„ì„\n",
        "    if len(successful_models) > 0:\n",
        "        print(f\"\\nğŸ“Š ë°ì´í„° í¬ê¸° vs ì„±ëŠ¥ ë¶„ì„:\")\n",
        "        for _, row in successful_models.iterrows():\n",
        "            if pd.notna(row['best_score']):\n",
        "                data_per_1k = row['data_size'] / 1000\n",
        "                print(f\"   ë¸Œëœì¹˜ {row['branch']}: {data_per_1k:.1f}kê°œ â†’ RMSE {row['best_score']:.4f}\")\n",
        "    \n",
        "    # LSTM ëª¨ë¸ íŠ¹ì„± ë¶„ì„\n",
        "    print(f\"\\nğŸ”¬ LSTM ëª¨ë¸ íŠ¹ì„±:\")\n",
        "    print(f\"   ì‹œí€€ìŠ¤ ê¸¸ì´: 24ì‹œê°„\")\n",
        "    print(f\"   í•™ìŠµ ëŒ€ìƒ: ë‚œë°©ì‹œì¦Œë§Œ\")\n",
        "    print(f\"   íƒ€ê²Ÿ ë¸Œëœì¹˜: A, B, C, D, G, H\")\n",
        "    \n",
        "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„ (ì„±ê³µí•œ ëª¨ë¸ë“¤)\n",
        "    if len(successful_models) > 0:\n",
        "        print(f\"\\nğŸ¯ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„:\")\n",
        "        for _, row in successful_models.iterrows():\n",
        "            model = models.get(row['model_name'])\n",
        "            if model and model.best_params:\n",
        "                params = model.best_params\n",
        "                lr = params.get('learning_rate', 'N/A')\n",
        "                units1 = params.get('lstm_units_1', 'N/A')\n",
        "                units2 = params.get('lstm_units_2', 'N/A')\n",
        "                dropout = params.get('dropout_rate', 'N/A')\n",
        "                print(f\"   ë¸Œëœì¹˜ {row['branch']}: lr={lr:.5f}, units=({units1},{units2}), dropout={dropout:.3f}\")\n",
        "    \n",
        "    return results_df, successful_models, failed_models\n",
        "\n",
        "# ì‹¤í–‰ í•¨ìˆ˜\n",
        "def run_lstm_analysis():\n",
        "    \"\"\"LSTM í›ˆë ¨ ê²°ê³¼ ë¶„ì„ ì‹¤í–‰\"\"\"\n",
        "    results_df, successful_models, failed_models = analyze_lstm_training_results(training_results, models)\n",
        "    return results_df, successful_models, failed_models\n",
        "\n",
        "print(\"âœ… LSTM í›ˆë ¨ ê²°ê³¼ ë¶„ì„ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")\n",
        "\n",
        "# LSTM í›ˆë ¨ ê²°ê³¼ ë¶„ì„ ì‹¤í–‰\n",
        "results_df, successful_models, failed_models = run_lstm_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdr_IyHVi6IU"
      },
      "source": [
        "## 7ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ 6ê°œ LSTM ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘...\n",
            "ğŸ“Š ë‚œë°©_A: 5,088ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 5064ê°œ (ì›ë³¸ 5088ê°œ)\n",
            "   âœ… ì™„ë£Œ: í‰ê· =162.31\n",
            "ğŸ“Š ë‚œë°©_B: 5,088ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 5064ê°œ (ì›ë³¸ 5088ê°œ)\n",
            "   âœ… ì™„ë£Œ: í‰ê· =348.25\n",
            "ğŸ“Š ë‚œë°©_C: 5,088ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 5064ê°œ (ì›ë³¸ 5088ê°œ)\n",
            "   âœ… ì™„ë£Œ: í‰ê· =355.05\n",
            "ğŸ“Š ë‚œë°©_D: 5,088ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 5064ê°œ (ì›ë³¸ 5088ê°œ)\n",
            "   âœ… ì™„ë£Œ: í‰ê· =233.65\n",
            "ğŸ“Š ë‚œë°©_G: 5,088ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 5064ê°œ (ì›ë³¸ 5088ê°œ)\n",
            "   âœ… ì™„ë£Œ: í‰ê· =301.11\n",
            "ğŸ“Š ë‚œë°©_H: 5,088ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
            "      ğŸ“Š ì‹œí€€ìŠ¤ ìƒì„±: 5064ê°œ (ì›ë³¸ 5088ê°œ)\n",
            "   âœ… ì™„ë£Œ: í‰ê· =207.50\n",
            "âœ… ì˜ˆì¸¡ ì™„ë£Œ: 6ê°œ ëª¨ë¸\n"
          ]
        }
      ],
      "source": [
        "# ì‹œì¦Œë³„ ì˜ˆì¸¡ (filtered_test_splits ì‚¬ìš©)\n",
        "print(\"ğŸ¯ 6ê°œ LSTM ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘...\")\n",
        "\n",
        "predictions = {}\n",
        "\n",
        "for model_key, model in models.items():\n",
        "    if model_key in filtered_test_splits:\n",
        "        test_data = filtered_test_splits[model_key]\n",
        "        \n",
        "        print(f\"ğŸ“Š {model_key}: {len(test_data):,}ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
        "        \n",
        "        try:\n",
        "            pred = model.predict(test_data)\n",
        "            predictions[model_key] = {\n",
        "                'data': test_data,\n",
        "                'predictions': pred\n",
        "            }\n",
        "            print(f\"   âœ… ì™„ë£Œ: í‰ê· ={np.mean(pred):.2f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)[:30]}...\")\n",
        "            predictions[model_key] = {\n",
        "                'data': test_data,\n",
        "                'predictions': np.zeros(len(test_data))\n",
        "            }\n",
        "\n",
        "print(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ëª¨ë¸\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1lscpyji6IV"
      },
      "source": [
        "## 8ï¸âƒ£ ì˜ˆì¸¡ ê²°ê³¼ í†µí•©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MmB8fvVHi6IV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì¤‘...\n",
            "ğŸ“Š ë‚œë°©_A: 5088ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
            "ğŸ“Š ë‚œë°©_B: 5088ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
            "ğŸ“Š ë‚œë°©_C: 5088ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
            "ğŸ“Š ë‚œë°©_D: 5088ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
            "ğŸ“Š ë‚œë°©_G: 5088ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
            "ğŸ“Š ë‚œë°©_H: 5088ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
            "âš ï¸ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ë°ì´í„°: 135912ê°œ (0ìœ¼ë¡œ ìœ ì§€)\n",
            "\n",
            "âœ… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì™„ë£Œ\n",
            "   ğŸ“Š ì´ ì˜ˆì¸¡ ê°œìˆ˜: 166,440ê°œ\n",
            "   ğŸ“ˆ ì˜ˆì¸¡ê°’ í†µê³„: í‰ê· =49.15, ìµœëŒ€=892.82\n"
          ]
        }
      ],
      "source": [
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë³¸ test_df ìˆœì„œì— ë§ê²Œ í†µí•©\n",
        "print(\"ğŸ”„ ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì¤‘...\")\n",
        "\n",
        "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë°°ì—´ ì´ˆê¸°í™”\n",
        "final_predictions = np.zeros(len(test_df))\n",
        "prediction_counts = np.zeros(len(test_df))  # ê° ì¸ë±ìŠ¤ë³„ ì˜ˆì¸¡ íšŸìˆ˜ ì¶”ì \n",
        "\n",
        "# ê° ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•´ë‹¹ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
        "for model_key, pred_info in predictions.items():\n",
        "    test_data = pred_info['data']\n",
        "    pred_values = pred_info['predictions']\n",
        "    \n",
        "    # ì›ë³¸ test_dfì—ì„œ í•´ë‹¹ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "    season, branch = model_key.split('_')\n",
        "    season_num = 1 if season == 'ë‚œë°©' else 0\n",
        "    \n",
        "    # ì¡°ê±´ì— ë§ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "    mask = (test_df['heating_season'] == season_num) & (test_df['branch_id'] == branch)\n",
        "    indices = test_df[mask].index.tolist()\n",
        "    \n",
        "    print(f\"ğŸ“Š {model_key}: {len(indices)}ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\")\n",
        "    \n",
        "    # ì˜ˆì¸¡ê°’ í• ë‹¹ (ì¸ë±ìŠ¤ ê°œìˆ˜ì™€ ì˜ˆì¸¡ê°’ ê°œìˆ˜ê°€ ë§ëŠ”ì§€ í™•ì¸)\n",
        "    if len(indices) == len(pred_values):\n",
        "        for i, idx in enumerate(indices):\n",
        "            final_predictions[idx] = pred_values[i]\n",
        "            prediction_counts[idx] += 1\n",
        "    else:\n",
        "        print(f\"   âš ï¸ í¬ê¸° ë¶ˆì¼ì¹˜: ì¸ë±ìŠ¤ {len(indices)}ê°œ vs ì˜ˆì¸¡ê°’ {len(pred_values)}ê°œ\")\n",
        "        # í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ìµœì†Œ ê°œìˆ˜ë§Œí¼ë§Œ í• ë‹¹\n",
        "        min_len = min(len(indices), len(pred_values))\n",
        "        for i in range(min_len):\n",
        "            final_predictions[indices[i]] = pred_values[i]\n",
        "            prediction_counts[indices[i]] += 1\n",
        "\n",
        "# ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ë°ì´í„° í™•ì¸\n",
        "unassigned_count = np.sum(prediction_counts == 0)\n",
        "if unassigned_count > 0:\n",
        "    print(f\"âš ï¸ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ë°ì´í„°: {unassigned_count}ê°œ (0ìœ¼ë¡œ ìœ ì§€)\")\n",
        "\n",
        "# ì¤‘ë³µ ì˜ˆì¸¡ í™•ì¸\n",
        "duplicate_count = np.sum(prediction_counts > 1)\n",
        "if duplicate_count > 0:\n",
        "    print(f\"âš ï¸ ì¤‘ë³µ ì˜ˆì¸¡ëœ ë°ì´í„°: {duplicate_count}ê°œ\")\n",
        "\n",
        "print(f\"\\nâœ… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì™„ë£Œ\")\n",
        "print(f\"   ğŸ“Š ì´ ì˜ˆì¸¡ ê°œìˆ˜: {len(final_predictions):,}ê°œ\")\n",
        "print(f\"   ğŸ“ˆ ì˜ˆì¸¡ê°’ í†µê³„: í‰ê· ={np.mean(final_predictions):.2f}, ìµœëŒ€={np.max(final_predictions):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3WOSlaHi6IV"
      },
      "source": [
        "## 9ï¸âƒ£ ìµœì¢… ê²°ê³¼ ì €ì¥ ë° í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥...\n",
            "ğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥: lightgbm_branch_season_predictions.csv\n",
            "\n",
            "ğŸ“Š RMSE ì„±ëŠ¥ í‰ê°€\n",
            "============================================================\n",
            "ğŸ† ì „ì²´ ì„±ëŠ¥:\n",
            "   RMSE: 70.8864\n",
            "   MAE:  52.6478\n",
            "   ìƒê´€ê³„ìˆ˜: 0.8915\n",
            "   ìœ íš¨ ë°ì´í„°: 166,440ê°œ\n",
            "\n",
            "ğŸ“ˆ ì‹œì¦Œë³„ RMSE ì„±ëŠ¥:\n",
            "   ë¹„ë‚œë°©ì‹œì¦Œ   : RMSE=51.8487 | MAE=40.9144 | ìƒê´€=   nan | 69,768ê°œ\n",
            "   ë‚œë°©ì‹œì¦Œ    : RMSE=81.9218 | MAE=61.1158 | ìƒê´€= 0.896 | 96,672ê°œ\n",
            "\n",
            "ğŸ“Š ë¸Œëœì¹˜ë³„ RMSE ì„±ëŠ¥:\n",
            "   ğŸ¥‡ RMSE ìš°ìˆ˜ ë¸Œëœì¹˜ (Top 5):\n",
            "      1. ë¸Œëœì¹˜ R: RMSE=17.1769 | MAE=14.6325 | 8,760ê°œ\n",
            "      2. ë¸Œëœì¹˜ S: RMSE=17.5521 | MAE=15.1737 | 8,760ê°œ\n",
            "      3. ë¸Œëœì¹˜ L: RMSE=29.6589 | MAE=23.1355 | 8,760ê°œ\n",
            "      4. ë¸Œëœì¹˜ A: RMSE=31.2909 | MAE=25.5237 | 8,760ê°œ\n",
            "      5. ë¸Œëœì¹˜ M: RMSE=44.5678 | MAE=35.3445 | 8,760ê°œ\n",
            "   ğŸ¥‰ RMSE ê°œì„  í•„ìš” ë¸Œëœì¹˜ (Bottom 5):\n",
            "      1. ë¸Œëœì¹˜ O: RMSE=85.3509 | MAE=65.9368 | 8,760ê°œ\n",
            "      2. ë¸Œëœì¹˜ I: RMSE=91.9005 | MAE=74.7925 | 8,760ê°œ\n",
            "      3. ë¸Œëœì¹˜ J: RMSE=102.2785 | MAE=87.2439 | 8,760ê°œ\n",
            "      4. ë¸Œëœì¹˜ N: RMSE=105.3003 | MAE=79.9602 | 8,760ê°œ\n",
            "      5. ë¸Œëœì¹˜ P: RMSE=116.2071 | MAE=96.9252 | 8,760ê°œ\n",
            "\n",
            "   ğŸ“ˆ ë¸Œëœì¹˜ë³„ RMSE í†µê³„:\n",
            "      í‰ê· : 65.0004\n",
            "      í‘œì¤€í¸ì°¨: 28.2812\n",
            "      ìµœì†Œ: 17.1769\n",
            "      ìµœëŒ€: 116.2071\n",
            "\n",
            "ğŸ”¥ ì‹œì¦ŒÃ—ë¸Œëœì¹˜ ì¡°í•©ë³„ RMSE (Ranking):\n",
            "    1. ë¹„ë‚œë°©ì‹œì¦Œ_L        : RMSE= 9.5005 | 3,672ê°œ\n",
            "    2. ë¹„ë‚œë°©ì‹œì¦Œ_R        : RMSE=10.6309 | 3,672ê°œ\n",
            "    3. ë¹„ë‚œë°©ì‹œì¦Œ_S        : RMSE=10.8434 | 3,672ê°œ\n",
            "    4. ë¹„ë‚œë°©ì‹œì¦Œ_M        : RMSE=16.1798 | 3,672ê°œ\n",
            "    5. ë‚œë°©ì‹œì¦Œ_R         : RMSE=20.6498 | 5,088ê°œ\n",
            "    6. ë‚œë°©ì‹œì¦Œ_S         : RMSE=21.1083 | 5,088ê°œ\n",
            "    7. ë¹„ë‚œë°©ì‹œì¦Œ_Q        : RMSE=25.9683 | 3,672ê°œ\n",
            "    8. ë¹„ë‚œë°©ì‹œì¦Œ_O        : RMSE=27.3052 | 3,672ê°œ\n",
            "    9. ë¹„ë‚œë°©ì‹œì¦Œ_N        : RMSE=28.5891 | 3,672ê°œ\n",
            "   10. ë‚œë°©ì‹œì¦Œ_A         : RMSE=29.6929 | 5,088ê°œ\n",
            "   11. ë¹„ë‚œë°©ì‹œì¦Œ_A        : RMSE=33.3788 | 3,672ê°œ\n",
            "   12. ë‚œë°©ì‹œì¦Œ_L         : RMSE=38.0704 | 5,088ê°œ\n",
            "   13. ë¹„ë‚œë°©ì‹œì¦Œ_I        : RMSE=40.5269 | 3,672ê°œ\n",
            "   14. ë‚œë°©ì‹œì¦Œ_H         : RMSE=43.1567 | 5,088ê°œ\n",
            "   15. ë¹„ë‚œë°©ì‹œì¦Œ_F        : RMSE=45.2663 | 3,672ê°œ\n",
            "   16. ë¹„ë‚œë°©ì‹œì¦Œ_H        : RMSE=46.5058 | 3,672ê°œ\n",
            "   17. ë‚œë°©ì‹œì¦Œ_D         : RMSE=47.2322 | 5,088ê°œ\n",
            "   18. ë‚œë°©ì‹œì¦Œ_G         : RMSE=49.6932 | 5,088ê°œ\n",
            "   19. ë¹„ë‚œë°©ì‹œì¦Œ_K        : RMSE=50.8846 | 3,672ê°œ\n",
            "   20. ë‚œë°©ì‹œì¦Œ_C         : RMSE=54.2458 | 5,088ê°œ\n",
            "   21. ë‚œë°©ì‹œì¦Œ_M         : RMSE=56.8407 | 5,088ê°œ\n",
            "   22. ë¹„ë‚œë°©ì‹œì¦Œ_D        : RMSE=61.9719 | 3,672ê°œ\n",
            "   23. ë¹„ë‚œë°©ì‹œì¦Œ_P        : RMSE=63.7274 | 3,672ê°œ\n",
            "   24. ë‚œë°©ì‹œì¦Œ_K         : RMSE=65.5340 | 5,088ê°œ\n",
            "   25. ë¹„ë‚œë°©ì‹œì¦Œ_E        : RMSE=65.5797 | 3,672ê°œ\n",
            "   26. ë¹„ë‚œë°©ì‹œì¦Œ_J        : RMSE=66.6835 | 3,672ê°œ\n",
            "   27. ë‚œë°©ì‹œì¦Œ_B         : RMSE=67.6216 | 5,088ê°œ\n",
            "   28. ë‚œë°©ì‹œì¦Œ_E         : RMSE=75.5598 | 5,088ê°œ\n",
            "   29. ë¹„ë‚œë°©ì‹œì¦Œ_G        : RMSE=76.7462 | 3,672ê°œ\n",
            "   30. ë¹„ë‚œë°©ì‹œì¦Œ_B        : RMSE=76.8676 | 3,672ê°œ\n",
            "   31. ë‚œë°©ì‹œì¦Œ_Q         : RMSE=93.2894 | 5,088ê°œ\n",
            "   32. ë‚œë°©ì‹œì¦Œ_F         : RMSE=96.5040 | 5,088ê°œ\n",
            "   33. ë¹„ë‚œë°©ì‹œì¦Œ_C        : RMSE=101.2537 | 3,672ê°œ\n",
            "   34. ë‚œë°©ì‹œì¦Œ_O         : RMSE=109.5633 | 5,088ê°œ\n",
            "   35. ë‚œë°©ì‹œì¦Œ_I         : RMSE=115.5664 | 5,088ê°œ\n",
            "   36. ë‚œë°©ì‹œì¦Œ_J         : RMSE=121.6607 | 5,088ê°œ\n",
            "   37. ë‚œë°©ì‹œì¦Œ_N         : RMSE=136.0169 | 5,088ê°œ\n",
            "   38. ë‚œë°©ì‹œì¦Œ_P         : RMSE=142.5448 | 5,088ê°œ\n",
            "\n",
            "ğŸ” ëª¨ë¸ ìµœì í™” vs ì‹¤ì œ ì„±ëŠ¥ ë¹„êµ:\n",
            "   í›ˆë ¨ì‹œ ìµœì í™” RMSE: í‰ê· =33.3423, ë²”ìœ„=[20.9227, 46.5207]\n",
            "   ì‹¤ì œ í…ŒìŠ¤íŠ¸ RMSE: 70.8864\n",
            "   ì„±ëŠ¥ ì°¨ì´: 37.5441\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ìµœì¢… ê²°ê³¼ë¥¼ test_dfì— ì¶”ê°€\n",
        "print(\"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥...\")\n",
        "\n",
        "# ì›ë³¸ test_df ë³µì‚¬\n",
        "result_df = test_df.copy()\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€ (ìŒìˆ˜ê°’ ì œê±°)\n",
        "result_df['pred_heat_demand'] = np.maximum(final_predictions, 0).round(1)\n",
        "\n",
        "# CSV íŒŒì¼ ì €ì¥\n",
        "output_filename = 'lightgbm_branch_season_predictions.csv'\n",
        "result_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"ğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥: {output_filename}\")\n",
        "\n",
        "# =============================================================================\n",
        "# RMSE ì¤‘ì‹¬ ì„±ëŠ¥ í‰ê°€ (ì‹¤ì œê°’ì´ ìˆëŠ” ê²½ìš°)\n",
        "# =============================================================================\n",
        "\n",
        "if 'heat_demand' in test_df.columns:\n",
        "    print(f\"\\nğŸ“Š RMSE ì„±ëŠ¥ í‰ê°€\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # ì „ì²´ RMSE\n",
        "    y_true = test_df['heat_demand'].values\n",
        "    y_pred = result_df['pred_heat_demand'].values\n",
        "    \n",
        "    # ìŒìˆ˜ë‚˜ NaN ê°’ ì œê±°\n",
        "    valid_mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
        "    y_true_clean = y_true[valid_mask]\n",
        "    y_pred_clean = y_pred[valid_mask]\n",
        "    \n",
        "    overall_rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "    overall_mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "    correlation = np.corrcoef(y_true_clean, y_pred_clean)[0, 1]\n",
        "    \n",
        "    print(f\"ğŸ† ì „ì²´ ì„±ëŠ¥:\")\n",
        "    print(f\"   RMSE: {overall_rmse:.4f}\")\n",
        "    print(f\"   MAE:  {overall_mae:.4f}\")\n",
        "    print(f\"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}\")\n",
        "    print(f\"   ìœ íš¨ ë°ì´í„°: {len(y_true_clean):,}ê°œ\")\n",
        "    \n",
        "    # ì‹œì¦Œë³„ RMSE (í•µì‹¬!)\n",
        "    print(f\"\\nğŸ“ˆ ì‹œì¦Œë³„ RMSE ì„±ëŠ¥:\")\n",
        "    season_names = {0: 'ë¹„ë‚œë°©ì‹œì¦Œ', 1: 'ë‚œë°©ì‹œì¦Œ'}\n",
        "    season_results = {}\n",
        "    \n",
        "    for season in [0, 1]:\n",
        "        mask = (test_df['heating_season'] == season) & valid_mask\n",
        "        if np.sum(mask) > 0:\n",
        "            season_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "            season_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
        "            season_corr = np.corrcoef(y_true[mask], y_pred[mask])[0, 1] if np.sum(mask) > 1 else 0\n",
        "            season_results[season] = {\n",
        "                'rmse': season_rmse, \n",
        "                'mae': season_mae, \n",
        "                'corr': season_corr,\n",
        "                'count': np.sum(mask)\n",
        "            }\n",
        "            \n",
        "            print(f\"   {season_names[season]:8s}: RMSE={season_rmse:7.4f} | MAE={season_mae:7.4f} | ìƒê´€={season_corr:6.3f} | {np.sum(mask):,}ê°œ\")\n",
        "    \n",
        "    # ë¸Œëœì¹˜ë³„ RMSE (ìƒìœ„/í•˜ìœ„ ë¶„ì„)\n",
        "    print(f\"\\nğŸ“Š ë¸Œëœì¹˜ë³„ RMSE ì„±ëŠ¥:\")\n",
        "    branch_results = {}\n",
        "    \n",
        "    for branch in sorted(test_df['branch_id'].unique()):\n",
        "        mask = (test_df['branch_id'] == branch) & valid_mask\n",
        "        if np.sum(mask) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆì–´ì•¼ RMSE ê³„ì‚° ê°€ëŠ¥\n",
        "            branch_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "            branch_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
        "            branch_results[branch] = {\n",
        "                'rmse': branch_rmse,\n",
        "                'mae': branch_mae, \n",
        "                'count': np.sum(mask)\n",
        "            }\n",
        "    \n",
        "    if branch_results:\n",
        "        # RMSE ê¸°ì¤€ ì •ë ¬\n",
        "        sorted_branches = sorted(branch_results.items(), key=lambda x: x[1]['rmse'])\n",
        "        \n",
        "        print(f\"   ğŸ¥‡ RMSE ìš°ìˆ˜ ë¸Œëœì¹˜ (Top 5):\")\n",
        "        for i, (branch, metrics) in enumerate(sorted_branches[:5], 1):\n",
        "            print(f\"      {i}. ë¸Œëœì¹˜ {branch}: RMSE={metrics['rmse']:7.4f} | MAE={metrics['mae']:7.4f} | {metrics['count']:,}ê°œ\")\n",
        "        \n",
        "        print(f\"   ğŸ¥‰ RMSE ê°œì„  í•„ìš” ë¸Œëœì¹˜ (Bottom 5):\")\n",
        "        for i, (branch, metrics) in enumerate(sorted_branches[-5:], 1):\n",
        "            print(f\"      {i}. ë¸Œëœì¹˜ {branch}: RMSE={metrics['rmse']:7.4f} | MAE={metrics['mae']:7.4f} | {metrics['count']:,}ê°œ\")\n",
        "        \n",
        "        # ë¸Œëœì¹˜ë³„ ì„±ëŠ¥ í†µê³„\n",
        "        rmse_values = [v['rmse'] for v in branch_results.values()]\n",
        "        print(f\"\\n   ğŸ“ˆ ë¸Œëœì¹˜ë³„ RMSE í†µê³„:\")\n",
        "        print(f\"      í‰ê· : {np.mean(rmse_values):.4f}\")\n",
        "        print(f\"      í‘œì¤€í¸ì°¨: {np.std(rmse_values):.4f}\")\n",
        "        print(f\"      ìµœì†Œ: {np.min(rmse_values):.4f}\")\n",
        "        print(f\"      ìµœëŒ€: {np.max(rmse_values):.4f}\")\n",
        "    \n",
        "    # ì‹œì¦ŒÃ—ë¸Œëœì¹˜ ì¡°í•©ë³„ RMSE (ìƒìœ„ 10ê°œë§Œ)\n",
        "    print(f\"\\nğŸ”¥ ì‹œì¦ŒÃ—ë¸Œëœì¹˜ ì¡°í•©ë³„ RMSE (Ranking):\")\n",
        "    combo_results = []\n",
        "    \n",
        "    for season in [0, 1]:\n",
        "        for branch in test_df['branch_id'].unique():\n",
        "            mask = (test_df['heating_season'] == season) & (test_df['branch_id'] == branch) & valid_mask\n",
        "            if np.sum(mask) > 1:\n",
        "                combo_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "                combo_name = f\"{season_names[season]}_{branch}\"\n",
        "                combo_results.append((combo_name, combo_rmse, np.sum(mask)))\n",
        "    \n",
        "    # RMSE ê¸°ì¤€ ì •ë ¬í•˜ì—¬ í‘œì‹œ\n",
        "    combo_results.sort(key=lambda x: x[1])\n",
        "    for i, (combo_name, rmse, count) in enumerate(combo_results, 1):\n",
        "        print(f\"   {i:2d}. {combo_name:15s}: RMSE={rmse:7.4f} | {count:,}ê°œ\")\n",
        "    \n",
        "    # í›ˆë ¨ëœ ëª¨ë¸ë“¤ì˜ ìµœì í™” ì„±ëŠ¥ê³¼ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¹„êµ\n",
        "    print(f\"\\nğŸ” ëª¨ë¸ ìµœì í™” vs ì‹¤ì œ ì„±ëŠ¥ ë¹„êµ:\")\n",
        "    optimization_rmses = [v['best_score'] for v in training_results.values() if v.get('best_score') is not None]\n",
        "    \n",
        "    if optimization_rmses:\n",
        "        print(f\"   í›ˆë ¨ì‹œ ìµœì í™” RMSE: í‰ê· ={np.mean(optimization_rmses):.4f}, ë²”ìœ„=[{np.min(optimization_rmses):.4f}, {np.max(optimization_rmses):.4f}]\")\n",
        "        print(f\"   ì‹¤ì œ í…ŒìŠ¤íŠ¸ RMSE: {overall_rmse:.4f}\")\n",
        "        print(f\"   ì„±ëŠ¥ ì°¨ì´: {abs(overall_rmse - np.mean(optimization_rmses)):.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"\\nâš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— heat_demand ì»¬ëŸ¼ì´ ì—†ì–´ì„œ RMSE í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"   ì˜ˆì¸¡ ê²°ê³¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    \n",
        "    # ì˜ˆì¸¡ê°’ ê¸°ë³¸ í†µê³„\n",
        "    print(f\"\\nğŸ“Š ì˜ˆì¸¡ê°’ ê¸°ë³¸ í†µê³„:\")\n",
        "    print(f\"   ê°œìˆ˜: {len(result_df):,}ê°œ\")\n",
        "    print(f\"   í‰ê· : {result_df['pred_heat_demand'].mean():.2f}\")\n",
        "    print(f\"   ì¤‘ì•™ê°’: {result_df['pred_heat_demand'].median():.2f}\")\n",
        "    print(f\"   í‘œì¤€í¸ì°¨: {result_df['pred_heat_demand'].std():.2f}\")\n",
        "    print(f\"   ë²”ìœ„: [{result_df['pred_heat_demand'].min():.2f}, {result_df['pred_heat_demand'].max():.2f}]\")\n",
        "    \n",
        "    # ì‹œì¦Œë³„ ì˜ˆì¸¡ í†µê³„\n",
        "    print(f\"\\nğŸ“ˆ ì‹œì¦Œë³„ ì˜ˆì¸¡ í†µê³„:\")\n",
        "    for season in [0, 1]:\n",
        "        season_data = result_df[result_df['heating_season'] == season]['pred_heat_demand']\n",
        "        if len(season_data) > 0:\n",
        "            season_name = 'ë¹„ë‚œë°©ì‹œì¦Œ' if season == 0 else 'ë‚œë°©ì‹œì¦Œ'\n",
        "            print(f\"   {season_name}: í‰ê· ={season_data.mean():.2f}, ê°œìˆ˜={len(season_data):,}ê°œ\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlR68tz9W1AF"
      },
      "source": [
        "## ğŸ”Ÿ ëª¨ë¸ ì •ë³´ ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xk3Cf_3YWyKq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ ëª¨ë¸ ì •ë³´ ì €ì¥...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'json' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# JSON íŒŒì¼ë¡œ ì €ì¥\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmodel_info_and_results.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mjson\u001b[49m.dump(model_info, f, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, default=\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“ model_info_and_results.json ì €ì¥ ì™„ë£Œ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
          ]
        }
      ],
      "source": [
        "# ëª¨ë¸ ì •ë³´ ë° ê²°ê³¼ ì €ì¥\n",
        "print(\"ğŸ’¾ ëª¨ë¸ ì •ë³´ ì €ì¥...\")\n",
        "\n",
        "# í›ˆë ¨ ê²°ê³¼ ë° ëª¨ë¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥\n",
        "model_info = {\n",
        "    'total_models': len(models),\n",
        "    'successful_models': len([k for k, v in training_results.items() if v.get('best_score') is not None]),\n",
        "    'training_results': training_results,\n",
        "    'prediction_stats': prediction_stats if 'prediction_stats' in locals() else {},\n",
        "    'feature_columns': models[list(models.keys())[0]].feature_cols if models else [],\n",
        "    'optimization_trials_per_model': n_trials_per_model,\n",
        "    'total_training_time_minutes': total_time_min if 'total_time_min' in locals() else 0\n",
        "}\n",
        "\n",
        "# RMSE ê²°ê³¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)\n",
        "if 'heat_demand' in test_df.columns:\n",
        "    model_info['evaluation_results'] = {\n",
        "        'overall_rmse': overall_rmse,\n",
        "        'overall_mae': overall_mae,\n",
        "        'correlation': correlation\n",
        "    }\n",
        "\n",
        "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
        "with open('model_info_and_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(model_info, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "print(\"ğŸ“ model_info_and_results.json ì €ì¥ ì™„ë£Œ\")\n",
        "\n",
        "# ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥\n",
        "print(f\"\\nğŸ“‹ ëª¨ë¸ ì •ë³´ ìš”ì•½:\")\n",
        "print(f\"   ğŸ”§ ì´ ëª¨ë¸ ìˆ˜: {model_info['total_models']}ê°œ\")\n",
        "print(f\"   âœ… ì„±ê³µí•œ ëª¨ë¸: {model_info['successful_models']}ê°œ\")\n",
        "print(f\"   ğŸ“Š ì‚¬ìš© íŠ¹ì„± ìˆ˜: {len(model_info['feature_columns'])}ê°œ\")\n",
        "print(f\"   ğŸ¯ ëª¨ë¸ë‹¹ ìµœì í™” ì‹œë„: {model_info['optimization_trials_per_model']}íšŒ\")\n",
        "\n",
        "# Google Drive ì €ì¥ (Colab í™˜ê²½)\n",
        "if IN_COLAB:\n",
        "    save_drive = input(\"\\nGoogle Driveì— ê²°ê³¼ íŒŒì¼ë“¤ì„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").lower() == 'y'\n",
        "    if save_drive:\n",
        "        try:\n",
        "            !cp {output_filename} /content/drive/MyDrive/\n",
        "            !cp model_info_and_results.json /content/drive/MyDrive/\n",
        "            print(\"âœ… Google Drive ì €ì¥ ì™„ë£Œ!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Google Drive ì €ì¥ ì‹¤íŒ¨: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leJLw4dVi6IW"
      },
      "source": [
        "## ğŸ¯ ìµœì¢… ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8stlKlsi6IX"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ”¥ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡: ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ XGBoost ëª¨ë¸ - ìµœì¢… ìš”ì•½\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nğŸ—ï¸ ëª¨ë¸ êµ¬ì„±:\")\n",
        "print(f\"   ğŸ“Š XGBoost ê°œë³„ ëª¨ë¸ (ì‹œì¦Œë³„ Ã— ë¸Œëœì¹˜ë³„)\")\n",
        "print(f\"   ğŸ¯ Optuna TPE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
        "print(f\"   ğŸ“‹ ì´ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ\")\n",
        "print(f\"   âœ… ì„±ê³µì  í›ˆë ¨: {len([k for k, v in training_results.items() if v.get('best_score') is not None])}ê°œ\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§:\")\n",
        "print(f\"   â­ HDD, wind_chill, ì˜¨ë„ ì œê³±/ì„¸ì œê³±\")\n",
        "print(f\"   ğŸ”„ ìˆœí™˜í˜• ì¸ì½”ë”© (ì‹œê°„, ì›”, ìš”ì¼)\")\n",
        "print(f\"   ğŸ“‹ ë²”ì£¼í˜•: heating_season, í”¼í¬ì‹œê°„, ê¸°ì˜¨ë²”ì£¼, ê°•ìˆ˜ê°•ë„\")\n",
        "print(f\"   ğŸ§® ìƒí˜¸ì‘ìš©: ìŠµë„Ã—ê¸°ì˜¨, ì›”Ã—ì¼\")\n",
        "# print(f\"   ğŸ“ StandardScaler ì •ê·œí™”\")\n",
        "\n",
        "print(f\"\\nğŸ¯ ëª¨ë¸ë§ ì „ëµ:\")\n",
        "print(f\"   â„ï¸ ë‚œë°©ì‹œì¦Œ (10,11,12,1,2,3,4ì›”) ì „ìš© ëª¨ë¸\")\n",
        "print(f\"   ğŸŒ ë¹„ë‚œë°©ì‹œì¦Œ (5,6,7,8,9ì›”) ì „ìš© ëª¨ë¸\")\n",
        "print(f\"   ğŸ¢ ë¸Œëœì¹˜ë³„ ê°œë³„ ëª¨ë¸ (ê° ì§€ì‚¬ì˜ íŠ¹ì„± ë°˜ì˜)\")\n",
        "print(f\"   âš¡ XGBoost with Early Stopping\")\n",
        "\n",
        "print(f\"\\nğŸ” ìµœì í™” ì„¤ì •:\")\n",
        "print(f\"   ğŸ“Š Optuna TPE Sampler\")\n",
        "print(f\"   ğŸ¯ ëª¨ë¸ë‹¹ {n_trials_per_model}íšŒ ì‹œë„\")\n",
        "print(f\"   ğŸ“ˆ ì‹œê³„ì—´ ê¸°ë°˜ Train/Validation ë¶„í•  (80:20)\")\n",
        "print(f\"   ğŸª XGBoost Pruning Callback ì‚¬ìš©\")\n",
        "\n",
        "if 'heat_demand' in test_df.columns:\n",
        "    print(f\"\\nğŸ† ìµœì¢… ì„±ëŠ¥:\")\n",
        "    print(f\"   ğŸ“Š ì „ì²´ RMSE: {overall_rmse:.4f}\")\n",
        "    print(f\"   ğŸ“ ì „ì²´ MAE: {overall_mae:.4f}\")\n",
        "    print(f\"   ğŸ“ˆ ìƒê´€ê³„ìˆ˜: {correlation:.4f}\")\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´\n",
        "if successful_models is not None and len(successful_models) > 0:\n",
        "    best_model_name = successful_models['best_score'].idxmin()\n",
        "    best_score = successful_models.loc[best_model_name, 'best_score']\n",
        "    print(f\"\\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (RMSE: {best_score:.4f})\")\n",
        "\n",
        "print(f\"\\nğŸ“ ì¶œë ¥ íŒŒì¼:\")\n",
        "print(f\"   â€¢ {output_filename} - ì˜ˆì¸¡ ê²°ê³¼\")\n",
        "print(f\"   â€¢ model_info_and_results.json - ëª¨ë¸ ì •ë³´ ë° í›ˆë ¨ ê²°ê³¼\")\n",
        "print(f\"   â€¢ í•µì‹¬ ì»¬ëŸ¼: pred_heat_demand (ì˜ˆì¸¡ê°’)\")\n",
        "\n",
        "print(f\"\\nâ±ï¸ ì‹¤í–‰ ì‹œê°„:\")\n",
        "if 'total_time_min' in locals():\n",
        "    print(f\"   ğŸš€ ì´ í›ˆë ¨ ì‹œê°„: {total_time_min:.1f}ë¶„\")\n",
        "    print(f\"   âš¡ í‰ê·  ëª¨ë¸ë‹¹: {total_time_min*60/len(models):.1f}ì´ˆ\")\n",
        "\n",
        "print(f\"\\nğŸ‰ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ”¬ í˜ì‹  í¬ì¸íŠ¸: ì‹œì¦ŒÃ—ë¸Œëœì¹˜ ì„¸ë¶„í™” + Optuna ìë™ ìµœì í™”\")\n",
        "print(f\"ğŸ“Š ì´ {len(models)}ê°œ ëª¨ë¸ë¡œ ì •ë°€í•œ ì§€ì—­ë³„-ì‹œì¦Œë³„ ì˜ˆì¸¡ êµ¬í˜„\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
