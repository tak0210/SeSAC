{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9N8kUTzi6IA"
      },
      "source": [
        "# ğŸ”¥ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡: ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ LightGBM ëª¨ë¸\n",
        "\n",
        "## ğŸ“‹ ëª¨ë¸ë§ ì „ëµ\n",
        "- **ì‹œì¦Œ ë¶„í• **: Heating Season vs Non-Heating Season\n",
        "- **ë¸Œëœì¹˜ë³„ ê°œë³„ ëª¨ë¸**: ê° branch_idë§ˆë‹¤ ì „ìš© ëª¨ë¸\n",
        "- **LightGBM**: ëª¨ë“  ëª¨ë¸ì— LightGBM ì‚¬ìš©\n",
        "- **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: Optuna TPE ì‚¬ìš©\n",
        "- **ì´ ëª¨ë¸ ìˆ˜**: 38ê°œ (2ì‹œì¦Œ Ã— 19ë¸Œëœì¹˜)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nmbT6Edi6ID"
      },
      "outputs": [],
      "source": [
        "# Google Colab í™˜ê²½ í™•ì¸ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"ğŸ”¥ Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\")\n",
        "    !pip install xgboost optuna\n",
        "    from google.colab import files, drive\n",
        "    print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc2w0lOti6IH"
      },
      "outputs": [],
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# ë¨¸ì‹ ëŸ¬ë‹\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "import lightgbm as lgb\n",
        "# XGBoostPruningCallback ì‚­ì œ (LightGBMì€ ë¶ˆí•„ìš”)\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsWjgGvyi6II"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
        "if IN_COLAB:\n",
        "    print(\"ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë°©ë²• ì„ íƒ:\")\n",
        "    print(\"1. ì§ì ‘ ì—…ë¡œë“œ\")\n",
        "    print(\"2. Google Drive\")\n",
        "\n",
        "    method = input(\"ì„ íƒ (1 ë˜ëŠ” 2): \")\n",
        "\n",
        "    if method == \"1\":\n",
        "        uploaded = files.upload()\n",
        "        files_list = list(uploaded.keys())\n",
        "        train_path = [f for f in files_list if 'train' in f.lower()][0]\n",
        "        test_path = [f for f in files_list if 'test' in f.lower()][0]\n",
        "    else:\n",
        "        drive.mount('/content/drive')\n",
        "        train_path = \"/content/drive/MyDrive/train_heat.csv\"\n",
        "        test_path = \"/content/drive/MyDrive/test_heat.csv\"\n",
        "else:\n",
        "    train_path = 'dataset/train_data_2122.csv'\n",
        "    test_path = 'dataset/test_data_23.csv'\n",
        "\n",
        "print(f\"âœ… íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BnYi_t0i6IJ"
      },
      "source": [
        "## 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy4VEB5gi6IK"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess(train_path, test_path):\n",
        "    print(\"ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\")\n",
        "\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    def process_df(df):\n",
        "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "            df = df.drop(columns=['Unnamed: 0'])\n",
        "        df.columns = [col.replace('train_heat.', '') for col in df.columns]\n",
        "\n",
        "        # ì‹œê°„ ë³€ìˆ˜\n",
        "        df['datetime'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
        "        # df['year'] = df['datetime'].dt.year\n",
        "        df['month'] = df['datetime'].dt.month\n",
        "        # df['day'] = df['datetime'].dt.day\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "        df['dayofweek'] = df['datetime'].dt.dayofweek\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        missing_cols = ['ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi']\n",
        "        if 'heat_demand' in df.columns:\n",
        "            missing_cols.append('heat_demand')\n",
        "\n",
        "        for col in missing_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].replace(-99, np.nan)\n",
        "\n",
        "        # wd -9.9 ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        df.loc[df['wd'] == -9.9, 'wd'] = np.nan\n",
        "\n",
        "        # ì¼ì‚¬ëŸ‰ ì•¼ê°„ ì²˜ë¦¬\n",
        "        if 'si' in df.columns:\n",
        "            night_mask = (df['hour'] < 8) | (df['hour'] > 18)\n",
        "            df.loc[night_mask & df['si'].isna(), 'si'] = 0\n",
        "\n",
        "        # ì§€ì‚¬ë³„ ë³´ê°„\n",
        "        df = df.sort_values(['branch_id', 'datetime'])\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        for branch in df['branch_id'].unique():\n",
        "            mask = df['branch_id'] == branch\n",
        "            df.loc[mask, numeric_cols] = df.loc[mask, numeric_cols].interpolate().fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        return df\n",
        "\n",
        "    train_df = process_df(train_df)\n",
        "    test_df = process_df(test_df)\n",
        "\n",
        "    print(f\"   í›ˆë ¨: {train_df.shape}, í…ŒìŠ¤íŠ¸: {test_df.shape}\")\n",
        "    print(f\"   ê¸°ê°„: {train_df['datetime'].min()} ~ {test_df['datetime'].max()}\")\n",
        "    print(f\"   ë¸Œëœì¹˜: {sorted(train_df['branch_id'].unique())}\")\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_and_preprocess(train_path, test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnbWsoYUi6IK"
      },
      "source": [
        "## 2ï¸âƒ£ íŒŒìƒë³€ìˆ˜ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmwGrilMi6IL"
      },
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    \"\"\"HDD, wind_chill, ìˆœí™˜í˜• ì¸ì½”ë”©, ë²”ì£¼í˜• ë³€ìˆ˜ ìƒì„±\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # â­ HDD (ìˆ˜ì¹˜í˜•)\n",
        "    if 'ta' in df.columns:\n",
        "        df['HDD_18'] = np.maximum(18 - df['ta'], 0)\n",
        "        df['HDD_20'] = np.maximum(20 - df['ta'], 0)\n",
        "\n",
        "    # â­ wind_chill (ìˆ˜ì¹˜í˜•)\n",
        "    if 'ta' in df.columns and 'ws' in df.columns:\n",
        "        df['wind_chill'] = np.where(\n",
        "            (df['ta'] <= 10) & (df['ws'] > 0),\n",
        "            13.12 + 0.6215 * df['ta'] - 11.37 * (df['ws'] ** 0.16) + 0.3965 * df['ta'] * (df['ws'] ** 0.16),\n",
        "            df['ta']\n",
        "        )\n",
        "\n",
        "    # â­ heating_season (ë²”ì£¼í˜•) - 3ì‹œì¦Œ ######################################\n",
        "    def assign_season(month):\n",
        "        if month in [5, 6, 7, 8, 9]:  # ë¹„ë‚œë°©\n",
        "            return 0\n",
        "        elif month in [10, 11, 12, 1]:  # ë‚œë°© ìƒìŠ¹\n",
        "            return 1\n",
        "        else:  # month in [2, 3, 4]  # ë‚œë°© í•˜ê°•\n",
        "            return 2\n",
        "\n",
        "    df['heating_season'] = df['month'].apply(assign_season)\n",
        "\n",
        "    # ì‹œê°„ëŒ€ ë²”ì£¼í˜•\n",
        "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
        "    df['is_peak_morning'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
        "    df['is_peak_evening'] = ((df['hour'] >= 18) & (df['hour'] <= 22)).astype(int)\n",
        "    df['is_night'] = ((df['hour'] >= 23) | (df['hour'] <= 5)).astype(int)\n",
        "\n",
        "    # í”¼í¬ì‹œê°„ í†µí•©\n",
        "    df['peak_time_category'] = 0\n",
        "    df.loc[df['is_peak_morning'] == 1, 'peak_time_category'] = 1\n",
        "    df.loc[df['is_peak_evening'] == 1, 'peak_time_category'] = 2\n",
        "    df.loc[df['is_night'] == 1, 'peak_time_category'] = 3\n",
        "\n",
        "    # â­ ê¸°ì˜¨ ë²”ì£¼ (ë²”ì£¼í˜•)\n",
        "    if 'ta' in df.columns:\n",
        "        df['temp_category'] = pd.cut(df['ta'],\n",
        "                                   bins=[-np.inf, 0, 10, 20, 30, np.inf],\n",
        "                                   labels=[0, 1, 2, 3, 4]).astype(int)\n",
        "\n",
        "    # â­ ê°•ìˆ˜ ê°•ë„ (ë²”ì£¼í˜•)\n",
        "    if 'rn_day' in df.columns:\n",
        "        df['rain_intensity'] = pd.cut(df['rn_day'],\n",
        "                                   bins=[-1, 0, 1, 5, 10, np.inf],\n",
        "                                   labels=[0, 1, 2, 3, 4]).astype(int)\n",
        "\n",
        "    # â­ ìˆœí™˜í˜• ì¸ì½”ë”© (ì‹œê°„ cos, sin)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
        "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
        "\n",
        "    # ì‹œê°„ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜\n",
        "    df['hour_squared'] = df['hour'] ** 2\n",
        "    df['month_day_interaction'] = df['month'] * df['datetime'].dt.day\n",
        "    \n",
        "    # ê¸°ì˜¨ ê´€ë ¨ íŒŒìƒë³€ìˆ˜\n",
        "    if 'ta' in df.columns:\n",
        "        df['ta_squared'] = df['ta'] ** 2\n",
        "        df['ta_cubed'] = df['ta'] ** 3\n",
        "    \n",
        "    # ìŠµë„ì™€ ê¸°ì˜¨ ìƒí˜¸ì‘ìš©\n",
        "    if 'hm' in df.columns and 'ta' in df.columns:\n",
        "        df['hm_ta_interaction'] = df['hm'] * df['ta']\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
        "train_df = create_features(train_df)\n",
        "test_df = create_features(test_df)\n",
        "\n",
        "print(f\"âœ… íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ: {train_df.shape[1]}ê°œ ì»¬ëŸ¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BnrDxhRi6IM"
      },
      "source": [
        "## 3ï¸âƒ£ ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ ë°ì´í„° ë¶„í• "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmdSkH3Ci6IN"
      },
      "outputs": [],
      "source": [
        "# ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ ë°ì´í„° ë¶„í• \n",
        "def split_by_season_and_branch(df):\n",
        "    data_splits = {}\n",
        "    \n",
        "    branches = sorted(df['branch_id'].unique())\n",
        "    seasons = [0, 1, 2]  # 0: ë¹„ë‚œë°©, 1: ë‚œë°©ìƒìŠ¹, 2: ë‚œë°©í•˜ê°•\n",
        "    season_names = {0: 'ë¹„ë‚œë°©', 1: 'ë‚œë°©ìƒìŠ¹', 2: 'ë‚œë°©í•˜ê°•'} ######################################\n",
        "    \n",
        "    print(f\"ğŸ“Š ë°ì´í„° ë¶„í•  ì •ë³´:\")\n",
        "    print(f\"   ë¸Œëœì¹˜: {len(branches)}ê°œ - {branches}\")\n",
        "    print(f\"   ì‹œì¦Œ: {len(seasons)}ê°œ - {[season_names[s] for s in seasons]}\")\n",
        "    print(f\"   ì´ ì¡°í•©: {len(branches) * len(seasons)}ê°œ\")\n",
        "    \n",
        "    for season in seasons:\n",
        "        for branch in branches:\n",
        "            key = f\"{season_names[season]}_{branch}\"\n",
        "            \n",
        "            # ì‹œì¦Œê³¼ ë¸Œëœì¹˜ë¡œ í•„í„°ë§\n",
        "            subset = df[(df['heating_season'] == season) & (df['branch_id'] == branch)].copy()\n",
        "            \n",
        "            if len(subset) > 0:\n",
        "                data_splits[key] = subset\n",
        "                print(f\"   {key}: {len(subset):,}ê°œ ë°ì´í„°\")\n",
        "            else:\n",
        "                print(f\"   {key}: ë°ì´í„° ì—†ìŒ âš ï¸\")\n",
        "    \n",
        "    return data_splits\n",
        "\n",
        "# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \n",
        "train_splits = split_by_season_and_branch(train_df)\n",
        "test_splits = split_by_season_and_branch(test_df)\n",
        "\n",
        "print(f\"\\nâœ… í›ˆë ¨ ë°ì´í„°: {len(train_splits)}ê°œ ë¶„í• \")\n",
        "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_splits)}ê°œ ë¶„í• \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Œ Validationì‹œ ì›”ë³„ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_temporal_split_indices(df, test_size=0.2, min_val_samples=10):\n",
        "    \"\"\"ì‹œê°„ ê¸°ë°˜ ë¶„í•  ì¸ë±ìŠ¤ ë°˜í™˜\"\"\"\n",
        "    \n",
        "    month_counts = df['month'].value_counts()\n",
        "    valid_months = month_counts[month_counts >= min_val_samples * 2].index\n",
        "    \n",
        "    if len(valid_months) < 3:\n",
        "        # í´ë°±: ê¸°ì¡´ ë°©ì‹\n",
        "        split_idx = int(len(df) * (1 - test_size))\n",
        "        return df.index[:split_idx], df.index[split_idx:]\n",
        "    \n",
        "    train_indices = []\n",
        "    val_indices = []\n",
        "    \n",
        "    for month in valid_months:\n",
        "        month_data = df[df['month'] == month].sort_values('datetime')\n",
        "        val_size = max(min_val_samples, int(len(month_data) * test_size))\n",
        "        \n",
        "        train_indices.extend(month_data.iloc[:-val_size].index)\n",
        "        val_indices.extend(month_data.iloc[-val_size:].index)\n",
        "    \n",
        "    print(f\"      ğŸ“… ì›”ë³„ ë¶„í• : {len(valid_months)}ê°œì›”, ê²€ì¦ {len(val_indices)}ê°œ\")\n",
        "    \n",
        "    return train_indices, val_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmL7oLW1i6IN"
      },
      "source": [
        "## 4ï¸âƒ£ LightGBM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimizedLightGBMModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "        self.feature_cols = None\n",
        "        self.study = None\n",
        "        self.best_score = None\n",
        "        \n",
        "        # LightGBM ë²„ì „ í™•ì¸\n",
        "        self.lgb_version = lgb.__version__\n",
        "        \n",
        "    def define_feature_columns(self, df):\n",
        "        \"\"\"íŠ¹ì„± ì»¬ëŸ¼ ì •ì˜\"\"\"\n",
        "        exclude_cols = [\n",
        "        'tm', 'datetime', 'year', 'heat_demand', 'branch_id', 'month'  # datetime ì œì™¸\n",
        "        ]\n",
        "        \n",
        "        self.feature_cols = [col for col in df.columns \n",
        "                           if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]\n",
        "        \n",
        "        print(f\"   ğŸ“‹ {self.model_name}: {len(self.feature_cols)}ê°œ íŠ¹ì„± ì‚¬ìš©\")\n",
        "        return self.feature_cols\n",
        "    \n",
        "    def objective(self, trial, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Optuna ëª©ì  í•¨ìˆ˜ (LightGBM íŒŒë¼ë¯¸í„°)\"\"\"\n",
        "        # LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„\n",
        "        params = {\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
        "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
        "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
        "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 50),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "            'random_state': 42,\n",
        "            'verbosity': -1,  # ë¡œê·¸ ì¶œë ¥ ì•ˆí•¨\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        # ë°ì´í„°ì…‹ ìƒì„±\n",
        "        train_data = lgb.Dataset(X_train, label=y_train)\n",
        "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "        \n",
        "        # ëª¨ë¸ í›ˆë ¨ (early stopping í¬í•¨)\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            valid_sets=[val_data],\n",
        "            num_boost_round=1000,\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(50),\n",
        "                lgb.log_evaluation(0)  # ë¡œê·¸ ì¶œë ¥ ì•ˆí•¨\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        # ê²€ì¦ ì˜ˆì¸¡\n",
        "        y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        \n",
        "        return rmse\n",
        "    \n",
        "    def fit(self, df, target_col='heat_demand', n_trials=20):\n",
        "        \"\"\"ëª¨ë¸ í›ˆë ¨ (Optuna ìµœì í™” í¬í•¨)\"\"\"\n",
        "        print(f\"\\nğŸ” {self.model_name} ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "        print(f\"   ğŸ”§ LightGBM ë²„ì „: {self.lgb_version}\")\n",
        "        \n",
        "        if len(df) < 10:\n",
        "            self._fit_basic_model(df, target_col)\n",
        "            return\n",
        "    \n",
        "        # ğŸ”¥ 1. ë¶„í•  ì¸ë±ìŠ¤ ë¨¼ì € êµ¬í•˜ê¸° (datetime ìˆì„ ë•Œ)\n",
        "        train_indices, val_indices = get_temporal_split_indices(df, test_size=0.2)\n",
        "        \n",
        "        # ğŸ”¥ 2. íŠ¹ì„± ì»¬ëŸ¼ ì •ì˜ (datetime ì œê±°)\n",
        "        self.define_feature_columns(df)\n",
        "        \n",
        "        # ğŸ”¥ 3. X, y ìƒì„± (datetime ì—†ìŒ)\n",
        "        X = df[self.feature_cols].copy()\n",
        "        y = df[target_col].copy()\n",
        "        \n",
        "        # ğŸ”¥ 4. ì¸ë±ìŠ¤ë¡œ ë¶„í• \n",
        "        X_train = X.loc[train_indices]\n",
        "        y_train = y.loc[train_indices]\n",
        "        X_val = X.loc[val_indices]\n",
        "        y_val = y.loc[val_indices]\n",
        "        \n",
        "        print(f\"      ğŸ“Š í›ˆë ¨: {len(X_train):,}ê°œ, ê²€ì¦: {len(X_val):,}ê°œ\")\n",
        "            \n",
        "        if len(X_train) < 5 or len(X_val) < 2:\n",
        "            print(f\"   âš ï¸ ë¶„í•  í›„ ë°ì´í„° ë¶€ì¡± - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
        "            self._fit_basic_model(df, target_col)\n",
        "            return\n",
        "        \n",
        "        # Optuna ìµœì í™”\n",
        "        print(f\"   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: \", end=\"\", flush=True)\n",
        "        \n",
        "        try:\n",
        "            # ëª¨ë“  ë¡œê·¸ ë„ê¸°\n",
        "            optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "            \n",
        "            self.study = optuna.create_study(\n",
        "                direction='minimize',\n",
        "                sampler=TPESampler(seed=42),\n",
        "                study_name=f\"lgb_{self.model_name}\"\n",
        "            )\n",
        "            \n",
        "            # ì§„í–‰ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ì½œë°±\n",
        "            def progress_callback(study, trial):\n",
        "                print(f\"\\r   ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: {len(study.trials)}/{n_trials} (Best RMSE: {study.best_value:.4f})\", end=\"\", flush=True)\n",
        "            \n",
        "            # ìµœì í™” ì‹¤í–‰\n",
        "            self.study.optimize(\n",
        "                lambda trial: self.objective(trial, X_train, y_train, X_val, y_val),\n",
        "                n_trials=n_trials,\n",
        "                callbacks=[progress_callback]\n",
        "            )\n",
        "            \n",
        "            print()  # ì¤„ë°”ê¿ˆ\n",
        "            \n",
        "            # ì•ˆì „í•œ best_value ì ‘ê·¼\n",
        "            if len(self.study.trials) > 0 and self.study.best_trial is not None:\n",
        "                self.best_score = self.study.best_value\n",
        "                self.best_params = self.study.best_params.copy()\n",
        "                \n",
        "                # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¶”ê°€\n",
        "                self.best_params.update({\n",
        "                    'objective': 'regression',\n",
        "                    'metric': 'rmse',\n",
        "                    'random_state': 42,\n",
        "                    'verbosity': -1,\n",
        "                    'n_jobs': -1\n",
        "                })\n",
        "                \n",
        "                # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í›ˆë ¨\n",
        "                train_data_full = lgb.Dataset(X, label=y)\n",
        "                val_data_full = lgb.Dataset(X_val, label=y_val, reference=train_data_full)\n",
        "                \n",
        "                self.model = lgb.train(\n",
        "                    self.best_params,\n",
        "                    train_data_full,\n",
        "                    valid_sets=[val_data_full],\n",
        "                    num_boost_round=1500,\n",
        "                    callbacks=[\n",
        "                        lgb.early_stopping(100),\n",
        "                        lgb.log_evaluation(0)\n",
        "                    ]\n",
        "                )\n",
        "                \n",
        "                # ì„±ëŠ¥ ì •ë³´\n",
        "                val_pred = self.model.predict(X_val, num_iteration=self.model.best_iteration)\n",
        "                val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "                \n",
        "                print(f\"   ğŸ“ˆ ìµœì í™” ì™„ë£Œ: Best RMSE = {self.best_score:.4f}\")\n",
        "                print(f\"   ğŸ“Š ê²€ì¦ RMSE = {val_rmse:.4f}\")\n",
        "                print(f\"   ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: lr={self.best_params.get('learning_rate', 0.1):.3f}, leaves={self.best_params.get('num_leaves', 31)}\")\n",
        "                \n",
        "            else:\n",
        "                print(f\"   âš ï¸ ìµœì í™” ì‹¤íŒ¨: ìœ íš¨í•œ trial ì—†ìŒ - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
        "                self._fit_basic_model(df, target_col)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"\\n   âš ï¸ ìµœì í™” ì‹¤íŒ¨: {str(e)[:30]}... - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
        "            self._fit_basic_model(df, target_col)\n",
        "    \n",
        "    def _fit_basic_model(self, df, target_col):\n",
        "        \"\"\"ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨ (ìµœì í™” ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)\"\"\"\n",
        "        if not hasattr(self, 'feature_cols') or self.feature_cols is None:\n",
        "            self.define_feature_columns(df)\n",
        "        \n",
        "        X = df[self.feature_cols].copy()\n",
        "        y = df[target_col].copy()\n",
        "        \n",
        "        \n",
        "        # ê¸°ë³¸ LightGBM íŒŒë¼ë¯¸í„°\n",
        "        params = {\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': 31,\n",
        "            'learning_rate': 0.1,\n",
        "            'feature_fraction': 0.8,\n",
        "            'bagging_fraction': 0.8,\n",
        "            'bagging_freq': 5,\n",
        "            'random_state': 42,\n",
        "            'verbosity': -1,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        # ë°ì´í„°ì…‹ ìƒì„± ë° í›ˆë ¨\n",
        "        train_data = lgb.Dataset(X, label=y)\n",
        "        \n",
        "        self.model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            num_boost_round=500,\n",
        "            callbacks=[lgb.log_evaluation(0)]\n",
        "        )\n",
        "        \n",
        "        self.best_score = None\n",
        "        self.best_params = None\n",
        "        print(f\"   ğŸ”§ ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\")\n",
        "    \n",
        "    def predict(self, df):\n",
        "        \"\"\"ì˜ˆì¸¡\"\"\"\n",
        "        if self.model is None:\n",
        "            return np.full(len(df), 0)\n",
        "        \n",
        "        # ë™ì¼í•œ íŠ¹ì„± ì»¬ëŸ¼ ì‚¬ìš©\n",
        "        X = df[self.feature_cols].copy()\n",
        "        X = X.fillna(0)\n",
        "        \n",
        "        # ì˜ˆì¸¡ (best_iteration ì‚¬ìš©)\n",
        "        if hasattr(self.model, 'best_iteration') and self.model.best_iteration > 0:\n",
        "            predictions = self.model.predict(X, num_iteration=self.model.best_iteration)\n",
        "        else:\n",
        "            predictions = self.model.predict(X)\n",
        "        \n",
        "        # ìŒìˆ˜ ê°’ ì²˜ë¦¬\n",
        "        predictions = np.maximum(predictions, 0)\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "print(\"âœ… LightGBM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SewcIVN_i6IR"
      },
      "source": [
        "## 5ï¸âƒ£ 57ê°œ ëª¨ë¸ í›ˆë ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 57ê°œ ëª¨ë¸ í›ˆë ¨\n",
        "print(\"ğŸš€ 57ê°œ LightGBM ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "models = {}\n",
        "training_results = {}\n",
        "n_trials_per_model = 20  # ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ 20ìœ¼ë¡œ ì¡°ì •\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´í„°\n",
        "success_count = 0\n",
        "basic_count = 0\n",
        "failed_count = 0\n",
        "\n",
        "# ëª¨ë“  í›ˆë ¨ ë°ì´í„° ë¶„í• ì— ëŒ€í•´ ëª¨ë¸ í›ˆë ¨\n",
        "for i, (model_key, train_data) in enumerate(train_splits.items(), 1):\n",
        "    print(f\"\\n[{i:2d}/{len(train_splits)}] ğŸ”¥ {model_key}\")\n",
        "    print(f\"         ğŸ“Š ë°ì´í„°: {len(train_data):,}ê°œ\")\n",
        "    \n",
        "    # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\n",
        "    model = OptimizedLightGBMModel(model_key)\n",
        "    \n",
        "    try:\n",
        "        model_start = datetime.now()\n",
        "        model.fit(train_data, n_trials=n_trials_per_model)\n",
        "        model_time = (datetime.now() - model_start).total_seconds()\n",
        "        \n",
        "        models[model_key] = model\n",
        "        \n",
        "        # ê²°ê³¼ ë¶„ë¥˜\n",
        "        if model.best_score is not None:\n",
        "            success_count += 1\n",
        "            status = \"âœ… ìµœì í™”\"\n",
        "            score_text = f\"RMSE: {model.best_score:.3f}\"\n",
        "        else:\n",
        "            basic_count += 1\n",
        "            status = \"ğŸ”§ ê¸°ë³¸ëª¨ë¸\"\n",
        "            score_text = \"ê¸°ë³¸íŒŒë¼ë¯¸í„°\"\n",
        "        \n",
        "        # ì•ˆì „í•œ ê²°ê³¼ ì €ì¥\n",
        "        training_results[model_key] = {\n",
        "            'data_size': len(train_data),\n",
        "            'training_time': model_time,\n",
        "            'best_score': model.best_score,\n",
        "            'best_params': model.best_params,\n",
        "            'optimization_success': model.best_score is not None\n",
        "        }\n",
        "        \n",
        "        print(f\"         {status} | {score_text} | â±ï¸ {model_time:.1f}ì´ˆ\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"         âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)[:30]}...\")\n",
        "        failed_count += 1\n",
        "        \n",
        "        # ì™„ì „ ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´\n",
        "        try:\n",
        "            basic_model = OptimizedLightGBMModel(model_key)\n",
        "            basic_model._fit_basic_model(train_data, 'heat_demand')\n",
        "            models[model_key] = basic_model\n",
        "            \n",
        "            training_results[model_key] = {\n",
        "                'data_size': len(train_data),\n",
        "                'training_time': 0,\n",
        "                'best_score': None,\n",
        "                'best_params': None,\n",
        "                'optimization_success': False,\n",
        "                'error': str(e)[:50]\n",
        "            }\n",
        "            print(f\"         ğŸ”§ ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´ ì™„ë£Œ\")\n",
        "            \n",
        "        except Exception as e2:\n",
        "            print(f\"         âŒ ê¸°ë³¸ ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨\")\n",
        "            # ë”ë¯¸ ëª¨ë¸ ì €ì¥\n",
        "            dummy_model = OptimizedLightGBMModel(model_key)\n",
        "            dummy_model.model = None\n",
        "            models[model_key] = dummy_model\n",
        "            \n",
        "            training_results[model_key] = {\n",
        "                'data_size': len(train_data),\n",
        "                'training_time': 0,\n",
        "                'best_score': None,\n",
        "                'best_params': None,\n",
        "                'optimization_success': False,\n",
        "                'error': f\"Complete failure: {str(e2)[:30]}\"\n",
        "            }\n",
        "\n",
        "total_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"ğŸ‰ ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "print(f\"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„ (í‰ê·  {total_time/len(train_splits):.1f}ì´ˆ/ëª¨ë¸)\")\n",
        "print(f\"ğŸ“Š í›ˆë ¨ ê²°ê³¼:\")\n",
        "print(f\"   âœ… ìµœì í™” ì„±ê³µ: {success_count}ê°œ\")\n",
        "print(f\"   ğŸ”§ ê¸°ë³¸ ëª¨ë¸: {basic_count}ê°œ\") \n",
        "print(f\"   âŒ ì™„ì „ ì‹¤íŒ¨: {failed_count}ê°œ\")\n",
        "print(f\"   ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {(success_count + basic_count)/len(train_splits)*100:.1f}%\")\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°\n",
        "successful_models = {k: v for k, v in training_results.items() \n",
        "                    if v.get('optimization_success', False)}\n",
        "\n",
        "if successful_models:\n",
        "    best_model = min(successful_models.items(), key=lambda x: x[1]['best_score'])\n",
        "    print(f\"ğŸ† ìµœê³  ì„±ëŠ¥: {best_model[0]} (RMSE: {best_model[1]['best_score']:.4f})\")\n",
        "    \n",
        "    # ì‹œì¦Œë³„ í‰ê·  ì„±ëŠ¥\n",
        "    season_scores = {'ë‚œë°©': [], 'ë¹„ë‚œë°©': []}\n",
        "    for model_name, result in successful_models.items():\n",
        "        season = model_name.split('_')[0]\n",
        "        if season in season_scores:\n",
        "            season_scores[season].append(result['best_score'])\n",
        "    \n",
        "    print(f\"ğŸ“ˆ ì‹œì¦Œë³„ í‰ê·  RMSE:\")\n",
        "    for season, scores in season_scores.items():\n",
        "        if scores:\n",
        "            print(f\"   {season}ì‹œì¦Œ: {np.mean(scores):.4f} ({len(scores)}ê°œ ëª¨ë¸)\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-cKYeui6IS"
      },
      "source": [
        "## 6ï¸âƒ£ í›ˆë ¨ ê²°ê³¼ ë¶„ì„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q73KBW9yi6IT"
      },
      "outputs": [],
      "source": [
        "# í›ˆë ¨ ê²°ê³¼ ë¶„ì„\n",
        "print(\"\\nğŸ“Š í›ˆë ¨ ê²°ê³¼ ë¶„ì„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ê²°ê³¼ ì •ë¦¬\n",
        "results_df = pd.DataFrame(training_results).T\n",
        "results_df['season'] = results_df.index.str.split('_').str[0]\n",
        "results_df['branch'] = results_df.index.str.split('_').str[1]\n",
        "\n",
        "# ì„±ê³µ/ì‹¤íŒ¨ í†µê³„\n",
        "successful_models = results_df[results_df['best_score'].notna()]\n",
        "failed_models = results_df[results_df['best_score'].isna()]\n",
        "\n",
        "print(f\"âœ… ì„±ê³µ: {len(successful_models)}ê°œ ëª¨ë¸\")\n",
        "print(f\"âŒ ì‹¤íŒ¨: {len(failed_models)}ê°œ ëª¨ë¸\")\n",
        "\n",
        "if len(successful_models) > 0:\n",
        "    print(f\"\\nğŸ† ìµœì í™” ì„±ëŠ¥ í†µê³„:\")\n",
        "    print(f\"   í‰ê·  RMSE: {successful_models['best_score'].mean():.4f}\")\n",
        "    print(f\"   ìµœì†Œ RMSE: {successful_models['best_score'].min():.4f}\")\n",
        "    print(f\"   ìµœëŒ€ RMSE: {successful_models['best_score'].max():.4f}\")\n",
        "    print(f\"   í‘œì¤€í¸ì°¨: {successful_models['best_score'].std():.4f}\")\n",
        "\n",
        "    # ì‹œì¦Œë³„ ì„±ëŠ¥\n",
        "    print(f\"\\nğŸ“ˆ ì‹œì¦Œë³„ í‰ê·  RMSE:\")\n",
        "    season_performance = successful_models.groupby('season')['best_score'].agg(['mean', 'count'])\n",
        "    for season, row in season_performance.iterrows():\n",
        "        print(f\"   {season}ì‹œì¦Œ: {row['mean']:.4f} ({int(row['count'])}ê°œ ëª¨ë¸)\")\n",
        "\n",
        "    # # ìƒìœ„ 5ê°œ ëª¨ë¸\n",
        "    # print(f\"\\nğŸ¥‡ ì„±ëŠ¥ ìƒìœ„ 5ê°œ ëª¨ë¸:\")\n",
        "    # top_models = successful_models.nsmallest(5, 'best_score')\n",
        "    # for idx, (model_name, row) in enumerate(top_models.iterrows(), 1):\n",
        "    #     print(f\"   {idx}. {model_name}: RMSE = {row['best_score']:.4f}\")\n",
        "\n",
        "# ì‹¤íŒ¨í•œ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì •ë³´ ì¶œë ¥\n",
        "if len(failed_models) > 0:\n",
        "    print(f\"\\nâš ï¸ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:\")\n",
        "    for model_name, row in failed_models.iterrows():\n",
        "        error_msg = row.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')\n",
        "        print(f\"   {model_name}: {error_msg}\")\n",
        "\n",
        "# í›ˆë ¨ ì‹œê°„ í†µê³„\n",
        "avg_time = results_df['training_time'].mean()\n",
        "total_time_min = results_df['training_time'].sum() / 60\n",
        "print(f\"\\nâ±ï¸ í›ˆë ¨ ì‹œê°„ í†µê³„:\")\n",
        "print(f\"   í‰ê·  ëª¨ë¸ë‹¹: {avg_time:.1f}ì´ˆ\")\n",
        "print(f\"   ì´ í›ˆë ¨ ì‹œê°„: {total_time_min:.1f}ë¶„\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdr_IyHVi6IU"
      },
      "source": [
        "## 7ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyMyt1tLi6IU"
      },
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
        "print(\"ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
        "predictions = {}\n",
        "prediction_stats = {}\n",
        "\n",
        "# ê° ëª¨ë¸ë³„ë¡œ í•´ë‹¹ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡\n",
        "for model_key, model in models.items():\n",
        "    if model_key in test_splits:\n",
        "        test_data = test_splits[model_key]\n",
        "        \n",
        "        print(f\"ğŸ“Š {model_key}: {len(test_data):,}ê°œ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
        "        \n",
        "        try:\n",
        "            pred = model.predict(test_data)\n",
        "            predictions[model_key] = {\n",
        "                'data': test_data,\n",
        "                'predictions': pred\n",
        "            }\n",
        "            \n",
        "            # ì˜ˆì¸¡ í†µê³„\n",
        "            prediction_stats[model_key] = {\n",
        "                'count': len(pred),\n",
        "                'mean': np.mean(pred),\n",
        "                'std': np.std(pred),\n",
        "                'min': np.min(pred),\n",
        "                'max': np.max(pred)\n",
        "            }\n",
        "            \n",
        "            print(f\"   âœ… ì™„ë£Œ: í‰ê· ={np.mean(pred):.2f}, ë²”ìœ„=[{np.min(pred):.2f}, {np.max(pred):.2f}]\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}\")\n",
        "            # ê¸°ë³¸ê°’ìœ¼ë¡œ 0 í• ë‹¹\n",
        "            predictions[model_key] = {\n",
        "                'data': test_data,\n",
        "                'predictions': np.zeros(len(test_data))\n",
        "            }\n",
        "    else:\n",
        "        print(f\"âš ï¸ {model_key}: ëŒ€ì‘í•˜ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ\")\n",
        "\n",
        "print(f\"\\nâœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ëª¨ë¸\")\n",
        "\n",
        "# ì˜ˆì¸¡ í†µê³„ ìš”ì•½\n",
        "if prediction_stats:\n",
        "    stats_df = pd.DataFrame(prediction_stats).T\n",
        "    print(f\"\\nğŸ“ˆ ì˜ˆì¸¡ê°’ í†µê³„ ìš”ì•½:\")\n",
        "    print(f\"   ì „ì²´ ì˜ˆì¸¡ ê°œìˆ˜: {stats_df['count'].sum():,}ê°œ\")\n",
        "    print(f\"   í‰ê·  ì˜ˆì¸¡ê°’ ë²”ìœ„: [{stats_df['mean'].min():.2f}, {stats_df['mean'].max():.2f}]\")\n",
        "    print(f\"   ìµœëŒ€ ì˜ˆì¸¡ê°’: {stats_df['max'].max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1lscpyji6IV"
      },
      "source": [
        "## 8ï¸âƒ£ ì˜ˆì¸¡ ê²°ê³¼ í†µí•©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmB8fvVHi6IV"
      },
      "outputs": [],
      "source": [
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë³¸ test_df ìˆœì„œì— ë§ê²Œ í†µí•©\n",
        "print(\"ğŸ”„ ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì¤‘...\")\n",
        "\n",
        "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë°°ì—´ ì´ˆê¸°í™”\n",
        "final_predictions = np.zeros(len(test_df))\n",
        "prediction_counts = np.zeros(len(test_df))  # ê° ì¸ë±ìŠ¤ë³„ ì˜ˆì¸¡ íšŸìˆ˜ ì¶”ì \n",
        "\n",
        "# ê° ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•´ë‹¹ ì¸ë±ìŠ¤ì— í• ë‹¹\n",
        "for model_key, pred_info in predictions.items():\n",
        "    test_data = pred_info['data']\n",
        "    pred_values = pred_info['predictions']\n",
        "    \n",
        "    # ì›ë³¸ test_dfì—ì„œ í•´ë‹¹ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "    season, branch = model_key.split('_')\n",
        "    season_num = 1 if season == 'ë‚œë°©' else 0\n",
        "    \n",
        "    # ì¡°ê±´ì— ë§ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "    mask = (test_df['heating_season'] == season_num) & (test_df['branch_id'] == branch)\n",
        "    indices = test_df[mask].index.tolist()\n",
        "    \n",
        "    print(f\"ğŸ“Š {model_key}: {len(indices)}ê°œ ì¸ë±ìŠ¤ì— í• ë‹¹\")\n",
        "    \n",
        "    # ì˜ˆì¸¡ê°’ í• ë‹¹ (ì¸ë±ìŠ¤ ê°œìˆ˜ì™€ ì˜ˆì¸¡ê°’ ê°œìˆ˜ê°€ ë§ëŠ”ì§€ í™•ì¸)\n",
        "    if len(indices) == len(pred_values):\n",
        "        for i, idx in enumerate(indices):\n",
        "            final_predictions[idx] = pred_values[i]\n",
        "            prediction_counts[idx] += 1\n",
        "    else:\n",
        "        print(f\"   âš ï¸ í¬ê¸° ë¶ˆì¼ì¹˜: ì¸ë±ìŠ¤ {len(indices)}ê°œ vs ì˜ˆì¸¡ê°’ {len(pred_values)}ê°œ\")\n",
        "        # í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ìµœì†Œ ê°œìˆ˜ë§Œí¼ë§Œ í• ë‹¹\n",
        "        min_len = min(len(indices), len(pred_values))\n",
        "        for i in range(min_len):\n",
        "            final_predictions[indices[i]] = pred_values[i]\n",
        "            prediction_counts[indices[i]] += 1\n",
        "\n",
        "# ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ë°ì´í„° í™•ì¸\n",
        "unassigned_count = np.sum(prediction_counts == 0)\n",
        "if unassigned_count > 0:\n",
        "    print(f\"âš ï¸ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ë°ì´í„°: {unassigned_count}ê°œ (0ìœ¼ë¡œ ìœ ì§€)\")\n",
        "\n",
        "# ì¤‘ë³µ ì˜ˆì¸¡ í™•ì¸\n",
        "duplicate_count = np.sum(prediction_counts > 1)\n",
        "if duplicate_count > 0:\n",
        "    print(f\"âš ï¸ ì¤‘ë³µ ì˜ˆì¸¡ëœ ë°ì´í„°: {duplicate_count}ê°œ\")\n",
        "\n",
        "print(f\"\\nâœ… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì™„ë£Œ\")\n",
        "print(f\"   ğŸ“Š ì´ ì˜ˆì¸¡ ê°œìˆ˜: {len(final_predictions):,}ê°œ\")\n",
        "print(f\"   ğŸ“ˆ ì˜ˆì¸¡ê°’ í†µê³„: í‰ê· ={np.mean(final_predictions):.2f}, ìµœëŒ€={np.max(final_predictions):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3WOSlaHi6IV"
      },
      "source": [
        "## 9ï¸âƒ£ ìµœì¢… ê²°ê³¼ ì €ì¥ ë° í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìµœì¢… ê²°ê³¼ë¥¼ test_dfì— ì¶”ê°€\n",
        "print(\"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥...\")\n",
        "\n",
        "# ì›ë³¸ test_df ë³µì‚¬\n",
        "result_df = test_df.copy()\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€ (ìŒìˆ˜ê°’ ì œê±°)\n",
        "result_df['pred_heat_demand'] = np.maximum(final_predictions, 0).round(1)\n",
        "\n",
        "# CSV íŒŒì¼ ì €ì¥\n",
        "output_filename = 'lightgbm_branch_season_predictions.csv'\n",
        "result_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"ğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥: {output_filename}\")\n",
        "\n",
        "# =============================================================================\n",
        "# RMSE ì¤‘ì‹¬ ì„±ëŠ¥ í‰ê°€ (ì‹¤ì œê°’ì´ ìˆëŠ” ê²½ìš°)\n",
        "# =============================================================================\n",
        "\n",
        "if 'heat_demand' in test_df.columns:\n",
        "    print(f\"\\nğŸ“Š RMSE ì„±ëŠ¥ í‰ê°€\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # ì „ì²´ RMSE\n",
        "    y_true = test_df['heat_demand'].values\n",
        "    y_pred = result_df['pred_heat_demand'].values\n",
        "    \n",
        "    # ìŒìˆ˜ë‚˜ NaN ê°’ ì œê±°\n",
        "    valid_mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
        "    y_true_clean = y_true[valid_mask]\n",
        "    y_pred_clean = y_pred[valid_mask]\n",
        "    \n",
        "    overall_rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "    overall_mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "    correlation = np.corrcoef(y_true_clean, y_pred_clean)[0, 1]\n",
        "    \n",
        "    print(f\"ğŸ† ì „ì²´ ì„±ëŠ¥:\")\n",
        "    print(f\"   RMSE: {overall_rmse:.4f}\")\n",
        "    print(f\"   MAE:  {overall_mae:.4f}\")\n",
        "    print(f\"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}\")\n",
        "    print(f\"   ìœ íš¨ ë°ì´í„°: {len(y_true_clean):,}ê°œ\")\n",
        "    \n",
        "    # ì‹œì¦Œë³„ RMSE (í•µì‹¬!)\n",
        "    print(f\"\\nğŸ“ˆ ì‹œì¦Œë³„ RMSE ì„±ëŠ¥:\")\n",
        "    season_names = {0: 'ë¹„ë‚œë°©ì‹œì¦Œ', 1: 'ë‚œë°©ìƒìŠ¹ì‹œì¦Œ', 2: 'ë‚œë°©í•˜ê°•ì‹œì¦Œ'}\n",
        "    season_results = {}\n",
        "\n",
        "    for season in [0, 1, 2]:\n",
        "        mask = (test_df['heating_season'] == season) & valid_mask\n",
        "        if np.sum(mask) > 0:\n",
        "            season_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "            season_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
        "            season_corr = np.corrcoef(y_true[mask], y_pred[mask])[0, 1] if np.sum(mask) > 1 else 0\n",
        "            season_results[season] = {\n",
        "                'rmse': season_rmse, \n",
        "                'mae': season_mae, \n",
        "                'corr': season_corr,\n",
        "                'count': np.sum(mask)\n",
        "            }\n",
        "            \n",
        "            print(f\"   {season_names[season]:12s}: RMSE={season_rmse:7.4f} | MAE={season_mae:7.4f} | ìƒê´€={season_corr:6.3f} | {np.sum(mask):,}ê°œ\")\n",
        "    \n",
        "    # ë¸Œëœì¹˜ë³„ RMSE (ìƒìœ„/í•˜ìœ„ ë¶„ì„)\n",
        "    print(f\"\\nğŸ“Š ë¸Œëœì¹˜ë³„ RMSE ì„±ëŠ¥:\")\n",
        "    branch_results = {}\n",
        "    \n",
        "    for branch in sorted(test_df['branch_id'].unique()):\n",
        "        mask = (test_df['branch_id'] == branch) & valid_mask\n",
        "        if np.sum(mask) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆì–´ì•¼ RMSE ê³„ì‚° ê°€ëŠ¥\n",
        "            branch_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "            branch_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
        "            branch_results[branch] = {\n",
        "                'rmse': branch_rmse,\n",
        "                'mae': branch_mae, \n",
        "                'count': np.sum(mask)\n",
        "            }\n",
        "    \n",
        "    if branch_results:\n",
        "        # RMSE ê¸°ì¤€ ì •ë ¬\n",
        "        sorted_branches = sorted(branch_results.items(), key=lambda x: x[1]['rmse'])\n",
        "        \n",
        "        print(f\"   ğŸ¥‡ RMSE ìš°ìˆ˜ ë¸Œëœì¹˜ (Top 5):\")\n",
        "        for i, (branch, metrics) in enumerate(sorted_branches[:5], 1):\n",
        "            print(f\"      {i}. ë¸Œëœì¹˜ {branch}: RMSE={metrics['rmse']:7.4f} | MAE={metrics['mae']:7.4f} | {metrics['count']:,}ê°œ\")\n",
        "        \n",
        "        print(f\"   ğŸ¥‰ RMSE ê°œì„  í•„ìš” ë¸Œëœì¹˜ (Bottom 5):\")\n",
        "        for i, (branch, metrics) in enumerate(sorted_branches[-5:], 1):\n",
        "            print(f\"      {i}. ë¸Œëœì¹˜ {branch}: RMSE={metrics['rmse']:7.4f} | MAE={metrics['mae']:7.4f} | {metrics['count']:,}ê°œ\")\n",
        "        \n",
        "        # ë¸Œëœì¹˜ë³„ ì„±ëŠ¥ í†µê³„\n",
        "        rmse_values = [v['rmse'] for v in branch_results.values()]\n",
        "        print(f\"\\n   ğŸ“ˆ ë¸Œëœì¹˜ë³„ RMSE í†µê³„:\")\n",
        "        print(f\"      í‰ê· : {np.mean(rmse_values):.4f}\")\n",
        "        print(f\"      í‘œì¤€í¸ì°¨: {np.std(rmse_values):.4f}\")\n",
        "        print(f\"      ìµœì†Œ: {np.min(rmse_values):.4f}\")\n",
        "        print(f\"      ìµœëŒ€: {np.max(rmse_values):.4f}\")\n",
        "    \n",
        "    # ì‹œì¦ŒÃ—ë¸Œëœì¹˜ ì¡°í•©ë³„ RMSE (ìƒìœ„ 10ê°œë§Œ)\n",
        "    print(f\"\\nğŸ”¥ ì‹œì¦ŒÃ—ë¸Œëœì¹˜ ì¡°í•©ë³„ RMSE (Ranking):\")\n",
        "    combo_results = []\n",
        "    \n",
        "    for season in [0, 1]:\n",
        "        for branch in test_df['branch_id'].unique():\n",
        "            mask = (test_df['heating_season'] == season) & (test_df['branch_id'] == branch) & valid_mask\n",
        "            if np.sum(mask) > 1:\n",
        "                combo_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "                combo_name = f\"{season_names[season]}_{branch}\"\n",
        "                combo_results.append((combo_name, combo_rmse, np.sum(mask)))\n",
        "    \n",
        "    # RMSE ê¸°ì¤€ ì •ë ¬í•˜ì—¬ í‘œì‹œ\n",
        "    combo_results.sort(key=lambda x: x[1])\n",
        "    for i, (combo_name, rmse, count) in enumerate(combo_results, 1):\n",
        "        print(f\"   {i:2d}. {combo_name:15s}: RMSE={rmse:7.4f} | {count:,}ê°œ\")\n",
        "    \n",
        "    # í›ˆë ¨ëœ ëª¨ë¸ë“¤ì˜ ìµœì í™” ì„±ëŠ¥ê³¼ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¹„êµ\n",
        "    print(f\"\\nğŸ” ëª¨ë¸ ìµœì í™” vs ì‹¤ì œ ì„±ëŠ¥ ë¹„êµ:\")\n",
        "    optimization_rmses = [v['best_score'] for v in training_results.values() if v.get('best_score') is not None]\n",
        "    \n",
        "    if optimization_rmses:\n",
        "        print(f\"   í›ˆë ¨ì‹œ ìµœì í™” RMSE: í‰ê· ={np.mean(optimization_rmses):.4f}, ë²”ìœ„=[{np.min(optimization_rmses):.4f}, {np.max(optimization_rmses):.4f}]\")\n",
        "        print(f\"   ì‹¤ì œ í…ŒìŠ¤íŠ¸ RMSE: {overall_rmse:.4f}\")\n",
        "        print(f\"   ì„±ëŠ¥ ì°¨ì´: {abs(overall_rmse - np.mean(optimization_rmses)):.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"\\nâš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— heat_demand ì»¬ëŸ¼ì´ ì—†ì–´ì„œ RMSE í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"   ì˜ˆì¸¡ ê²°ê³¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    \n",
        "    # ì˜ˆì¸¡ê°’ ê¸°ë³¸ í†µê³„\n",
        "    print(f\"\\nğŸ“Š ì˜ˆì¸¡ê°’ ê¸°ë³¸ í†µê³„:\")\n",
        "    print(f\"   ê°œìˆ˜: {len(result_df):,}ê°œ\")\n",
        "    print(f\"   í‰ê· : {result_df['pred_heat_demand'].mean():.2f}\")\n",
        "    print(f\"   ì¤‘ì•™ê°’: {result_df['pred_heat_demand'].median():.2f}\")\n",
        "    print(f\"   í‘œì¤€í¸ì°¨: {result_df['pred_heat_demand'].std():.2f}\")\n",
        "    print(f\"   ë²”ìœ„: [{result_df['pred_heat_demand'].min():.2f}, {result_df['pred_heat_demand'].max():.2f}]\")\n",
        "    \n",
        "    # ì‹œì¦Œë³„ ì˜ˆì¸¡ í†µê³„\n",
        "    print(f\"\\nğŸ“ˆ ì‹œì¦Œë³„ ì˜ˆì¸¡ í†µê³„:\")\n",
        "    season_names = {0: 'ë¹„ë‚œë°©ì‹œì¦Œ', 1: 'ë‚œë°©ìƒìŠ¹ì‹œì¦Œ', 2: 'ë‚œë°©í•˜ê°•ì‹œì¦Œ'}\n",
        "    for season in [0, 1, 2]:\n",
        "        season_data = result_df[result_df['heating_season'] == season]['pred_heat_demand']\n",
        "        if len(season_data) > 0:\n",
        "            season_name = season_names[season]\n",
        "            print(f\"   {season_name}: í‰ê· ={season_data.mean():.2f}, ê°œìˆ˜={len(season_data):,}ê°œ\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlR68tz9W1AF"
      },
      "source": [
        "## ğŸ”Ÿ ëª¨ë¸ ì •ë³´ ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk3Cf_3YWyKq"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì •ë³´ ë° ê²°ê³¼ ì €ì¥\n",
        "print(\"ğŸ’¾ ëª¨ë¸ ì •ë³´ ì €ì¥...\")\n",
        "\n",
        "# í›ˆë ¨ ê²°ê³¼ ë° ëª¨ë¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥\n",
        "model_info = {\n",
        "    'total_models': len(models),\n",
        "    'successful_models': len([k for k, v in training_results.items() if v.get('best_score') is not None]),\n",
        "    'training_results': training_results,\n",
        "    'prediction_stats': prediction_stats if 'prediction_stats' in locals() else {},\n",
        "    'feature_columns': models[list(models.keys())[0]].feature_cols if models else [],\n",
        "    'optimization_trials_per_model': n_trials_per_model,\n",
        "    'total_training_time_minutes': total_time_min if 'total_time_min' in locals() else 0\n",
        "}\n",
        "\n",
        "# RMSE ê²°ê³¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)\n",
        "if 'heat_demand' in test_df.columns:\n",
        "    model_info['evaluation_results'] = {\n",
        "        'overall_rmse': overall_rmse,\n",
        "        'overall_mae': overall_mae,\n",
        "        'correlation': correlation\n",
        "    }\n",
        "\n",
        "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
        "with open('model_info_and_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(model_info, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "print(\"ğŸ“ model_info_and_results.json ì €ì¥ ì™„ë£Œ\")\n",
        "\n",
        "# ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥\n",
        "print(f\"\\nğŸ“‹ ëª¨ë¸ ì •ë³´ ìš”ì•½:\")\n",
        "print(f\"   ğŸ”§ ì´ ëª¨ë¸ ìˆ˜: {model_info['total_models']}ê°œ\")\n",
        "print(f\"   âœ… ì„±ê³µí•œ ëª¨ë¸: {model_info['successful_models']}ê°œ\")\n",
        "print(f\"   ğŸ“Š ì‚¬ìš© íŠ¹ì„± ìˆ˜: {len(model_info['feature_columns'])}ê°œ\")\n",
        "print(f\"   ğŸ¯ ëª¨ë¸ë‹¹ ìµœì í™” ì‹œë„: {model_info['optimization_trials_per_model']}íšŒ\")\n",
        "\n",
        "# Google Drive ì €ì¥ (Colab í™˜ê²½)\n",
        "if IN_COLAB:\n",
        "    save_drive = input(\"\\nGoogle Driveì— ê²°ê³¼ íŒŒì¼ë“¤ì„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").lower() == 'y'\n",
        "    if save_drive:\n",
        "        try:\n",
        "            !cp {output_filename} /content/drive/MyDrive/\n",
        "            !cp model_info_and_results.json /content/drive/MyDrive/\n",
        "            print(\"âœ… Google Drive ì €ì¥ ì™„ë£Œ!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Google Drive ì €ì¥ ì‹¤íŒ¨: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leJLw4dVi6IW"
      },
      "source": [
        "## ğŸ¯ ìµœì¢… ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8stlKlsi6IX"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ”¥ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡: ì‹œì¦Œë³„-ë¸Œëœì¹˜ë³„ XGBoost ëª¨ë¸ - ìµœì¢… ìš”ì•½\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nğŸ—ï¸ ëª¨ë¸ êµ¬ì„±:\")\n",
        "print(f\"   ğŸ“Š XGBoost ê°œë³„ ëª¨ë¸ (ì‹œì¦Œë³„ Ã— ë¸Œëœì¹˜ë³„)\")\n",
        "print(f\"   ğŸ¯ Optuna TPE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
        "print(f\"   ğŸ“‹ ì´ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ\")\n",
        "print(f\"   âœ… ì„±ê³µì  í›ˆë ¨: {len([k for k, v in training_results.items() if v.get('best_score') is not None])}ê°œ\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§:\")\n",
        "print(f\"   â­ HDD, wind_chill, ì˜¨ë„ ì œê³±/ì„¸ì œê³±\")\n",
        "print(f\"   ğŸ”„ ìˆœí™˜í˜• ì¸ì½”ë”© (ì‹œê°„, ì›”, ìš”ì¼)\")\n",
        "print(f\"   ğŸ“‹ ë²”ì£¼í˜•: heating_season, í”¼í¬ì‹œê°„, ê¸°ì˜¨ë²”ì£¼, ê°•ìˆ˜ê°•ë„\")\n",
        "print(f\"   ğŸ§® ìƒí˜¸ì‘ìš©: ìŠµë„Ã—ê¸°ì˜¨, ì›”Ã—ì¼\")\n",
        "# print(f\"   ğŸ“ StandardScaler ì •ê·œí™”\")\n",
        "\n",
        "print(f\"\\nğŸ¯ ëª¨ë¸ë§ ì „ëµ:\")\n",
        "print(f\"   â„ï¸ ë‚œë°©ì‹œì¦Œ (10,11,12,1,2,3,4ì›”) ì „ìš© ëª¨ë¸\")\n",
        "print(f\"   ğŸŒ ë¹„ë‚œë°©ì‹œì¦Œ (5,6,7,8,9ì›”) ì „ìš© ëª¨ë¸\")\n",
        "print(f\"   ğŸ¢ ë¸Œëœì¹˜ë³„ ê°œë³„ ëª¨ë¸ (ê° ì§€ì‚¬ì˜ íŠ¹ì„± ë°˜ì˜)\")\n",
        "print(f\"   âš¡ XGBoost with Early Stopping\")\n",
        "\n",
        "print(f\"\\nğŸ” ìµœì í™” ì„¤ì •:\")\n",
        "print(f\"   ğŸ“Š Optuna TPE Sampler\")\n",
        "print(f\"   ğŸ¯ ëª¨ë¸ë‹¹ {n_trials_per_model}íšŒ ì‹œë„\")\n",
        "print(f\"   ğŸ“ˆ ì‹œê³„ì—´ ê¸°ë°˜ Train/Validation ë¶„í•  (80:20)\")\n",
        "print(f\"   ğŸª XGBoost Pruning Callback ì‚¬ìš©\")\n",
        "\n",
        "if 'heat_demand' in test_df.columns:\n",
        "    print(f\"\\nğŸ† ìµœì¢… ì„±ëŠ¥:\")\n",
        "    print(f\"   ğŸ“Š ì „ì²´ RMSE: {overall_rmse:.4f}\")\n",
        "    print(f\"   ğŸ“ ì „ì²´ MAE: {overall_mae:.4f}\")\n",
        "    print(f\"   ğŸ“ˆ ìƒê´€ê³„ìˆ˜: {correlation:.4f}\")\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´\n",
        "if successful_models is not None and len(successful_models) > 0:\n",
        "    best_model_name = successful_models['best_score'].idxmin()\n",
        "    best_score = successful_models.loc[best_model_name, 'best_score']\n",
        "    print(f\"\\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (RMSE: {best_score:.4f})\")\n",
        "\n",
        "print(f\"\\nğŸ“ ì¶œë ¥ íŒŒì¼:\")\n",
        "print(f\"   â€¢ {output_filename} - ì˜ˆì¸¡ ê²°ê³¼\")\n",
        "print(f\"   â€¢ model_info_and_results.json - ëª¨ë¸ ì •ë³´ ë° í›ˆë ¨ ê²°ê³¼\")\n",
        "print(f\"   â€¢ í•µì‹¬ ì»¬ëŸ¼: pred_heat_demand (ì˜ˆì¸¡ê°’)\")\n",
        "\n",
        "print(f\"\\nâ±ï¸ ì‹¤í–‰ ì‹œê°„:\")\n",
        "if 'total_time_min' in locals():\n",
        "    print(f\"   ğŸš€ ì´ í›ˆë ¨ ì‹œê°„: {total_time_min:.1f}ë¶„\")\n",
        "    print(f\"   âš¡ í‰ê·  ëª¨ë¸ë‹¹: {total_time_min*60/len(models):.1f}ì´ˆ\")\n",
        "\n",
        "print(f\"\\nğŸ‰ ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ”¬ í˜ì‹  í¬ì¸íŠ¸: ì‹œì¦ŒÃ—ë¸Œëœì¹˜ ì„¸ë¶„í™” + Optuna ìë™ ìµœì í™”\")\n",
        "print(f\"ğŸ“Š ì´ {len(models)}ê°œ ëª¨ë¸ë¡œ ì •ë°€í•œ ì§€ì—­ë³„-ì‹œì¦Œë³„ ì˜ˆì¸¡ êµ¬í˜„\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
